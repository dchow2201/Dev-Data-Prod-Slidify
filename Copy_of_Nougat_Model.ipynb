{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dchow2201/Dev-Data-Prod-Slidify/blob/master/Copy_of_Nougat_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OapoNtm7keMD",
        "outputId": "e73da6e5-7fd0-4f33-e0de-63b912cb098e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.4/277.4 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq nougat-ocr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain openai tiktoken\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryJ8e6x_qY6C",
        "outputId": "836c8c31-f91f-49db-da4d-e5dd8bec5751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.9/219.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the paper using Nougat model from Meta"
      ],
      "metadata": {
        "id": "Ko5VIhseqhsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nougat.utils.checkpoint import get_checkpoint\n",
        "CHECKPOINT = get_checkpoint('nougat')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj0bi8qwoUGq",
        "outputId": "eb086221-8ec9-43b1-f1d2-88c2fb862edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading nougat checkpoint version 0.1.0-small to path nougat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "config.json: 100%|██████████| 557/557 [00:00<00:00, 459kb/s]\n",
            "pytorch_model.bin: 100%|██████████| 956M/956M [00:02<00:00, 374Mb/s]\n",
            "special_tokens_map.json: 100%|██████████| 96.0/96.0 [00:00<00:00, 359kb/s]\n",
            "tokenizer.json: 100%|██████████| 2.04M/2.04M [00:00<00:00, 27.5Mb/s]\n",
            "tokenizer_config.json: 100%|██████████| 106/106 [00:00<00:00, 131kb/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import uuid\n",
        "import requests\n",
        "import re\n",
        "\n",
        "# Download pdf from a given link\n",
        "def get_pdf(pdf_link):\n",
        "  # Generate a unique filename\n",
        "  unique_filename = f\"/content/input/downloaded_paper_{uuid.uuid4().hex}.pdf\"\n",
        "\n",
        "  # Send a GET request to the PDF link\n",
        "  response = requests.get(pdf_link)\n",
        "\n",
        "  if response.status_code == 200:\n",
        "      # Save the PDF content to a local file\n",
        "      with open(unique_filename, 'wb') as pdf_file:\n",
        "          pdf_file.write(response.content)\n",
        "      print(\"PDF downloaded successfully.\")\n",
        "  else:\n",
        "      print(\"Failed to download the PDF.\")\n",
        "  return unique_filename\n",
        "\n",
        "\n",
        "# Run nougat on the pdf file\n",
        "def nougat_ocr(file_name):\n",
        "\n",
        "  # Command to run\n",
        "  cli_command = [\n",
        "      'nougat',\n",
        "      '--out', 'output',\n",
        "      'pdf', file_name,\n",
        "      '--checkpoint', CHECKPOINT,\n",
        "      '--markdown'\n",
        "  ]\n",
        "\n",
        "  # Run the command\n",
        "  subprocess.run(cli_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "  return\n",
        "\n",
        "\n",
        "# predict function / driver function\n",
        "def paper_read(pdf_file=None, pdf_link=None):\n",
        "  if pdf_file is None:\n",
        "    if pdf_link == '':\n",
        "      print(\"No file is uploaded and No link is provided\")\n",
        "      return \"No data provided. Upload a pdf file or provide a pdf link and try again!\"\n",
        "    else:\n",
        "      file_name = get_pdf(pdf_link)\n",
        "  else:\n",
        "    file_name = pdf_file\n",
        "\n",
        "  nougat_ocr(file_name)\n",
        "\n",
        "  # Open the file for reading\n",
        "  file_name = file_name.split('/')[-1][:-4]\n",
        "  with open(f'/content/output/{file_name}.mmd', 'r') as file:\n",
        "      content = file.read()\n",
        "\n",
        "  return content\n"
      ],
      "metadata": {
        "id": "w-rNSg0moLOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('input', exist_ok=True)\n",
        "os.makedirs('output', exist_ok=True)"
      ],
      "metadata": {
        "id": "h8614q-itpOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "content = paper_read(\"/content/input/2205.14135.pdf\")\n",
        "print(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ortVi_5g3ADU",
        "outputId": "b6d1f6b5-0d1e-4dfa-9414-01c2ca4373a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n",
            "\n",
            "Tri Dao\n",
            "\n",
            "Department of Computer Science, Stanford University\n",
            "\n",
            "Daniel Y. Fu\n",
            "\n",
            "Department of Computer Science and Engineering, University at Buffalo, SUNY\n",
            "\n",
            "Stefano Ermon\n",
            "\n",
            "Department of Computer Science, Stanford University\n",
            "\n",
            "Atri Rudra\n",
            "\n",
            "Department of Computer Science and Engineering, University at Buffalo, SUNY\n",
            "\n",
            "Christopher Re\n",
            "\n",
            "Department of Computer Science and Engineering, University at Buffalo, SUNY\n",
            "\n",
            "###### Abstract\n",
            "\n",
            "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms _IO-aware_--accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3x speedup on GPT-2 (seq. length 1K), and 2.4x speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n",
            "\n",
            "## 1 Introduction\n",
            "\n",
            "Transformer models [82] have emerged as the most widely used architecture in applications such as natural language processing and image classification. Transformers have grown larger [5] and deeper [83], but equipping them with longer context remains difficult [80], since the self-attention module at their heart has time and memory complexity quadratic in sequence length. An important question is whether making attention faster and more memory-efficient can help Transformer models address their runtime and memory challenges for long sequences.\n",
            "\n",
            "Many approximate attention methods have aimed to reduce the compute and memory requirements of attention. These methods range from sparse-approximation [51, 74] to low-rank approximation [12, 50, 84], and their combinations [92, 3, 9]. Although these methods reduce the compute requirements to linear or near-linear in sequence length, many of them do not display wall-clock speedup against standard attention and have not gained wide adoption. One main reason is that they focus on FLOP reduction (which may not correlate with wall-clock speed) and tend to ignore overheads from memory access (IO).\n",
            "\n",
            "In this paper, we argue that a missing principle is making attention algorithms _IO-aware_[1]--that is, carefully accounting for reads and writes to different levels of fast and slow memory (e.g., between fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM [45], Figure 1 left). On modernGPUs, compute speed has out-paced memory speed [61, 62, 63], and most operations in Transformers are bottlenecked by memory accesses [43]. IO-aware algorithms have been critical for similar memory-bound operations, when reading and writing data can account for a large portion of the runtime--such as database joins [71], image processing [70], numerical linear algebra [4], and more [40, 85]. However, common Python interfaces to deep learning such as PyTorch and Tensorflow do not allow fine-grained control of memory access.\n",
            "\n",
            "We propose FlashAttention, a new attention algorithm that computes exact attention with far fewer memory accesses. Our main goal is to avoid reading and writing the attention matrix to and from HBM. This requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large intermediate attention matrix for the backward pass. We apply two well-established techniques to address these challenges. (i) We restructure the attention computation to split the input into blocks and make several passes over input blocks, thus incrementally performing the softmax reduction (also known as **tiling**). (ii) We store the softmax normalization factor from the forward pass to quickly **recompute** attention on-chip in the backward pass, which is faster than the standard approach of reading the intermediate attention matrix from HBM. We implement FlashAttention in CUDA to achieve fine-grained control over memory access and fuse all the attention operations into one GPU kernel. Even with the increased FLOPs due to recomputation, our algorithm both **runs faster** (up to 7.6x on GPT-2 [67], Figure 1 right) and **uses less memory**--linear in sequence length--than standard attention, thanks to the massively reduced amount of HBM access.\n",
            "\n",
            "We analyze the IO complexity [1] of FlashAttention, proving that it requires \\(O(N^{2}d^{2}M^{-1})\\) HBM accesses where \\(d\\) is the head dimension and \\(M\\) is the size of SRAM, as compared to \\(\\Omega(Nd+N^{2})\\) of standard attention. For typical values of \\(d\\) and \\(M\\), FlashAttention requires many times fewer HBM accesses compared to standard attention (up to 9x fewer, as shown in Fig. 2). Moreover, we provide a lower bound, showing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over all SRAM sizes.\n",
            "\n",
            "We also show that FlashAttention can serve as a useful primitive for realizing the potential of approximate attention algorithms by overcoming their issues with memory access overhead. As a proof of concept, we implement block-sparse FlashAttention, a sparse attention algorithm that is 2-4x faster than even FlashAttention, scaling up to sequence length of 64k. We prove that block-sparse FlashAttention has better IO complexity than FlashAttention by a factor proportional to the sparsity ratio. We discuss further extensions to other operations (attention on multi-GPU, kernel regression, block-sparse matrix\n",
            "\n",
            "Figure 1: **Left:** FlashAttention uses tiling to prevent materialization of the large \\(N\\times N\\) attention matrix (dotted box) on (relatively) slow GPU HBM. In the outer loop (red arrows), FlashAttention loops through blocks of the \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) matrices and loads them to fast on-chip SRAM. In each block, FlashAttention loops over blocks of \\(\\mathbf{Q}\\) matrix (blue arrows), loading them to SRAM, and writing the output of the attention computation back to HBM. **Right:** Speedup over the PyTorch implementation of attention on GPT-2. FlashAttention does not read and write the large \\(N\\times N\\) attention matrix to HBM, resulting in an \\(7.6\\times\\) speedup on the attention computation.\n",
            "\n",
            " multiply) in Section 5. We open-source FlashAttention to make it easier to build on this primitive.1\n",
            "\n",
            "Footnote 1: FlashAttention code is available at [https://github.com/HazyResearch/flash-attention](https://github.com/HazyResearch/flash-attention)\n",
            "\n",
            "We empirically validate that FlashAttention speeds up model training and improves model quality by modeling longer context. We also benchmark the runtime and memory footprint of FlashAttention and block-sparse FlashAttention compared to prior attention implementations.\n",
            "\n",
            "* **Faster Model Training.** FlashAttention trains Transformer models faster in wall-clock time. We train BERT-large (seq. length 512) 15% faster than the training speed record in MLPerf 1.1 [58], GPT2 (seq. length 1K) 3\\(\\times\\) faster than baseline implementations from HuggingFace [87] and Megatron-LM [77], and long-range arena (seq. length 1K-4K) 2.4\\(\\times\\) faster than baselines.\n",
            "* **Higher Quality Models.** FlashAttention scales Transformers to longer sequences, which improves their quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and 6.4 points of lift from modeling longer sequences on long-document classification [13]. FlashAttention enables the first Transformer that can achieve better-than-chance performance on the Path-X [80] challenge, solely from using a longer sequence length (16K). Block-sparse FlashAttention enables a Transformer to scale to even longer sequences (64K), resulting in the first model that can achieve better-than-chance performance on Path-256.\n",
            "* **Benchmarking Attention.** FlashAttention is up to 3\\(\\times\\) faster than the standard attention implementation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512, FlashAttention is both faster and more memory-efficient than any existing attention method, whereas for sequence length beyond 1K, some approximate attention methods (e.g., Linformer) start to become faster. On the other hand, block-sparse FlashAttention is faster than all existing approximate attention methods that we know of.\n",
            "\n",
            "## 2 Background\n",
            "\n",
            "We provide some background on the performance characteristics of common deep learning operations on modern hardware (GPUs). We also describe the standard implementation of attention.\n",
            "\n",
            "### Hardware Performance\n",
            "\n",
            "We focus here on GPUs. Performance on other hardware accelerators are similar [46, 48].\n",
            "\n",
            "**GPU Memory Hierarchy.** The GPU memory hierarchy (Fig. 1 left) comprises multiple forms of memory of different sizes and speeds, with smaller memory being faster. As an example, the A100 GPU has 40-80GB of high bandwidth memory (HBM) with bandwidth 1.5-2.0TB/s and 192KB of on-chip SRAM per each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s [44, 45]. The on-chip SRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size. As compute has gotten faster relative to memory speed [61, 62, 63], operations are increasingly bottlenecked by memory (HBM) accesses. Thus exploiting fast SRAM becomes more important.\n",
            "\n",
            "**Execution Model.** GPUs have a massive number of threads to execute an operation (called a kernel). Each kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM.\n",
            "\n",
            "**Performance characteristics.** Depending on the balance of computation and memory accesses, operations can be classified as either compute-bound or memory-bound. This is commonly measured by the _arithmetic intensity_[85], which is the number of arithmetic operations per byte of memory access.\n",
            "\n",
            "1. [leftmargin=*]\n",
            "2. Compute-bound: the time taken by the operation is determined by how many arithmetic operations there are, while time accessing HBM is much smaller. Typical examples are matrix multiply with large inner dimension, and convolution with large number of channels.\n",
            "3. Memory-bound: the time taken by the operation is determined by the number of memory accesses, while time spent in computation is much smaller. Examples include most other operations: elementwise (e.g., activation, dropout), and reduction (e.g., sum, softmax, batch norm, layer norm).\n",
            "\n",
            "**Kernel fusion.** The most common approach to accelerate memory-bound operations is kernel fusion: if there are multiple operations applied to the same input, the input can be loaded once from HBM, instead of multiple times for each operation. Compilers can automatically fuse many elementwise operations [53, 65, 75].\n",
            "\n",
            "However, in the context of model training, the intermediate values still need to be written to HBM to save for the backward pass, reducing the effectiveness of naive kernel fusion.\n",
            "\n",
            "### Standard Attention Implementation\n",
            "\n",
            "Given input sequences \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{N\\times d}\\) where \\(N\\) is the sequence length and \\(d\\) is the head dimension, we want to compute the attention output \\(\\mathbf{O}\\in\\mathbb{R}^{N\\times d}\\):\n",
            "\n",
            "\\[\\mathbf{S}=\\mathbf{Q}\\mathbf{K}^{\\top}\\in\\mathbb{R}^{N\\times N},\\quad\\mathbf{P} =\\text{softmax}(\\mathbf{S})\\in\\mathbb{R}^{N\\times N},\\quad\\mathbf{O}=\\mathbf{ PV}\\in\\mathbb{R}^{N\\times d},\\]\n",
            "\n",
            "where softmax is applied row-wise.\n",
            "\n",
            "Standard attention implementations materialize the matrices \\(\\mathbf{S}\\) and \\(\\mathbf{P}\\) to HBM, which takes \\(O(N^{2})\\) memory. Often \\(N\\gg d\\) (e.g., for GPT2, \\(N=1024\\) and \\(d=64\\)). We describe the standard attention implementation in Algorithm 0. As some or most of the operations are memory-bound (e.g., softmax), the large number of memory accesses translates to slow wall-clock time.\n",
            "\n",
            "This problem is exacerbated by other elementwise operations applied to the attention matrix, such as masking applied to \\(\\mathbf{S}\\) or dropout applied to \\(\\mathbf{P}\\). As a result, there have been many attempts to fuse several elementwise operations, such as fusing masking with softmax [77].\n",
            "\n",
            "In Section 3.2, we will show that the standard attention implementation performs HBM accesses quadratic in the sequence length \\(N\\). We also compare the number of FLOPs and number of HBM accesses of standard attention and of our method (FlashAttention).\n",
            "\n",
            "```\n",
            "0: Matrices \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{N\\times d}\\) in HBM.\n",
            "1: Load \\(\\mathbf{Q},\\mathbf{K}\\) by blocks from HBM, compute \\(\\mathbf{S}=\\mathbf{Q}\\mathbf{K}^{\\top}\\), write \\(\\mathbf{S}\\) to HBM.\n",
            "2: Read \\(\\mathbf{S}\\) from HBM, compute \\(\\mathbf{P}=\\text{softmax}(\\mathbf{S})\\), write \\(\\mathbf{P}\\) to HBM.\n",
            "3: Load \\(\\mathbf{P}\\) and \\(\\mathbf{V}\\) by blocks from HBM, compute \\(\\mathbf{O}=\\mathbf{PV}\\), write \\(\\mathbf{O}\\) to HBM.\n",
            "4: Return \\(\\mathbf{O}\\).\n",
            "```\n",
            "\n",
            "**Algorithm 0** Standard Attention Implementation\n",
            "\n",
            "## 3 FlashAttention: Algorithm, Analysis, and Extensions\n",
            "\n",
            "We show how to compute exact attention with fewer HBM reads/writes and without storing large intermediate matrices for the backward pass. This yields an attention algorithm that is both memory efficient and faster in wall-clock time. We analyze its IO complexity, showing that our method requires much fewer HBM accesses compared to standard attention. We further show that FlashAttention can serve as a useful primitive by extending it to handle block-sparse attention.\n",
            "\n",
            "We focus here on the forward pass for ease of exposition; Appendix B contains details for the backward.\n",
            "\n",
            "### An Efficient Attention Algorithm With Tiling and Recomputation\n",
            "\n",
            "Given the inputs \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{N\\times d}\\) in HBM, we aim to compute the attention output \\(\\mathbf{O}\\in\\mathbb{R}^{N\\times d}\\) and write it to HBM. Our goal is to reduce the amount of HBM accesses (to sub-quadratic in \\(N\\)).\n",
            "\n",
            "We apply two established techniques (tiling, recomputation) to overcome the technical challenge of computing exact attention in sub-quadratic HBM accesses. We describe this in Algorithm 1. The main idea is that we split the inputs \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\) into blocks, load them from slow HBM to fast SRAM, then compute the attention output with respect to those blocks. By scaling the output of each block by the right normalization factor before adding them up, we get the correct result at the end.\n",
            "\n",
            "**Tiling.** We compute attention by blocks. Softmax couples columns of \\(\\mathbf{K}\\), so we decompose the large softmax with scaling [60, 66, 51]. For numerical stability, the softmax of vector \\(x\\in\\mathbb{R}^{B}\\) is computed as:\n",
            "\n",
            "\\[m(x):=\\max_{i}\\ \\ x_{i},\\quad f(x):=\\left[e^{x_{1}-m(x)}\\quad\\dots\\quad e^{x_{B}-m( x)}\\right],\\quad\\ell(x):=\\sum_{i}f(x)_{i},\\quad\\text{softmax}(x):=\\frac{f(x)}{\\ell(x)}.\\]For vectors \\(x^{(1)},x^{(2)}\\in\\mathbb{R}^{B}\\), we can decompose the softmax of the concatenated \\(x=\\left[x^{(1)}\\ x^{(2)}\\right]\\in\\mathbb{R}^{2B}\\) as:\n",
            "\n",
            "\\[m(x)=m\\big{(}\\big{[}x^{(1)}\\ x^{(2)}\\big{]}\\big{)}=\\max(m(x^{(1)}),m(x^{(2)})), \\quad f(x)=\\left[e^{m(x^{(1)})-m(x)}f(x^{(1)})\\quad e^{m(x^{(2)})-m(x)}f(x^{(2) })\\right],\\]\n",
            "\n",
            "\\[\\ell(x)=\\ell\\big{(}\\big{[}x^{(1)}\\ x^{(2)}\\big{]}\\big{)}=e^{m(x^{(1)})-m(x)} \\ell(x^{(1)})+e^{m(x^{(2)})-m(x)}\\ell(x^{(2)}),\\quad\\text{softmax}(x)=\\frac{f(x)} {\\ell(x)}.\\]\n",
            "\n",
            "Therefore if we keep track of some extra statistics \\((m(x),\\ell(x))\\), we can compute softmax one block at a time.2 We thus split the inputs \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\) into blocks (Algorithm 1 line 3), compute the softmax values along with extra statistics (Algorithm 1 line 10), and combine the results (Algorithm 1 line 12).\n",
            "\n",
            "Footnote 2: This style of aggregation is called _algebraic aggregation_[33].\n",
            "\n",
            "**Recomputation.** One of our goals is to not store \\(O(N^{2})\\) intermediate values for the backward pass. The backward pass typically requires the matrices \\(\\mathbf{S},\\mathbf{P}\\in\\mathbb{R}^{N\\times N}\\) to compute the gradients with respect to \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\). However, by storing the output \\(\\mathbf{O}\\) and the softmax normalization statistics \\((m,\\ell)\\), we can recompute the attention matrix \\(\\mathbf{S}\\) and \\(\\mathbf{P}\\) easily in the backward pass from blocks of \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\) in SRAM. This can be seen as a form of selective gradient checkpointing [10, 34]. While gradient checkpointing has been suggested to reduce the maximum amount of memory required [66], all implementations (that we know off) have to trade speed for memory. In contrast, even with more FLOPs, our recomputation speeds up the backward pass due to reduced HBM accesses (Fig. 2). The full backward pass description is in Appendix B.\n",
            "\n",
            "**Implementation details: Kernel fusion.** Tiling enables us to implement our algorithm in one CUDA kernel, loading input from HBM, performing all the computation steps (matrix multiply, softmax, optionally masking and dropout, matrix multiply), then write the result back to HBM (masking and dropout in Appendix B). This avoids repeatedly reading and writing of inputs and outputs from and to HBM.\n",
            "\n",
            "```\n",
            "0: Matrices \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{N\\times d}\\) in HBM, on-chip SRAM of size \\(M\\).\n",
            "1: Set block sizes \\(B_{c}=\\left\\lceil\\frac{M}{dd}\\right\\rceil,B_{r}=\\min\\left(\\left\\lceil\\frac{M}{ dd}\\right\\rceil,d\\right)\\).\n",
            "2: Initialize \\(\\mathbf{O}=(0)_{N\\times d}\\in\\mathbb{R}^{N\\times d},\\ell=(0)_{N}\\in\\mathbb{R}^ {N},m=(-\\infty)_{N}\\in\\mathbb{R}^{N}\\) in HBM.\n",
            "3: Divide \\(\\mathbf{Q}\\) into \\(T_{r}=\\left\\lceil\\frac{N}{B_{r}}\\right\\rceil\\) blocks \\(\\mathbf{Q}_{1},\\ldots,\\mathbf{Q}_{T_{r}}\\) of size \\(B_{r}\\times d\\) each, and divide \\(\\mathbf{K},\\mathbf{V}\\) in to \\(T_{c}=\\left\\lceil\\frac{N}{B_{c}}\\right\\rceil\\) blocks \\(\\mathbf{K}_{1},\\ldots,\\mathbf{K}_{T_{c}}\\) and \\(\\mathbf{V}_{1},\\ldots,\\mathbf{V}_{T_{c}}\\), of size \\(B_{c}\\times d\\) each.\n",
            "4: Divide \\(\\mathbf{O}\\) into \\(T_{r}\\) blocks \\(\\mathbf{O}_{i},\\ldots,\\mathbf{O}_{T_{r}}\\) of size \\(B_{r}\\times d\\) each, divide \\(\\ell\\) into \\(T_{r}\\) blocks \\(\\ell_{i},\\ldots,\\ell_{T_{r}}\\) of size \\(B_{r}\\) each, divide \\(m\\) into \\(T_{r}\\) blocks \\(m_{1},\\ldots,m_{T_{r}}\\) of size \\(B_{r}\\) each.\n",
            "5:for\\(1\\leq j\\leq T_{c}\\)do\n",
            "6: Load \\(\\mathbf{K}_{j},\\mathbf{V}_{j}\\) from HBM to on-chip SRAM.\n",
            "7:for\\(1\\leq i\\leq T_{r}\\)do\n",
            "8: Load \\(\\mathbf{Q}_{i},\\mathbf{O}_{i},\\ell_{i},m_{i}\\) from HBM to on-chip SRAM.\n",
            "9: On chip, compute \\(\\mathbf{S}_{ij}=\\mathbf{Q}_{i}\\mathbf{K}_{j}^{T}\\in\\mathbb{R}^{B_{r}\\times B _{c}}\\).\n",
            "10: On chip, compute \\(\\tilde{m}_{ij}=\\text{rowmax}(\\mathbf{S}_{ij})\\in\\mathbb{R}^{B_{r}}\\), \\(\\tilde{\\mathbf{P}}_{ij}=\\exp(\\mathbf{S}_{ij}-\\tilde{m}_{ij})\\in\\mathbb{R}^{B _{r}\\times\\mathbf{R}_{c}}\\) (pointwise), \\(\\tilde{\\ell}_{ij}=\\text{rowsum}(\\tilde{\\mathbf{P}}_{ij})\\in\\mathbb{R}^{B_{r}}\\).\n",
            "11: On chip, compute \\(m_{i}^{\\text{new}}=\\max(m_{i},\\tilde{m}_{ij})\\in\\mathbb{R}^{B_{r}}\\), \\(\\ell_{i}^{\\text{new}}=e^{m_{i}-m_{i}^{\\text{new}}}\\ell_{i}+e^{\\tilde{m}_{ij}-m _{i}^{\\text{new}}}\\tilde{\\ell}_{ij}\\in\\mathbb{R}^{B_{r}}\\).\n",
            "12: Write \\(\\mathbf{O}_{i}\\leftarrow\\text{diag}(\\ell_{i}^{\\text{new}})^{-1}(\\text{diag}( \\ell_{i})\\mathbf{e}^{m_{i}-m_{i}^{\\text{new}}}\\mathbf{O}_{i}+e^{\\tilde{m}_{ij}-m _{i}^{\\text{new}}}\\tilde{\\mathbf{P}}_{ij}\\mathbf{V}_{j})\\) to HBM.\n",
            "13: Write \\(\\ell_{i}\\leftarrow\\ell_{i}^{\\text{new}}\\), \\(m_{i}\\gets m_{i}^{\\text{new}}\\) to HBM.\n",
            "14:endfor\n",
            "15:endfor\n",
            "16:Return \\(\\mathbf{O}\\).\n",
            "```\n",
            "\n",
            "**Algorithm 1**FlashAttention\n",
            "\n",
            "We show FlashAttention's correctness, runtime, and memory requirement (proof in Appendix C).\n",
            "\n",
            "**Theorem 1**.: _Algorithm 1 returns \\(\\mathbf{O}=\\text{softmax}(\\mathbf{Q}\\mathbf{K}^{\\top})\\mathbf{V}\\) with \\(O(N^{2}d)\\) FLOPs and requires \\(O(N)\\) additional memory beyond inputs and output._\n",
            "\n",
            "### Analysis: IO Complexity of FlashAttention\n",
            "\n",
            "We analyze the IO complexity of FlashAttention, showing significant reduction in HBM accesses compared to standard attention. We also provide a lower bound, proving that no exact attention algorithm can asymptotically improve on HBM accesses over all SRAM sizes. Proofs are in Appendix C.\n",
            "\n",
            "**Theorem 2**.: _Let \\(N\\) be the sequence length, \\(d\\) be the head dimension, and \\(\\mathbf{M}\\) be size of SRAM with \\(d\\leq M\\leq Nd\\). Standard attention (Algorithm 0) requires \\(\\Theta(Nd+N^{2})\\) HBM accesses, while FlashAttention (Algorithm 1) requires \\(\\Theta(N^{2}d^{2}M^{-1})\\) HBM accesses._\n",
            "\n",
            "For typical values of \\(d\\) (64-128) and \\(M\\) (around 100KB), \\(d^{2}\\) is many times smaller than \\(M\\), and thus FlashAttention requires many times fewer HBM accesses than standard implementation. This leads to both faster execution and lower memory footprint, which we validate in Section 4.3.\n",
            "\n",
            "The main idea of the proof is that given the SRAM size of \\(M\\), we can load blocks of \\(\\mathbf{K},\\mathbf{V}\\) of size \\(\\Theta(M)\\) each (Algorithm 1 line 6). For each block of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\), we iterate over all blocks of \\(\\mathbf{Q}\\) (Algorithm 1 line 8) to compute the intermediate values, resulting in \\(\\Theta(NdM^{-1})\\) passes over \\(\\mathbf{Q}\\). Each pass loads \\(\\Theta(Nd)\\) elements, which amounts to \\(\\Theta(N^{2}d^{2}M^{-1})\\) HBM accesses. We similarly prove that the backward pass of standard attention requires \\(\\Theta(Nd+N^{2})\\) HBM accesses while the backward pass of FlashAttention requires \\(\\Theta(N^{2}d^{2}M^{-1})\\) HBM accesses (Appendix B).\n",
            "\n",
            "We prove a lower-bound: one cannot asymptotically improve on the number of HBM accesses for all values of \\(M\\) (the SRAM size) when computing exact attention.\n",
            "\n",
            "**Proposition 3**.: _Let \\(N\\) be the sequence length, \\(d\\) be the head dimension, and \\(M\\) be size of SRAM with \\(d\\leq M\\leq Nd\\). There does not exist an algorithm to compute exact attention with \\(o(N^{2}d^{2}M^{-1})\\) HBM accesses for all \\(M\\) in the range \\([d,Nd]\\)._\n",
            "\n",
            "The proof relies on the fact that for \\(M=\\Theta(Nd)\\) any algorithm must perform \\(\\Omega(N^{2}d^{2}M^{-1})=\\Omega(Nd)\\) HBM accesses. This type of lower bound over a subrange of \\(M\\) is common in the streaming algorithms literature [88]. We leave proving parameterized complexity [27] lower bounds in terms of \\(M\\) as exciting future work.\n",
            "\n",
            "We validate that the number of HBM accesses is the main determining factor of attention run-time. In Fig. 2 (left), we see that even though FlashAttention has higher FLOP count compared to standard attention (due to recomputation in the backward pass), it has much fewer HBM accesses, resulting in much faster runtime. In Fig. 2 (middle), we vary the block size \\(B_{c}\\) of FlashAttention, which results in different amounts of HBM accesses, and measure the runtime of the forward pass. As block size increases, the number of HBM accesses decreases (as we make fewer passes over the input), and runtime decreases. For large enough block size (beyond 256), the runtime is then bottlenecked by other factors (e.g., arithmetic operations). Moreover, larger block size will not fit into the small SRAM size.\n",
            "\n",
            "### Extension: Block-Sparse FlashAttention\n",
            "\n",
            "We extend FlashAttention to approximate attention: we propose block-sparse FlashAttention, whose IO complexity is smaller than FlashAttention by a factor proportional to the sparsity.\n",
            "\n",
            "Given inputs \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{N\\times d}\\) and a mask matrix \\(\\tilde{\\mathbf{M}}\\in\\{0,1\\}^{N\\times N}\\), we want to compute:\n",
            "\n",
            "\\[\\mathbf{S}=\\mathbf{Q}\\mathbf{K}^{\\top}\\in\\mathbb{R}^{N\\times N},\\quad\\mathbf{ P}=\\operatorname{softmax}(\\mathbf{S}\\odot 1_{\\tilde{\\mathbf{M}}})\\in\\mathbb{R}^{N \\times N},\\quad\\mathbf{O}=\\mathbf{PV}\\in\\mathbb{R}^{N\\times d},\\]\n",
            "\n",
            "where \\((\\mathbf{S}\\odot 1_{\\tilde{\\mathbf{M}}})_{kl}=\\mathbf{S}_{kl}\\) if \\(\\tilde{\\mathbf{M}}_{kl}=1\\) and \\(-\\infty\\) if \\(\\mathbf{M}_{kl}=0\\). We require \\(\\tilde{\\mathbf{M}}\\) to have block form: for some block sizes \\(B_{r},B_{c}\\), for all \\(k,l\\), \\(\\tilde{\\mathbf{M}}_{k,l}=\\mathbf{M}_{ij}\\) with \\(i=\\lfloor k/B_{r}\\rfloor,j=\\lfloor l/B_{c}\\rfloor\\) for some \\(\\mathbf{M}\\in\\{0,1\\}^{N/B_{r}\\times N/B_{c}}\\).\n",
            "\n",
            "Figure 2: **Left**: Forward + backward runtime of standard attention and FlashAttention for GPT-2 medium (seq: length 1024, head dim. 64, 16 heads, batch size 64) on A100 GPU. HBM access is the primary factor affecting runtime. **Middle**: Forward runtime of FlashAttention (seq. length 1024, head dim. 64, 16 heads, batch size 64) on A100 GPU. Fewer HBM accesses result in faster runtime, up to a point. **Right**: The runtime (for seq. length 4K) of block-sparse FlashAttention is faster than FlashAttention by a factor proportional to the sparsity.\n",
            "\n",
            "Given a predefined block sparsity mask \\(\\textbf{M}\\in\\{0,1\\}^{N/B_{r}\\times N/B_{c}}\\) we can easily adapt Algorithm 1 to only compute the nonzero blocks of the attention matrix. The algorithm is identical to Algorithm 1, except we skip zero blocks. We reproduce the algorithm description in Algorithm 5 in Appendix B.\n",
            "\n",
            "We also analyze the IO complexity of block-sparse FlashAttention.\n",
            "\n",
            "**Proposition 4**.: _Let N be the sequence length, d be the head dimension, and M be size of SRAM with \\(d\\leq M\\leq Nd\\). Block-sparse FlashAttention (Algorithm 5) requires \\(\\Theta(Nd+N^{2}d^{2}M^{-1}s)\\) HBM accesses where \\(s\\) is the fraction of nonzero blocks in the block-sparsity mask._\n",
            "\n",
            "We see that applying block-sparsity yields a direct improvement by the sparsity to the larger term in the IO complexity. For large sequence lengths \\(N\\), \\(s\\) is often set to \\(N^{-1/2}\\)[11] or \\(N^{-1}\\log N\\)[3, 17, 92], resulting in \\(\\Theta(N\\sqrt{N})\\) or \\(\\Theta(N\\log N)\\) IO complexity. For downstream experiments, we use the fixed butterfly sparsity pattern [17], which has been shown to be able to approximate arbitrary sparsity [16].\n",
            "\n",
            "In Fig. 2 (right), we validate that as the sparsity increases, the runtime of block-sparse FlashAttention improves proportionally. On the LRA benchmark, block-sparse FlashAttention achieves \\(2.8\\times\\) speedup, while performing on par with standard attention (Section 4).\n",
            "\n",
            "## 4 Experiments\n",
            "\n",
            "We evaluate the impact of using FlashAttention to train Transformer models. We validate two claims about training time and model accuracy, and report attention runtime and memory benchmarks.\n",
            "\n",
            "* **Training Speed.** FlashAttention outperforms the MLPerf 1.1 [58] speed record for BERT by 15%, and speeds up GPT-2 up to \\(3\\times\\) over HuggingFace [87] and \\(1.8\\times\\) over Megatron [77] over standard Transformers. FlashAttention speeds up the long-range arena (LRA) benchmark \\(2.4\\times\\).\n",
            "* **Quality.** FlashAttention scales Transformers to longer sequences, yielding higher quality. FlashAttention trains GPT-2 with context length \\(4\\)K faster than Megatron trains GPT-2 with context length \\(1\\)K, while achieving \\(0.7\\) better perplexity. Modeling longer sequences yields \\(6.4\\) points of lift on two long-document classification tasks. Finally, FlashAttention yields the **first Transformer** that can achieve better-than-random performance on the challenging Path-X task (sequence length \\(16\\)K), and block-sparse FlashAttention yields the **first sequence model** that we know of that can achieve better-than-random performance on Path-256 (sequence length \\(64\\)K).\n",
            "* **Benchmarking Attention.** We measure the runtime and memory performance of FlashAttention and block-sparse FlashAttention based on sequence length. We confirm that the memory footprint of FlashAttention scales linearly with seq. length and is up to \\(3\\times\\) faster than standard attention for common seq. lengths (up to \\(2\\)K). We confirm that runtime of block-sparse FlashAttention scales linearly in seq. length and is faster than all existing approximate attention baselines. Additional experiment details are in Appendix E.\n",
            "\n",
            "### Faster Models with FlashAttention\n",
            "\n",
            "Bert.FlashAttention yields the fastest single-node BERT training speed that we know of. We train a BERT-large [22] model with FlashAttention on Wikipedia. Table 1 compares our training time to the implementation from Nvidia that set the training speed record for MLPerf 1.1 [58]. Our implementation is \\(15\\%\\) faster.\n",
            "\n",
            "Gpt-2.FlashAttention yields faster training times for GPT-2 [67] on the large OpenWebtext dataset [32] than the widely used HuggingFace [87] and Megatron-LM [77] implementations. Table 2 shows up to \\(3\\times\\) end-to-end speedup compared to Huggingface and \\(1.7\\times\\) speedup compared to Megatron-LM. FlashAttention\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{l|c} BERT Implementation & Training time (minutes) \\\\ \\hline Nvidia MLPerf 1.1 [58] & \\(20.0\\pm 1.5\\) \\\\ FlashAttention (ours) & \\(\\textbf{17.4}\\pm 1.4\\) \\\\ \\end{tabular}\n",
            "\\end{table}\n",
            "Table 1: Training time of BERT-large, starting from the same initialization provided by the MLPerf benchmark, to reach the target accuracy of \\(72.0\\%\\) on masked language modeling. Averaged over \\(10\\) runs on \\(8\\times\\)A\\(100\\) GPUs.\n",
            "\n",
            "achieves the same perplexity as the other two implementations, as we do not change the model definition. Appendix E includes plots of the validation perplexity throughout training, confirming that FlashAttention is as numerically stable as the baselines and produces the same training / validation curves.\n",
            "\n",
            "Long-range Arena.We compare vanilla Transformer (with either standard implementation or FlashAttention) on the long-range arena (LRA [80]) benchmark. We measure accuracy, throughput, and training time of all models. Each task has a different sequence length varying between 1024 and 4096. We follow the implementation and experimental setting in Tay et al. [80]and Xiong et al. [90].3 Table 3 shows that FlashAttention achieves up 2.4\\(\\times\\) speed-up compared to standard attention. Block-sparse FlashAttention is faster than all of the approximate attention methods that we have tested.\n",
            "\n",
            "Footnote 3: LRA accuracy results are known to be highly dependent on the tuning procedure [90]. Our reproduced baselines perform better than as reported in the original comparison [80].\n",
            "\n",
            "### Better Models with Longer Sequences\n",
            "\n",
            "Language Modeling with Long Context.The runtime and memory-efficiency of FlashAttention allow us to increase the context length of GPT-2 by 4\\(\\times\\) while still running faster than the optimized implementation from Megatron-LM. Table 4 shows that that GPT-2 with FlashAttention and context length 4K is still 30% faster than GPT-2 from Megatron with context length 1K, while achieving 0.7 better perplexity.\n",
            "\n",
            "Long Document Classification.Training Transformers with longer sequences with FlashAttention improves performance on the MIMIC-III [47] and ECtHR [6, 7] datasets. MIMIC-III contains intensive care unit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{c|c c c} Model implementations & Context length & OpenWebText (ppl) & Training time (speedup) \\\\ \\hline GPT-2 small - Megatron-LM & 1k & 18.2 & 4.7 days (1.0x) \\\\ GPT-2 small - FlashAttention & 1k & 18.2 & **2.7 days (1.7\\(\\times\\))** \\\\ GPT-2 small - FlashAttention & 2k & 17.6 & 3.0 days (1.6x) \\\\ GPT-2 small - FlashAttention & 4k & **17.5** & 3.6 days (1.3x) \\\\ \\end{tabular}\n",
            "\\end{table}\n",
            "Table 4: GPT-2 small with FlashAttention, with 4\\(\\times\\) larger context length compared to Megatron-LM, is still 30% faster while achieving 0.7 better perplexity. Training time on 8\\(\\times\\)A100 GPUs is reported.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{c|c c c|c|c|c} Model implementations & OpenWebText (ppl) & Training time (speedup) \\\\ \\hline GPT-2 small - Huggingface [87] & 18.2 & 9.5 days (1.0x) \\\\ GPT-2 small - Megatron-LM [77] & 18.2 & 4.7 days (2.0x) \\\\ GPT-2 small - FlashAttention & 18.2 & **2.7 days (3.5\\(\\times\\))** \\\\ \\hline GPT-2 medium - Huggingface [87] & 14.2 & 21.0 days (1.0\\(\\times\\)) \\\\ GPT-2 medium - Megatron-LM [77] & 14.3 & 11.5 days (1.8\\(\\times\\)) \\\\ GPT-2 medium - FlashAttention & 14.3 & **6.9 days (3.0\\(\\times\\))** \\\\ \\end{tabular}\n",
            "\\end{table}\n",
            "Table 2: GPT-2 small and medium using FlashAttention achieve up to 3\\(\\times\\) speed up compared to Huggingface implementation and up to 1.7\\(\\times\\) compared to Megatron-LM. Training time reported on 8\\(\\times\\)A100s GPUs.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{c|c c c c c|c|c} Models & ListOps & Text & Retrieval & Image & Pathfinder & Avg & Speedup \\\\ \\hline Transformer & 36.0 & 63.6 & 81.6 & 42.3 & 72.7 & 59.3 & - \\\\ FlashAttention & 37.6 & 63.9 & 81.4 & 43.5 & 72.7 & 59.8 & 2.4\\(\\times\\) \\\\ Block-sparse FlashAttention & 37.0 & 63.0 & 81.3 & 43.6 & 73.3 & 59.6 & **2.8\\(\\times\\)** \\\\ \\hline Linformer [84] & 35.6 & 55.9 & 77.7 & 37.8 & 67.6 & 54.9 & 2.5\\(\\times\\) \\\\ Linear Attention [50] & 38.8 & 63.2 & 80.7 & 42.6 & 72.5 & 59.6 & 2.3\\(\\times\\) \\\\ Performer [12] & 36.8 & 63.6 & 82.2 & 42.1 & 69.9 & 58.9 & 1.8\\(\\times\\) \\\\ Local Attention [80] & 36.1 & 60.2 & 76.7 & 40.6 & 66.6 & 56.0 & 1.7\\(\\times\\) \\\\ Reformer [51] & 36.5 & 63.8 & 78.5 & 39.6 & 69.4 & 57.6 & 1.3\\(\\times\\) \\\\ Smyrf [19] & 36.1 & 64.1 & 79.0 & 39.6 & 70.5 & 57.9 & 1.7\\(\\times\\) \\\\ \\end{tabular}\n",
            "\\end{table}\n",
            "Table 3: The performance of standard attention, FlashAttention, block-sparse FlashAttention, and approximate attention baselines on the Long-Range-Arena benchmarks.\n",
            "\n",
            "European Court of Human Rights, each of which is mapped to articles of the Convention of Human Rights that were allegedly violaged. Both of these datasets contain very long text documents; the average number of tokens in MIMIC is 2,395 tokens, and the longest document contains 14,562 tokens, while the average and longest numbers in ECtHR are 2,197 and 49,392, respectively. We evaluate lift from increasing the sequence length of a pretrained RoBERTa model [56] (we repeat the positional embeddings, as in Beltagy et al. [3]).\n",
            "\n",
            "Table 5 shows that sequence length 16K outperforms length 512 by 4.3 points on MIMIC, and that length 8K outperforms length 512 by 8.5 points on ECtHR. The discrepancies may be due to subtle distribution shifts: MIMIC-III contains specialized medical text and thus may be more susceptible to a distribution shift in the document length, whereas ECtHR contains general language.\n",
            "\n",
            "Path-X and Path-256.The Path-X and Path-256 benchmarks are challenging tasks from the long-range arena benchmark designed to test long context. The task is to classify whether two points in a black and white 128\\(\\times\\)128 (or 256\\(\\times\\)256) image have a path connecting them, and the images are fed to the transformer one pixel at a time. In prior work, all transformer models have either run out of memory, or only achieved random performance [80]. There has been a search for alternative architectures that can model such long context [37]. We present here the first result of Transformer models being able to solve Path-X and Path-256 (Table 6). We pretrain a transformer on Path-64, and then transfer to Path-X by spatially interpolating the positional embeddings. FlashAttention achieves 61.4 accuracy on Path-X. Additionally, block-sparse FlashAttention enables the Transformers to scale to sequence length 64K, achieving 63.1 accuracy4 on Path-256.\n",
            "\n",
            "Footnote 4: Path-256 requires longer sequences but has relatively shorter paths than Path-X, so it is easier to obtain a higher accuracy.\n",
            "\n",
            "### Benchmarking Attention\n",
            "\n",
            "We vary sequence length and measure runtime and memory usage of FlashAttention and block-sparse FlashAttention against various attention baselines on one A100 GPU with 40 GB HBM, with dropout and a padding mask. We compare against reference implementations for exact attention, approximate attention, and sparse attention. We report a subset of baselines in the main body; Appendix E contains more baselines and full details.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{r|c c}\n",
            "**Model** & Path-X & Path-256 \\\\ \\hline Transformer & ✗ & ✗ \\\\ Linformer [84] & ✗ & ✗ \\\\ Linear Attention [50] & ✗ & ✗ \\\\ Performance [12] & ✗ & ✗ \\\\ Local Attention [80] & ✗ & ✗ \\\\ Reformer [51] & ✗ & ✗ \\\\ SMYF [19] & ✗ & ✗ \\\\ \\hline FlashAttention & **61.4** & ✗ \\\\ Block-sparse FlashAttention & 56.0 & **63.1** \\\\ \\end{tabular}\n",
            "\\end{table}\n",
            "Table 6: We report the first Transformer model that can achieve non-random performance on Path-X and Path-256.\n",
            "\n",
            "Figure 3: **Left: runtime of forward pass + backward pass. Right: attention memory usage.**\n",
            "\n",
            "Runtime.Figure 3 (left) reports the runtime in milliseconds of the forward + backward pass of FlashAttention and block-sparse FlashAttention compared to the baselines in exact, approximate, and sparse attention (exact numbers in Appendix E). Runtime grows quadratically with sequence length, but FlashAttention runs significantly faster than **exact attention** baselines, up to 3\\(\\times\\) faster than the PyTorch implementation. The runtimes of many approximate/sparse attention mechanisms grow linearly with sequence length, but FlashAttention still runs faster than approximate and sparse attention for short sequences due to fewer memory accesses. The **approximate attention** runtimes begin to cross over with FlashAttention at sequences between 512 and 1024. On the other hand, block-sparse FlashAttention is faster than all implementations of exact, sparse, and approximate attention that we know of, across all sequence lengths.\n",
            "\n",
            "Memory Footprint.Figure 3 (right) shows the memory footprint of FlashAttention and block-sparse FlashAttention compared to various exact, approximate, and sparse attention baselines. FlashAttention and block-sparse FlashAttention have the same memory footprint, which grows linearly with sequence length. FlashAttention is up to 20\\(\\times\\) more memory efficient than **exact attention** baselines, and is more memory-efficient than the **approximate attention** baselines. All other algorithms except for Linformer run out of memory on an A100 GPU before 64K, and FlashAttention is still 2\\(\\times\\) more efficient than Linformer.\n",
            "\n",
            "## 5 Limitations and Future Directions\n",
            "\n",
            "We discuss limitations of our approach and future directions. Related work is given in Appendix A.\n",
            "\n",
            "**Compiling to CUDA.** Our current approach to building IO-aware implementations of attention requires writing a new CUDA kernel for each new attention implementation. This requires writing the attention algorithm in a considerably lower-level language than PyTorch, and requires significant engineering effort. Implementations may also not be transferable across GPU architectures. These limitations suggest the need for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and compiling to IO-aware implementations in CUDA--similar to efforts such as Halide in image processing [70].\n",
            "\n",
            "**IO-Aware Deep Learning.** We believe that the IO-aware approach can extend beyond attention. Attention is the most memory-intensive computation in Transformers, but every layer in a deep network touches GPU HBM. We hope our work inspires IO-aware implementations of additional modules. We discuss these potential extensions in Appendix D.\n",
            "\n",
            "**Multi-GPU IO-Aware Methods.** Our IO-aware implementation of attention is optimal within constants for computing attention on a single GPU. However, the attention computation may be parallelizable across multiple GPUs [72]. Using multiple GPUs adds an additional layer to IO analysis--accounting for data transfer between GPUs. We hope our work inspires future work in this direction.\n",
            "\n",
            "#### Acknowledgments\n",
            "\n",
            "Our implementation uses Apex's FMHA code ([https://github.com/WVIDIA/apex/tree/master/apex/contrib/csrc/fmha](https://github.com/WVIDIA/apex/tree/master/apex/contrib/csrc/fmha)) as a starting point. We thank Young-Jun Ko for the in-depth explanation of his FMHA implementation and for his thoughtful answers to our questions about CUDA. We thank Sabri Eyuboglu, Megan Leszczynski, Laurel Orr, Yuhuai Wu, Beidi Chen, and Xun Huang for their constructive feedback and suggestions on early drafts of the paper. We thank Markus Rabe and Charles Staats for helpful discussion of their attention algorithm.\n",
            "\n",
            "We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ARL under No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP & HAI-Azure Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, and members of the Stanford DAWN project: Facebook, Google, and VMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposesnotwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government. Atri Rudra's research is supported by NSF grant CCF-1763481.\n",
            "\n",
            "## References\n",
            "\n",
            "* [1] Alok Aggarwal and S Vitter, Jeffrey. The input/output complexity of sorting and related problems. _Communications of the ACM_, 31(9):1116-1127, 1988.\n",
            "* [2] Irwan Bello. LambdaNetworks: Modeling long-range interactions without attention. _arXiv preprint arXiv:2102.08602_, 2021.\n",
            "* [3] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. _arXiv preprint arXiv:2004.05150_, 2020.\n",
            "* [4] L Susan Blackford, Antoine Petitet, Roldan Pozo, Karin Remington, R Clint Whaley, James Demmel, Jack Dongarra, Iain Duff, Sven Hammarling, Greg Henry, et al. An updated set of basic linear algebra subprograms (blas). _ACM Transactions on Mathematical Software_, 28(2):135-151, 2002.\n",
            "* [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n",
            "* [6] Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. Neural legal judgment prediction in English. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 4317-4323, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1424. URL [https://www.aclweb.org/anthology/P19-1424](https://www.aclweb.org/anthology/P19-1424).\n",
            "* [7] Ilias Chalkidis, Manos Fergadiotis, Dimitrios Tsarapatsanis, Nikolaos Aletras, Ion Androutsopoulos, and Prodromos Malakasiotis. Paragraph-level rationale extraction through regularization: A case study on european court of human rights cases. In _Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics_, Mexico City, Mexico, 2021. Association for Computational Linguistics.\n",
            "* [8] Benjamin Charlier, Jean Feydy, Joan Alexis Glaunes, Francois-David Collin, and Ghislain Durif. Kernel operations on the gpu, with autodiff, without memory overflows. _Journal of Machine Learning Research_, 22(74):1-6, 2021. URL [http://jmlr.org/papers/v22/20-275.html](http://jmlr.org/papers/v22/20-275.html).\n",
            "* [9] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Re. Scatterbrain: Unifying sparse and low-rank attention. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.\n",
            "* [10] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. _arXiv preprint arXiv:1604.06174_, 2016.\n",
            "* [11] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. _arXiv preprint arXiv:1904.10509_, 2019.\n",
            "* [12] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In _International Conference on Learning Representations (ICLR)_, 2020.\n",
            "* [13] Xiang Dai, Ilias Chalkidis, Sune Darkner, and Desmond Elliott. Revisiting transformer-based models for long document classification. _arXiv preprint arXiv:2204.06683_, 2022.\n",
            "* [14] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 2978-2988, 2019.\n",
            "\n",
            "* [15] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher Re. Learning fast algorithms for linear transforms using butterfly factorizations. In _International Conference on Machine Learning (ICML)_, 2019.\n",
            "* [16] Tri Dao, Nimit Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra, and Christopher Re. Kaleidoscope: An efficient, learnable representation for all structured linear maps. In _International Conference on Learning Representations (ICLR)_, 2020.\n",
            "* [17] Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Re. Pixelated butterfly: Simple and efficient sparse training for neural network models. In _International Conference on Learning Representations (ICLR)_, 2022.\n",
            "* [18] Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher Re. Monarch: Expressive structured matrices for efficient and accurate training. In _International Conference on Machine Learning (ICML)_, 2022.\n",
            "* [19] Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrf-efficient attention using asymmetric clustering. _Advances in Neural Information Processing Systems_, 33:6476-6489, 2020.\n",
            "* [20] Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher Re, and Atri Rudra. A two-pronged progress in structured dense matrix vector multiplication. In _Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms_, pages 1060-1079. SIAM, 2018.\n",
            "* [21] Peter J Denning. The working set model for program behavior. _Communications of the ACM_, 11(5):323-333, 1968.\n",
            "* [22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. 2019.\n",
            "* [23] Xin Dong, Shangyu Chen, and Sinno Jialin Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. _arXiv preprint arXiv:1705.07565_, 2017.\n",
            "* [24] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2020.\n",
            "* [25] Y Eidelman and I Gohberg. On a new class of structured matrices. _Integral Equations and Operator Theory_, 34(3):293-324, 1999.\n",
            "* [26] Jean Feydy, Joan Glaunes, Benjamin Charlier, and Michael Bronstein. Fast geometric learning with symbolic matrices. _Advances in Neural Information Processing Systems_, 33, 2020.\n",
            "* [27] Jorg Flum and Martin Grohe. _Parameterized Complexity Theory_. Springer, 2006.\n",
            "* [28] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In _International Conference on Learning Representations_, 2018.\n",
            "* [29] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Stabilizing the lottery ticket hypothesis. _arXiv preprint arXiv:1903.01611_, 2019.\n",
            "* [30] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In _International Conference on Machine Learning_, pages 3259-3269. PMLR, 2020.\n",
            "* [31] Karan Goel, Albert Gu, Chris Donahue, and Christopher Re. It's raw! audio generation with state-space models. In _International Conference on Machine Learning (ICML)_, 2022.\n",
            "* [32] Aaron Gokaslan, Vanya Cohen, Pavlick Ellie, and Stefanie Tellex. Openwebtext corpus, 2019.\n",
            "\n",
            "* [33] Jim Gray, Surajit Chaudhuri, Adam Bosworth, Andrew Layman, Don Reichart, Murali Venkatrao, Frank Pellow, and Hamid Pirahesh. Data cube: A relational aggregation operator generalizing group-by, cross-tab, and sub-totals. _Data mining and knowledge discovery_, 1(1):29-53, 1997.\n",
            "* [34] Andreas Griewank and Andrea Walther. _Evaluating derivatives: principles and techniques of algorithmic differentiation_. SIAM, 2008.\n",
            "* [35] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory with optimal polynomial projections. In _Advances in neural information processing systems (NeurIPS)_, 2020.\n",
            "* [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Re. Combining recurrent, convolutional, and continuous-time models with linear state space layers. _Advances in Neural Information Processing Systems_, 34, 2021.\n",
            "* [37] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In _The International Conference on Learning Representations (ICLR)_, 2022.\n",
            "* [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. _arXiv preprint arXiv:1506.02626_, 2015.\n",
            "* [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In _International Conference on Learning Representations_, 2016.\n",
            "* [40] John Hennessy and David Patterson. Memory hierarchy design. _Computer Architecture: A Quantitative Approach_, pages 390-525, 2003.\n",
            "* [41] Sara Hooker. The hardware lottery. _arXiv preprint arXiv:2009.06489_, 2020.\n",
            "* [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. _arXiv preprint arXiv:2202.10447_, 2022.\n",
            "* [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. _Proceedings of Machine Learning and Systems_, 3:711-732, 2021.\n",
            "* [44] Zhe Jia and Peter Van Sandt. Dissecting the Ampere GPU architecture via microbenchmarking. GPU Technology Conference, 2021.\n",
            "* [45] Zhe Jia, Marco Maggioni, Benjamin Staiger, and Daniele P Scarpazza. Dissecting the nvidia Volta GPU architecture via microbenchmarking. _arXiv preprint arXiv:1804.06826_, 2018.\n",
            "* [46] Zhe Jia, Blake Tillman, Marco Maggioni, and Daniele Paolo Scarpazza. Dissecting the graphcore IPU architecture via microbenchmarking. _arXiv preprint arXiv:1912.03413_, 2019.\n",
            "* [47] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. _Scientific data_, 3(1):1-9, 2016.\n",
            "* [48] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor processing unit. In _Proceedings of the 44th annual international symposium on computer architecture_, pages 1-12, 2017.\n",
            "* [49] Thomas Kailath, Sun-Yuan Kung, and Martin Morf. Displacement ranks of matrices and linear equations. _Journal of Mathematical Analysis and Applications_, 68(2):395-407, 1979.\n",
            "* [50] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In _International Conference on Machine Learning_, pages 5156-5165. PMLR, 2020.\n",
            "\n",
            "* [51] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In _The International Conference on Machine Learning (ICML)_, 2020.\n",
            "* [52] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite BEDRT for self-supervised learning of language representations. In _The International Conference on Learning Representations (ICLR)_, 2020.\n",
            "* [53] Mingzhen Li, Yi Liu, Xiaoyan Liu, Qingxiao Sun, Xin You, Hailong Yang, Zhongzhi Luan, Lin Gan, Guangwen Yang, and Depei Qian. The deep learning compiler: A comprehensive survey. _IEEE Transactions on Parallel and Distributed Systems_, 32(3):708-727, 2020.\n",
            "* [54] Valerii Likhosherstov, Krzysztof Choromanski, Jared Davis, Xingyou Song, and Adrian Weller. Sub-linear memory: How to make performers slim. _arXiv preprint arXiv:2012.11346_, 2020.\n",
            "* [55] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.\n",
            "* [56] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.\n",
            "* [57] Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Linear unified nested attention. _Advances in Neural Information Processing Systems_, 34, 2021.\n",
            "* [58] Peter Mattson, Christine Cheng, Gregory Diamos, Cody Coleman, Paulius Micikevicius, David Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor Bittorf, et al. Mlperf training benchmark. _Proceedings of Machine Learning and Systems_, 2:336-349, 2020.\n",
            "* [59] Frank McSherry, Michael Isard, and Derek G Murray. Scalability! but at what {COST}? In _15th Workshop on Hot Topics in Operating Systems (HotOS XV)_, 2015.\n",
            "* [60] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. _arXiv preprint arXiv:1805.02867_, 2018.\n",
            "* [61] NVIDIA. Nvidia Tesla V100 GPU architecture, 2017.\n",
            "* [62] NVIDIA. Nvidia A100 tensor core GPU architecture, 2020.\n",
            "* [63] NVIDIA. Nvidia H100 tensor core GPU architecture, 2022.\n",
            "* [64] D Stott Parker. Random butterfly transformations with applications in computational linear algebra. 1995.\n",
            "* [65] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.\n",
            "* [66] Markus N Rabe and Charles Staats. Self-attention does not need \\(O(n^{2})\\) memory. _arXiv preprint arXiv:2112.05682_, 2021.\n",
            "* [67] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.\n",
            "* [68] Jack Bae and Ali Razavi. Do transformers need deep long-range memory? In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, Online, July 2020. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/2020.acl-main.672](https://www.aclweb.org/anthology/2020.acl-main.672).\n",
            "* [69] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. In _The International Conference on Learning Representations (ICLR)_, 2020.\n",
            "\n",
            "* [70] Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Fredo Durand, and Saman Amarasinghe. Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. _Acm Sigplan Notices_, 48(6):519-530, 2013.\n",
            "* [71] Raghu Ramakrishnan, Johannes Gehrke, and Johannes Gehrke. _Database management systems_, volume 3. McGraw-Hill New York, 2003.\n",
            "* [72] Benjamin Recht and Christopher Re. Parallel stochastic gradient algorithms for large-scale matrix completion. _Mathematical Programming Computation_, 5(2):201-226, 2013.\n",
            "* [73] Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost. _Advances in Neural Information Processing Systems_, 34, 2021.\n",
            "* [74] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. _Transactions of the Association for Computational Linguistics_, 9:53-68, 2021.\n",
            "* [75] Amit Sabne. XLA: Compiling machine learning for peak performance. 2020.\n",
            "* [76] Victor Sanh, Thomas Wolf, and Alexander M Rush. Movement pruning: Adaptive sparsity by fine-tuning. _arXiv preprint arXiv:2005.07683_, 2020.\n",
            "* [77] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: Training multi-billion parameter language models using model parallelism. _arXiv preprint arXiv:1909.08053_, 2019.\n",
            "* [78] Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep learning. In _Advances in Neural Information Processing Systems_, pages 3088-3096, 2015.\n",
            "* [79] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in transformers. In _Proceedings of the Annual Meeting of the Association for Computational Linguistics_, 2019.\n",
            "* [80] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. In _International Conference on Learning Representations_, 2020.\n",
            "* [81] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. _arXiv preprint arXiv:2009.06732_, 2020.\n",
            "* [82] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.\n",
            "* [83] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet: Scaling transformers to 1,000 layers. _arXiv preprint arXiv:2203.00555_, 2022.\n",
            "* [84] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. _arXiv preprint arXiv:2006.04768_, 2020.\n",
            "* [85] Samuel Williams, Andrew Waterman, and David Patterson. Roofline: an insightful visual performance model for multicore architectures. _Communications of the ACM_, 52(4):65-76, 2009.\n",
            "* [86] Michael E Wolf and Monica S Lam. A data locality optimizing algorithm. In _Proceedings of the ACM SIGPLAN 1991 conference on Programming language design and implementation_, pages 30-44, 1991.\n",
            "\n",
            "* [87] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 38-45, Online, October 2020. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/2020.emnlp-demos.6](https://www.aclweb.org/anthology/2020.emnlp-demos.6).\n",
            "* [88] David P Woodruff. Optimal space lower bounds for all frequency moments. In _SODA_, volume 4, pages 167-175. Citeseer, 2004.\n",
            "* [89] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with lightweight and dynamic convolutions. In _The International Conference on Learning Representations (ICLR)_, 2019.\n",
            "* [90] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystromformer: A nystom-based algorithm for approximating self-attention. In _Proceedings of the AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence_, volume 35, page 14138, 2021.\n",
            "* [91] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 558-567, 2021.\n",
            "* [92] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. _Advances in Neural Information Processing Systems_, 33, 2020.\n",
            "* [93] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. An attention free transformer. _arXiv preprint arXiv:2105.14103_, 2021.\n",
            "* [94] Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. _Advances in Neural Information Processing Systems_, 34, 2021.\n",
            "\n",
            "Related Work\n",
            "\n",
            "**IO-Aware Runtime Optimization.** The broad concept of optimizing for reading and writing to fast/slow memory has a long history in computer science and has been known by many names. We draw the most direct connection to the literature of analyzing I/O complexity in this work [1], but concepts of memory hierarchies are fundamental and has appeared in many forms, from the working set model [21], to data locality [86], to the Roofline model of arithmetic intensity [85], to analyses of scalability [59], to standard textbook treatments of computer architecture [40]. We hope that this work encourages the community to adopt these ideas in more parts of the deep learning stack.\n",
            "\n",
            "**Efficient ML Models with Structured Matrices.** Matrix multiply is the core computational bottleneck of most machine learning models. To reduce the computational complexity, there have been numerous approaches to learn over a more efficient set of matrices. These matrices are called _structured matrices_, which have subquadratic (\\(o(n^{2})\\) for dimension \\(n\\times n\\)) number of parameters and runtime. Most common examples of structured matrices are sparse and low-rank matrices, along with fast transforms commonly encountered in signal processing (Fourier, Chebyshev, sine/cosine, orthogonal polynomials). There have been several more general classes of structured matrices proposed in machine learning: Toeplitz-like [78], low-displacement rank [49], quasi-separable [25]). The butterfly pattern we use for our block-sparse attention is motivated by the fact that butterfly matrices [15, 64] and their products have been shown to be able to express any structured matrices with almost optimal runtime and number of parameters [16, 20]. However, even though structured matrices are efficient in theory, they have not seen wide adoption since it is hard to translate their efficiency to wall-clock speedup since dense unconstrained matrix multiply has very optimize implementation, a phenomenon known as the hardware lottery [41]. Extensions of butterfly matrices [17, 18] aimed to make butterfly matrices more hardware-friendly.\n",
            "\n",
            "**Sparse Training.** Our block-sparse FlashAttention can be seen as a step towards making sparse model training more efficient. Sparse models have seen success in compressing models for inference (pruning) by sparsifying the weight matrices [23, 38, 39, 55, 76]. For model training, the lottery tickets hypothesis [28, 29, 30] suggests that there are a set of small sub-networks derived from a larger dense network that performs as well as the original dense network. Out block-sparse FlashAttention can also be seen as a fixed lottery ticket in the context of attention: we fix the sparsity pattern to be the butterfly pattern through training, and observe that it performs almost as well as the (dense) FlashAttention on the Long-range Arena tasks.\n",
            "\n",
            "**Efficient Transformer.** Transformer-based models have become the most widely-used architecture in natural language processing [22] and computer vision [24, 91]. However, one of their computational bottlenecks is that their time and memory scales quadratic in the sequence length. There are numerous approaches to overcome this bottleneck, including approximation with hashing (i.e., sparse) such as Reformer [51] and Smyrf [19] and with low-rank approximation such as Performer [12, 54]. One can even combine sparse and low-rank approximation for better accuracy (e.g., Longformer [3], BigBird [92], Scatterbrain [9], Long-short transformer [94], Combiner [73]). Other approaches include compressing along the sequence dimension to attend to multiple tokens at once [52, 57, 79, 89]. One can also attend over the states from previous sequences to help lengthen the context (e.g., Transformer-XL [14] and Compressive Transformer [69]). We recommend the survey [81] for more details.\n",
            "\n",
            "There are several lines of work on developing other modules instead of attention to model longer context. HiPPO [35] and its extensions, most notably S4 [31, 36, 37] projects the history on a polynomial basis, allowing accurate reconstruction of the history through state-space models. They combine the strengths of CNNs (efficient training), RNNs (efficient inference), and continuous models (robust to change in sampling rates). LambdaNetworks [2], AFT [93] and FLASH [42] are other attempts at replacing attention in the context of image classification and language modeling.\n",
            "\n",
            "## Appendix B Algorithm Details\n",
            "\n",
            "We first derive the forward and backward passes of attention and show that they can be computed in a memory-efficient manner (requiring extra memory linear instead of quadratic in the sequence length). Though they reduce the amount of extra memory required, naively they still incur quadratic HBM accesses, resulting in slower execution speed. We describe the FlashAttention algorithm to implement both the forward and the backward passes on GPUs that reduces HBM accesses, leading to both faster runtime and smaller memory footprint.\n",
            "\n",
            "### Memory-efficient forward pass\n",
            "\n",
            "The main challenge in making attention memory-efficient is the softmax that couples the columns of \\(\\mathbf{K}\\) (and columns of \\(\\mathbf{V}\\)). Our approach is to compute the softmax normalization constant separately to decouple the columns. This technique [60] has been used in the literature [51, 66] to show that attention computation does not need quadratic _extra_ memory (though the number of HBM accesses is still quadratic, resulting in slow run-time).\n",
            "\n",
            "For simplicity, we omit here the max-shifting step during softmax. The full algorithm in Appendix B.3 contains all the steps.\n",
            "\n",
            "Recall that given input sequences \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{N\\times d}\\), we want to compute the attention output \\(\\mathbf{O}\\in\\mathbb{R}^{N\\times d}\\):\n",
            "\n",
            "\\[\\mathbf{S}=\\mathbf{Q}\\mathbf{K}^{\\top}\\in\\mathbb{R}^{N\\times N},\\quad\\mathbf{ P}=\\operatorname{softmax}(\\mathbf{S})\\in\\mathbb{R}^{N\\times N},\\quad\\mathbf{O}= \\mathbf{PV}\\in\\mathbb{R}^{N\\times d}.\\]\n",
            "\n",
            "We have that \\(S_{ij}=q_{i}^{T}k_{j}\\) where \\(q_{i}\\) and \\(k_{j}\\) are the \\(i\\)-th and \\(j\\)-th columns of \\(\\mathbf{Q}\\) and \\(\\mathbf{K}\\) respectively. Define the normalization constants of softmax:\n",
            "\n",
            "\\[L_{i}=\\sum_{j}e^{q_{i}^{T}k_{j}}. \\tag{1}\\]\n",
            "\n",
            "Let \\(v_{j}\\) be the \\(j\\)-th column of \\(\\mathbf{V}\\), then the \\(i\\)-th columns of the output is\n",
            "\n",
            "\\[o_{i}=P_{i:}\\mathbf{V}=\\sum_{j}P_{ij}v_{j}=\\sum_{j}\\frac{e^{q_{i}^{T}k_{j}}}{L _{i}}v_{j}. \\tag{2}\\]\n",
            "\n",
            "We see that once \\(L_{i}\\) is computed, we can compute \\(o_{i}\\) without extra memory by repeatedly summing \\(\\frac{e^{q_{i}^{T}k_{j}}}{L_{i}}v_{j}\\). Therefore the forward pass can be computed with \\(O(n)\\) extra memory:\n",
            "\n",
            "1. Compute \\(L_{i}\\) for all \\(i\\) according to Eq. (1), which takes \\(O(n)\\) extra memory.\n",
            "2. Compute \\(o_{i}\\) for all \\(i\\) according to Eq. (2), which takes \\(O(d)\\) extra memory.\n",
            "\n",
            "### Memory-efficient backward pass\n",
            "\n",
            "We derive the backward pass of attention and show that it can also be computed with linear memory. Rabe and Staats [66] suggests that the backward pass can be done without quadratic extra memory by applying gradient checkpointing to the memory-efficient forward pass. We instead derive the backward pass explicitly and show how it can be computed in a memory-efficient manner.\n",
            "\n",
            "Suppose that there is a scalar loss function \\(\\phi\\), and let the output gradient be \\(\\mathbf{d}\\mathbf{O}\\in\\mathbb{R}^{n\\times d}\\) (where \\(\\mathbf{d}\\mathbf{O}\\) denotes \\(\\frac{\\partial\\phi}{\\partial\\mathbf{0}}\\)). We want to compute the input gradients \\(\\mathbf{d}\\mathbf{Q},\\mathbf{d}\\mathbf{K},\\mathbf{d}\\mathbf{V}\\in\\mathbb{R}^ {n\\times d}\\) (where \\(\\mathbf{d}\\mathbf{Q},\\mathbf{d}\\mathbf{K},\\mathbf{d}\\mathbf{V}\\) denote \\(\\frac{\\partial\\phi}{\\partial\\mathbf{Q}},\\frac{\\partial\\phi}{\\partial\\mathbf{K} },\\frac{\\partial\\phi}{\\partial\\mathbf{V}}\\) respectively).\n",
            "\n",
            "The gradient \\(\\mathbf{d}\\mathbf{V}\\) is easy to see. Applying reverse-mode autodiff by hand (aka the chain rule), we obtain (in matrix notation) \\(\\mathbf{d}\\mathbf{V}=\\mathbf{P}^{T}\\mathbf{d}\\mathbf{O}\\). Thus:\n",
            "\n",
            "\\[d\\nu_{j}=\\sum_{i}P_{ij}do_{i}=\\sum_{i}\\frac{e^{q_{i}^{T}k_{j}}}{L_{i}}do_{i}. \\tag{3}\\]\n",
            "\n",
            "Since we already computed \\(L_{i}\\), \\(d\\nu_{j}\\) can be computed without extra memory by repeated summing.\n",
            "\n",
            "The gradients \\(\\mathbf{d}\\mathbf{Q}\\) and \\(\\mathbf{d}\\mathbf{K}\\) are a little more complicated. We go through the gradients \\(\\mathbf{d}\\mathbf{P}\\) and \\(\\mathbf{d}\\mathbf{S}\\) first. From Eq. (2), we have that \\(\\mathbf{d}\\mathbf{P}=\\mathbf{d}\\mathbf{O}\\mathbf{V}^{T}\\), and so:\n",
            "\n",
            "\\[dP_{ij}=do_{i}^{T}v_{j}.\\]\n",
            "\n",
            "Recall that \\(P_{i:}=\\operatorname{softmax}(S_{i:})\\). Using the fact that the Jacobian of \\(y=\\operatorname{softmax}(x)\\) is \\(\\operatorname{diag}(y)-y\\mathbf{y}^{T}\\), we have that\n",
            "\n",
            "\\[dS_{i:}=(\\operatorname{diag}(P_{i:})-P_{i:}P_{i:}^{T})dP_{i:}=P_{i:}\\circ dP_{i: }-(P_{i:}^{T}dP_{i:})P_{i:},\\]where \\(\\circ\\) denotes pointwise multiplication.\n",
            "\n",
            "Define\n",
            "\n",
            "\\[D_{i}=P_{i:}^{T}dP_{i:}=\\sum_{j}\\frac{e^{q_{i}^{T}k_{j}}}{L_{i}}d\\sigma_{i}^{T}v_ {j}=d\\sigma_{i}^{T}\\,\\sum_{j}\\frac{e^{q_{i}^{T}k_{j}}}{L_{i}}v_{j}=d\\sigma_{i}^{ T}\\,o_{i}, \\tag{4}\\]\n",
            "\n",
            "then\n",
            "\n",
            "\\[dS_{i:}=P_{i:}\\circ dP_{i:}-D_{i}P_{i:}.\\]\n",
            "\n",
            "Hence\n",
            "\n",
            "\\[dS_{ij}=P_{ij}dP_{ij}-D_{i}P_{ij}=P_{ij}(dP_{ij}-D_{i}).\\]\n",
            "\n",
            "Now we can get the gradients \\(\\mathbf{dQ}\\) and \\(\\mathbf{dK}\\). Recall that \\(S_{ij}=q_{i}^{T}k_{j}\\), so\n",
            "\n",
            "\\[dq_{i}=\\sum_{j}dS_{ij}k_{j}=\\sum_{j}P_{ij}(dP_{ij}-D_{i})k_{j}=\\sum_{j}\\frac{e ^{q_{i}^{T}k_{j}}}{L_{i}}(d\\sigma_{i}^{T}v_{j}-D_{i})k_{j}. \\tag{5}\\]\n",
            "\n",
            "Similarly,\n",
            "\n",
            "\\[dk_{j}=\\sum_{i}dS_{ij}q_{i}=\\sum_{i}P_{ij}(dP_{ij}-D_{i})q_{i}=\\sum_{i}\\frac{e ^{q_{i}^{T}k_{j}}}{L_{i}}(d\\sigma_{i}^{T}v_{j}-D_{i})q_{i}. \\tag{6}\\]\n",
            "\n",
            "Therefore the backward pass can also be computed with \\(O(n)\\) extra memory:\n",
            "\n",
            "1. Compute \\(dv_{j}\\) for all \\(j\\) according to Eq. (3), which takes \\(O(d)\\) extra memory.\n",
            "2. Compute \\(D_{i}\\) for all \\(i\\) according to Eq. (4), which takes \\(O(n)\\) extra memory.\n",
            "3. Compute \\(dq_{i}\\) for all \\(i\\) according to Eq. (5), which takes \\(O(d)\\) extra memory.\n",
            "4. Compute \\(dk_{j}\\) for all \\(j\\) according to Eq. (6), which takes \\(O(d)\\) extra memory.\n",
            "\n",
            "### FlashAttention: Forward Pass\n",
            "\n",
            "We describe the full details of FlashAttention forward pass. Given input sequences \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{N\\times d}\\), we want to compute the attention output \\(\\mathbf{O}\\in\\mathbb{R}^{N\\times d}\\):\n",
            "\n",
            "\\[\\mathbf{S}=\\tau\\mathbf{Q}\\mathbf{K}^{\\top}\\in\\mathbb{R}^{N\\times N },\\quad\\mathbf{S}^{\\text{masked}}=\\textsc{mask}(S)\\in\\mathbb{R}^{N\\times N}, \\quad\\mathbf{P}=\\text{softmax}(\\mathbf{S}^{\\text{masked}})\\in\\mathbb{R}^{N \\times N},\\] \\[\\mathbf{P}^{\\text{dropped}}=\\text{dropout}(\\mathbf{P},p_{\\text{ drop}}),\\quad\\mathbf{O}=\\mathbf{P}^{\\text{dropped}}\\mathbf{V}\\in\\mathbb{R}^{N\\times d},\\]\n",
            "\n",
            "where \\(\\tau\\in\\mathbb{R}\\) is some softmax scaling (typically \\(\\frac{1}{\\sqrt{d}}\\)), mask is some masking function that sets some entries of the input to \\(-\\infty\\) and keep other entries the same (e.g., key padding mask when sequences in the batch don't have the same lengths and are padded), and \\(\\text{dropout}(x,p)\\) applies dropout to \\(x\\) elementwise (i.e., output \\(\\frac{x}{1-p}\\) with probability \\(1-p\\) and output \\(0\\) with probability \\(p\\) for each element \\(x\\)).\n",
            "\n",
            "The full algorithm is in Algorithm 2. We save the output \\(\\mathbf{O}\\), the softmax statistics \\(\\ell\\) and \\(m\\), and the pseudo-random number generator state \\(\\mathcal{R}\\) for the backward pass.\n",
            "\n",
            "```\n",
            "0: Matrices \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{N\\times d}\\) in HBM, on-chip SRAM of size \\(M\\), softmax scaling constant \\(\\tau\\in\\mathbb{R}\\), masking function mask, dropout probability \\(p_{\\mathrm{drop}}\\).\n",
            "1: Initialize the pseudo-random number generator state \\(\\mathcal{R}\\) and save to HBM.\n",
            "2: Set block sizes \\(B_{c}=\\left\\lceil\\frac{M}{4d}\\right\\rceil,B_{r}=\\min\\left(\\left\\lceil\\frac{M}{4d }\\right\\rceil,d\\right)\\).\n",
            "3: Initialize \\(\\mathbf{O}=(0)N_{\\times d}\\in\\mathbb{R}^{N\\times d},\\ell=(0)N\\in\\mathbb{R}^{N}, m=(-\\infty)_{N}\\in\\mathbb{R}^{N}\\) in HBM.\n",
            "4: Divide \\(\\mathbf{Q}\\) into \\(T_{r}=\\left\\lceil\\frac{N}{B_{r}}\\right\\rceil\\) blocks \\(\\mathbf{Q}_{1},\\ldots,\\mathbf{Q}_{T_{r}}\\) of size \\(B_{r}\\times d\\) each, and divide \\(\\mathbf{K},\\mathbf{V}\\) in to \\(T_{c}=\\left\\lceil\\frac{N}{B_{c}}\\right\\rceil\\) blocks \\(\\mathbf{K}_{1},\\ldots,\\mathbf{K}_{T_{c}}\\) and \\(\\mathbf{V}_{1},\\ldots,\\mathbf{V}_{T_{c}}\\), of size \\(B_{c}\\times d\\) each.\n",
            "5: Divide \\(\\mathbf{O}\\) into \\(T_{r}\\) blocks \\(\\mathbf{O}_{i},\\ldots,\\mathbf{O}_{T_{r}}\\) of size \\(B_{r}\\times d\\) each, divide \\(\\ell\\) into \\(T_{r}\\) blocks \\(\\ell_{i},\\ldots,\\ell_{T_{r}}\\) of size \\(B_{r}\\) each, divide \\(m\\) into \\(T_{r}\\) blocks \\(m_{1},\\ldots,m_{T_{r}}\\) of size \\(B_{r}\\) each.\n",
            "6:for\\(1\\leq j\\leq T_{c}\\)do\n",
            "7: Load \\(\\mathbf{K}_{j},\\mathbf{V}_{j}\\) from HBM to on-chip SRAM.\n",
            "8:for\\(1\\leq i\\leq T_{r}\\)do\n",
            "9: Load \\(\\mathbf{Q}_{i},\\mathbf{O}_{i},\\ell_{i},m_{i}\\) from HBM to on-chip SRAM.\n",
            "10: On chip, compute \\(\\mathbf{S}_{ij}=\\tau\\mathbf{Q}_{i}\\mathbf{K}_{j}^{T}\\in\\mathbb{R}^{B_{r} \\times B_{c}}\\).\n",
            "11: On chip, compute \\(\\mathbf{S}_{ij}^{\\mathrm{masked}}=\\textsc{mask}(\\mathbf{S}_{ij})\\).\n",
            "12: On chip, compute \\(\\tilde{m}_{ij}=\\mathrm{rowmax}(\\mathbf{S}_{ij}^{\\mathrm{masked}})\\in \\mathbb{R}^{B_{r}}\\), \\(\\tilde{\\mathbf{P}}_{ij}=\\exp(\\mathbf{S}_{ij}^{\\mathrm{masked}}-\\tilde{m}_{ij })\\in\\mathbb{R}^{B_{r}\\times B_{c}}\\) (pointwise), \\(\\tilde{\\ell}_{ij}=\\mathrm{rowsum}(\\tilde{\\mathbf{P}}_{ij})\\in\\mathbb{R}^{B_{r}}\\).\n",
            "13: On chip, compute \\(m_{i}^{\\mathrm{new}}=\\max(m_{i},\\tilde{m}_{ij})\\in\\mathbb{R}^{B_{r}}\\), \\(\\ell_{i}^{\\mathrm{new}}=e^{m_{i}-m_{i}^{\\mathrm{new}}}\\ell_{i}+e^{\\tilde{m}_{ ij}-m_{i}^{\\mathrm{new}}}\\tilde{\\ell}_{ij}\\in\\mathbb{R}^{B_{r}}\\).\n",
            "14: On chip, compute \\(\\tilde{\\mathbf{P}}_{ij}^{\\mathrm{dropped}}=\\mathrm{dropout}(\\tilde{\\mathbf{P}}_{ij},p_{ \\mathrm{drop}})\\).\n",
            "15: Write \\(\\mathbf{O}_{i}\\leftarrow\\mathrm{diag}(\\ell_{i}^{\\mathrm{new}})^{-1}(\\mathrm{ diag}(\\ell_{i})e^{m_{i}-m_{i}^{\\mathrm{new}}}\\mathbf{O}_{i}+e^{\\tilde{m}_{ij}-m_{i}^{ \\mathrm{new}}}\\tilde{\\mathbf{P}}_{ij}^{\\mathrm{dropped}}\\mathbf{V}_{j})\\) to HBM.\n",
            "16: Write \\(\\ell_{i}\\leftarrow\\ell_{i}^{\\mathrm{new}}\\), \\(m_{i}\\gets m_{i}^{\\mathrm{new}}\\) to HBM.\n",
            "17:endfor\n",
            "18:endfor\n",
            "19: Return \\(\\mathbf{O},\\ell,m,\\mathcal{R}\\).\n",
            "```\n",
            "\n",
            "**Algorithm 2** FlashAttention Forward Pass\n",
            "\n",
            "### FlashAttention: Backward Pass\n",
            "\n",
            "We describe the full details of FlashAttention backward pass. Given input sequences \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{N\\times d}\\), the output \\(\\mathbf{0}\\in\\mathbb{R}^{N\\times d}\\), and the output gradient \\(\\mathbf{d}\\mathbf{O}\\), we want to compute the input gradients \\(\\mathbf{d}\\mathbf{Q},\\mathbf{d}\\mathbf{K},\\mathbf{d}\\mathbf{V}\\in\\mathbb{R}^{ N\\times d}\\).\n",
            "\n",
            "We first describe the standard attention backward pass in Algorithm 3 for completeness.\n",
            "\n",
            "```\n",
            "0: Matrices \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V},\\mathbf{d}\\mathbf{0}\\in\\mathbb{R}^{N\\times d}\\), \\(\\mathbf{P}\\in\\mathbb{R}^{N\\times N}\\) in HBM.\n",
            "1: Load \\(\\mathbf{P},\\mathbf{d}\\mathbf{O}\\) by blocks from HBM, compute \\(\\mathbf{d}\\mathbf{V}=\\mathbf{P}^{\\top}\\mathbf{d}\\mathbf{O}\\in\\mathbb{R}^{N \\times d}\\), write \\(\\mathbf{d}\\mathbf{V}\\) to HBM.\n",
            "2: Load \\(\\mathbf{d}\\mathbf{O},\\mathbf{V}\\) by blocks from HBM, compute \\(\\mathbf{d}\\mathbf{P}=\\mathbf{d}\\mathbf{O}\\mathbf{V}^{\\top}\\in\\mathbb{R}^{N \\times N}\\), write \\(\\mathbf{d}\\mathbf{P}\\) to HBM.\n",
            "3: Read \\(\\mathbf{P},\\mathbf{d}\\mathbf{P}\\) from HBM, compute \\(\\mathbf{d}\\mathbf{S}\\in\\mathbb{R}^{N\\times N}\\) where \\(dS_{ij}=P_{ij}(dP_{ij}-\\sum_{l}P_{il}dP_{il})\\), write \\(\\mathbf{d}\\mathbf{S}\\) to HBM.\n",
            "4: Load \\(\\mathbf{d}\\mathbf{S}\\) and \\(\\mathbf{K}\\) by blocks from HBM, compute \\(\\mathbf{d}\\mathbf{Q}=\\mathbf{d}\\mathbf{S}\\mathbf{K}\\), write \\(\\mathbf{d}\\mathbf{Q}\\) to HBM.\n",
            "5: Load \\(\\mathbf{d}\\mathbf{S}\\) and \\(\\mathbf{Q}\\) by blocks from HBM, compute \\(\\mathbf{d}\\mathbf{K}=\\mathbf{d}\\mathbf{S}^{\\top}\\mathbf{Q}\\), write \\(\\mathbf{d}\\mathbf{K}\\) to HBM.\n",
            "6: Return \\(\\mathbf{d}\\mathbf{d},\\mathbf{d}\\mathbf{K},\\mathbf{d}\\mathbf{V}\\).\n",
            "```\n",
            "\n",
            "**Algorithm 3** Standard Attention Backward Pass\n",
            "\n",
            "We now make two observations about FlashAttention backward pass:\n",
            "\n",
            "1. We do not need to store the dropout mask of size \\(O(N^{2})\\) from the forward pass. Instead, we can save the pseudo-random number generator states from the forward pass and re-generate the dropout mask in the backward pass. This allows us to only use \\(O(N)\\) extra memory.\n",
            "2. When computing the softmax gradient, we use Eq. (4) to compute \\(D_{i}=P_{i;}^{\\top}dP_{i;}\\) without reducing over \\(P_{i;}\\) and \\(dP_{i;}\\) of size \\(N\\) (they might not fit into SRAM). Instead we can rewrite \\(D_{i}=do_{i}^{\\top}o_{i}\\) and compute the dot product between vectors of size \\(d\\).\n",
            "\n",
            "The full FlashAttention backward pass algorithm is in Algorithm 4. Conceptually it is just a block version of the derivation in Appendix B.2.\n",
            "\n",
            "```\n",
            "0: Matrices \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V},\\mathbf{0},\\mathbf{d}\\mathbf{O}\\in\\mathbb{R}^{N \\times d}\\) in HBM, vectors \\(\\ell,m\\in\\mathbb{R}^{N}\\) in HBM, on-chip SRAM of size \\(M\\), softmax scaling constant \\(\\tau\\in\\mathbb{R}\\), masking function mask, dropout probability \\(p_{\\mathrm{drop}}\\), pseudo-random number generator state \\(\\mathcal{R}\\) from the forward pass.\n",
            "1: Set the pseudo-random number generator state to \\(\\mathcal{R}\\).\n",
            "2: Set block sizes \\(B_{c}=\\left\\lceil\\frac{M}{d\\ell}\\right\\rceil,B_{r}=\\min\\left(\\left\\lceil\\frac{ M}{d\\ell}\\right\\rceil,d\\right)\\).\n",
            "3: Divide \\(\\mathbf{Q}\\) into \\(T_{r}=\\left\\lceil\\frac{N}{B_{r}}\\right\\rceil\\) blocks \\(\\mathbf{Q}_{1},\\ldots,\\mathbf{Q}_{T_{r}}\\) of size \\(B_{r}\\times d\\) each, and divide \\(\\mathbf{K},\\mathbf{V}\\) in to \\(T_{c}=\\left\\lceil\\frac{N}{B_{c}}\\right\\rceil\\) blocks \\(\\mathbf{K}_{1},\\ldots,\\mathbf{K}_{T_{c}}\\) and \\(\\mathbf{V}_{1},\\ldots,\\mathbf{V}_{T_{c}}\\), of size \\(B_{c}\\times d\\) each.\n",
            "4: Divide \\(\\mathbf{O}\\) into \\(T_{r}\\) blocks \\(\\mathbf{O}_{i},\\ldots,\\mathbf{O}_{T_{r}}\\) of size \\(B_{r}\\times d\\) each, divide \\(\\mathbf{d}\\mathbf{O}\\) into \\(T_{r}\\) blocks \\(\\mathbf{d}\\mathbf{O}_{i},\\ldots,\\mathbf{d}\\mathbf{O}_{T_{r}}\\) of size \\(B_{r}\\times d\\) each, divide \\(\\ell\\) into \\(T_{r}\\) blocks \\(\\ell_{i},\\ldots,\\ell_{T_{r}}\\) of size \\(B_{r}\\) each, divide \\(m\\) into \\(T_{r}\\) blocks \\(m_{1},\\ldots,m_{T_{r}}\\) of size \\(B_{r}\\) each.\n",
            "5: Initialize \\(\\mathbf{d}\\mathbf{Q}=(0)_{N\\times d}\\) in HBM and divide it into \\(T_{r}\\) blocks \\(\\mathbf{d}\\mathbf{Q}_{1},\\ldots,\\mathbf{d}\\mathbf{Q}_{T_{r}}\\) of size \\(B_{r}\\times d\\) each. Initialize \\(\\mathbf{d}\\mathbf{K}=(0)_{N\\times d},\\mathbf{d}\\mathbf{V}=(0)_{N\\times d}\\) in HBM and divide \\(\\mathbf{d}\\mathbf{K},\\mathbf{d}\\mathbf{V}\\) in to \\(T_{c}\\) blocks \\(\\mathbf{d}\\mathbf{K}_{1},\\ldots,\\mathbf{d}\\mathbf{K}_{T_{c}}\\) and \\(\\mathbf{d}\\mathbf{V}_{1},\\ldots,\\mathbf{d}\\mathbf{V}_{T_{c}}\\), of size \\(B_{c}\\times d\\) each.\n",
            "6:for\\(1\\leq j\\leq T_{c}\\)do\n",
            "7: Load \\(\\mathbf{K}_{j},\\mathbf{V}_{j}\\) from HBM to on-chip SRAM.\n",
            "8: Initialize \\(\\tilde{\\mathbf{d}}\\tilde{\\mathbf{K}}_{j}=(0)_{B_{c}\\times d},\\tilde{\\mathbf{ V}}_{j}=(0)_{B_{c}\\times d}\\) on SRAM.\n",
            "9:for\\(1\\leq i\\leq T_{r}\\)do\n",
            "10: Load \\(\\mathbf{Q}_{i},\\mathbf{O}_{i},\\mathbf{d}\\mathbf{Q}_{i},\\mathbf{d}\\mathbf{Q}_{ \\ell},i_{m}\\) from HBM to on-chip SRAM.\n",
            "11: On chip, compute \\(\\mathbf{S}_{ij}=\\tau\\mathbf{Q}_{i}\\mathbf{K}_{j}^{T}\\in\\mathbb{R}^{B_{r} \\times B_{c}}\\).\n",
            "12: On chip, compute \\(\\mathbf{S}_{ij}^{\\mathrm{masked}}=\\textsc{mask}(\\mathbf{S}_{ij})\\).\n",
            "13: On chip, compute \\(\\mathbf{P}_{ij}=\\mathrm{diag}(l_{i})^{-1}\\exp(\\mathbf{S}_{ij}^{\\mathrm{masked}}-m_{i})\\in \\mathbb{R}^{B_{r}\\times B_{c}}\\).\n",
            "14: On chip, compute dropout mask \\(\\mathbf{Z}_{ij}\\in\\mathbb{R}^{B_{r}\\times B_{c}}\\) where each entry has value \\(\\frac{1}{1-p_{\\mathrm{drop}}}\\) with probability \\(1-p_{\\mathrm{drop}}\\) and value \\(0\\) with probability \\(p_{\\mathrm{drop}}\\).\n",
            "15: On chip, compute \\(\\mathbf{p}_{ij}^{\\mathrm{dropped}}=\\mathbf{P}_{ij}\\circ\\mathbf{Z}_{ij}\\) (pointwise multiply).\n",
            "16: On chip, compute \\(\\tilde{\\mathbf{d}}\\tilde{\\mathbf{V}}_{j}\\leftarrow\\tilde{\\mathbf{d}}\\tilde{ \\mathbf{V}}_{j}+(\\mathbf{P}_{ij}^{\\mathrm{dropped}})^{\\tau}\\mathbf{d}\\mathbf{O}_ {i}\\in\\mathbb{R}^{B_{c}\\times d}\\).\n",
            "17: On chip, compute \\(\\mathbf{d}\\mathbf{P}_{ij}^{\\mathrm{dropped}}=\\mathbf{d}\\mathbf{O}_{i}\\mathbf{ V}_{j}^{T}\\in\\mathbb{R}^{B_{r}\\times B_{c}}\\).\n",
            "18: On chip, compute \\(\\mathbf{d}\\mathbf{P}_{ij}=\\mathbf{d}\\mathbf{P}_{ij}^{\\mathrm{dropped}}\\circ \\mathbf{Z}_{ij}\\) (pointwise multiply).\n",
            "19: On chip, compute \\(D_{i}=\\mathrm{rowsum}(\\mathbf{d}\\mathbf{O}_{i}\\circ\\mathbf{O}_{i})\\in \\mathbb{R}^{B_{r}}\\).\n",
            "20: On chip, compute \\(\\mathbf{d}\\mathbf{S}_{ij}=\\mathbf{P}_{ij}\\circ(\\mathbf{d}\\mathbf{P}_{ij}-D_{i}) \\in\\mathbb{R}^{B_{r}\\times B_{c}}\\).\n",
            "21: Write \\(\\mathbf{d}\\mathbf{Q}_{i}\\leftarrow\\mathbf{d}\\mathbf{Q}_{i}+\\tau\\mathbf{d} \\mathbf{S}_{ij}\\mathbf{K}_{j}\\in\\mathbb{R}^{B_{r}\\times d}\\) to HBM.\n",
            "22: On chip, compute \\(\\tilde{\\mathbf{d}}\\tilde{\\mathbf{K}}_{j}\\leftarrow\\tilde{\\mathbf{d}}\\tilde{ \\mathbf{K}}_{j}+\\tau\\mathbf{d}\\mathbf{S}_{ij}^{\\mathrm{d}}\\mathbf{Q}_{i}\\in \\mathbb{R}^{B_{c}\\times d}\\).\n",
            "23:endfor\n",
            "24: Write \\(\\mathbf{d}\\mathbf{K}_{j}\\leftarrow\\tilde{\\mathbf{d}}\\tilde{\\mathbf{K}}_{j}, \\mathbf{d}\\mathbf{V}_{j}\\leftarrow\\tilde{\\mathbf{d}}\\tilde{\\mathbf{V}}_{j}\\) to HBM.\n",
            "25:endfor\n",
            "26: Return \\(\\mathbf{d}\\mathbf{Q},\\mathbf{d}\\mathbf{K},\\mathbf{d}\\mathbf{V}\\).\n",
            "```\n",
            "\n",
            "**Algorithm 4** FlashAttention Backward Pass\n",
            "\n",
            "We see that similar to the forward pass, the backward pass performs \\(O(N^{2})\\) FLOPs and only requires \\(O(N)\\) extra memory beyond inputs, output, output gradient, and input gradients.\n",
            "\n",
            "We analyze the IO-complexity of the backward pass, similar to the forward pass (Theorem 2).\n",
            "\n",
            "**Theorem 5**.: _Let \\(N\\) be the sequence length, \\(d\\) be the head dimension, and \\(M\\) be size of SRAM with \\(d\\leq M\\leq Nd\\). Standard attention (Algorithm 0) backward pass requires \\(\\Theta(Nd+N^{2})\\) HBM accesses, while FlashAttention backward pass (Algorithm 4) requires \\(\\Theta(N^{2}d^{2}M^{-1})\\) HBM accesses._\n",
            "\n",
            "The proof is in Appendix C.\n",
            "\n",
            "### Comparison with Rabe and Staats [66]\n",
            "\n",
            "We describe here some similarities and differences between our FlashAttention algorithm and the algorithm of Rabe and Staats [66].\n",
            "\n",
            "Conceptually, both FlashAttention and Rabe and Staats [66] operate on blocks of the attention matrix using the well-established technique of tiling (or softmax scaling) [51, 60]. To reduce the memory footprint, both methods avoid storing the large attention matrix in the forward pass and recompute it in the backward pass.\n",
            "\n",
            "The first major difference is that Rabe and Staats [66] focuses on the reducing the total memory footprint (maximum amount of GPU memory required) while FlashAttention focuses on reducing memory accesses (the number of memory reads/writes). As mentioned in Section 2, the amount of memory access is the primary determining factor of runtime. Reducing memory accesses also necessarily reduces the total amount of memory required (e.g., if an operation incurs \\(A\\) memory accesses, then its total memory requirement is at most \\(A\\)). As a result, FlashAttention is faster than standard attention (2-4\\(\\times\\)) while Rabe and Staats [66] is around the same speed or slightly slower than standard attention. In terms of total memory required, both methods offer substantial memory saving.\n",
            "\n",
            "The second difference between the two methods is the way information is summarized from each block to pass to the next block. Rabe and Staats [66] summarizes each block with its temporary output along with the softmax normalization statistics. At the end of the forward pass, the temporary outputs of all the blocks are combined using the statistics to produce the final output. FlashAttention instead incrementally updates the output (Algorithm 1 line 12) after processing each block, so only one copy of the output is needed (instead of \\(K\\) copies for \\(K\\) blocks). This means that FlashAttention has smaller total memory requirement compared to Rabe and Staats [66].\n",
            "\n",
            "The final major difference is the way the backward pass is computed. Rabe and Staats [66] uses gradient checkpointing to recompute the attention matrix and the temporary output of each block. FlashAttention instead simplifies the backward pass analytically (Appendices B.2 and B.4). It only recomputes the attention matrix and does not recompute the temporary output of each block. This reduces the memory requirement for the backward pass and yields speedup.\n",
            "\n",
            "## Appendix C Proofs\n",
            "\n",
            "Proof of Theorem 1.: We first count the number of FLOPs and extra memory required.\n",
            "\n",
            "The dominating FLOPs are from matrix multiplication. In the inner loop, (Algorithm 1 line 9), we compute \\(\\mathbf{Q}_{i}\\mathbf{K}_{j}^{\\top}\\in\\mathbb{R}^{B_{r}\\times B_{c}}\\) for \\(\\mathbf{Q}_{i}\\in\\mathbb{R}^{B_{r}\\times d}\\) and \\(\\mathbf{K}_{j}\\in\\mathbb{R}^{B_{c}\\times d}\\), which takes \\(O(B_{r}B_{c}d)\\) FLOPs. We also compute (Algorithm 1 line 12) \\(\\tilde{\\mathbf{P}}_{ij}\\mathbf{V}_{j}\\in\\mathbb{R}^{B_{r}\\times d}\\) for \\(\\tilde{\\mathbf{P}}_{ij}\\in\\mathbb{R}^{B_{r}\\times B_{c}}\\) and \\(\\mathbf{V}_{j}\\in\\mathbb{R}^{B_{c}\\times d}\\), which takes \\(O(B_{r}B_{c}d)\\) FLOPs. We execute the inner loops \\(T_{c}T_{r}=\\left\\lceil\\frac{N}{B_{c}}\\right\\rceil\\left\\lceil\\frac{N}{B_{r}}\\right\\rceil\\) times. Therefore the total number of FLOPs is\n",
            "\n",
            "\\[O\\left(\\frac{N^{2}}{B_{c}B_{r}}B_{r}B_{c}d\\right)=O(N^{2}d).\\]\n",
            "\n",
            "In terms of extra memory required, we see that we need \\(O(N)\\) memory to store the statistics \\((\\ell,m)\\).\n",
            "\n",
            "We now prove the algorithm's correctness by induction on \\(j\\) for \\(0\\leq j\\leq T_{c}\\). Let \\(\\mathbf{K}_{:j}\\in\\mathbb{R}^{jB_{c}\\times d}\\) be the first \\(jB_{c}\\) rows of \\(\\mathbf{K}\\), and similarly \\(\\mathbf{V}_{:j}\\in\\mathbb{R}^{jB_{c}\\times d}\\) the the first \\(jB_{c}\\) rows of \\(\\mathbf{V}\\). Let \\(\\mathbf{S}_{::,j}=\\mathbf{Q}\\mathbf{K}_{:j}^{\\top}\\in\\mathbb{R}^{N\\times jB_{c}}\\), and \\(\\mathbf{P}_{:,:j}=\\text{softmax}(\\mathbf{S}_{:,:j})\\in\\mathbb{R}^{N\\times jB_{ c}}\\) (softmax applied row-wise). Let \\(m^{j},\\ell^{(j)},\\mathbf{O}^{(j)}\\) be the values of \\(m,\\ell,\\mathbf{O}\\) in HBM after the \\(j\\)-th iteration of the outer loop (Algorithm 1 line 5). (Note that these values of \\(m,\\ell,\\mathbf{O}\\) are updated after each iteration of the outer loop.) We want to show that after the \\(j\\)-th iteration of the outer loop, we have computed in HBM:\n",
            "\n",
            "\\[m^{(j)}=\\text{rowmax}(\\mathbf{S}_{:,:j})\\in\\mathbb{R}^{N},\\quad\\ell^{(j)}= \\text{rowsum}(\\exp(\\mathbf{S}_{:,:j}-m^{(j)}))\\in\\mathbb{R}^{N},\\quad\\mathbf{ O}^{(j)}=\\mathbf{P}_{:,:j}\\mathbf{V}_{:j}\\in\\mathbb{R}^{N\\times d}.\\]\n",
            "\n",
            "Based on our initialization (Algorithm 1 line 2), this claim is true for \\(j=0\\) (i.e., before the any iteration of the outer loop is executed). Suppose that the claim holds for some \\(j=0,\\dots,T_{c}-1\\). We want to show that the claim also holds for \\(j+1\\). Indeed, when we update the statistics in the inner loop (Algorithm 1 line 10)on the \\((j+1)\\)-th iteration of the outer loop, we update \\(m^{\\,(j+1)}=\\max(m^{\\,(j)},\\tilde{m})\\) where \\(\\tilde{m}\\in\\mathbb{R}^{N}\\) is the row-max of \\(\\mathbf{S}_{:,j:j+1}\\), the slice of \\(\\mathbf{S}\\) from column \\(jB_{c}\\) to column \\((j+1)B_{c}-1\\). This implies that\n",
            "\n",
            "\\[m^{\\,(j+1)}=\\operatorname{rowmax}(\\mathbf{S}_{:,:j+1})\\in\\mathbb{R}^{N}.\\]\n",
            "\n",
            "Similarly, we update\n",
            "\n",
            "\\[\\ell^{(j+1)}=e^{m^{\\,(j)}-m^{\\,(j+1)}}\\ell^{(j)}+e^{\\tilde{m}-m^{\\,(j+1)}}\\tilde {\\ell},\\]\n",
            "\n",
            "where \\(\\tilde{\\ell}=\\operatorname{rowsum}(\\exp(\\mathbf{S}_{:,:j:j+1}-\\tilde{m})) \\in\\mathbb{R}^{N}\\). By the same algebraic manipulation in Section 3.1, we obtain:\n",
            "\n",
            "\\[\\ell^{(j+1)}=\\operatorname{rowsum}(\\exp(\\mathbf{S}_{:,:j+1}-m^{\\,(j+1)}))\\in \\mathbb{R}^{N}.\\]\n",
            "\n",
            "Let \\(\\mathbf{V}_{j:j+1}\\) be the slice of \\(\\mathbf{V}\\) from column \\(jB_{c}\\) to column \\((j+1)B_{c}-1\\), we also update:\n",
            "\n",
            "\\[\\mathbf{O}^{(j+1)} =\\operatorname{diag}(\\ell^{(j+1)})^{-1}(\\operatorname{diag}(\\ell ^{(j)})e^{m^{\\,(j)}-m^{\\,(j+1)}}\\mathbf{O}^{(j)}+e^{\\tilde{m}-m^{\\,(j+1)}}\\exp (\\mathbf{S}_{j:j+1}-\\tilde{m})\\mathbf{V}_{j:j+1})\\] \\[=\\operatorname{diag}(\\ell^{(j+1)})^{-1}(\\operatorname{diag}(\\ell ^{(j)})e^{m^{\\,(j)}-m^{\\,(j+1)}}\\mathbf{P}_{:,:j}\\mathbf{V}_{:j}+e^{-m^{\\,(j+1 )}}\\exp(\\mathbf{S}_{j:j+1})\\mathbf{V}_{j:j+1})\\] \\[=\\operatorname{diag}(\\ell^{(j+1)})^{-1}(\\operatorname{diag}(\\ell ^{(j)})e^{m^{\\,(j)}-m^{\\,(j+1)}}\\operatorname{diag}(\\ell^{(j)})\\exp(\\mathbf{S} _{:,:j}-m^{\\,(j)})\\mathbf{V}_{:j}+e^{-m^{\\,(j+1)}}\\exp(\\mathbf{S}_{j:j+1}) \\mathbf{V}_{j:j+1})\\] \\[=\\operatorname{diag}(\\ell^{(j+1)})^{-1}(e^{-m^{\\,(j+1)}}\\exp( \\mathbf{S}_{:,:j})\\mathbf{V}_{:j}+e^{-m^{\\,(j+1)}}\\exp(\\mathbf{S}_{j:j+1}) \\mathbf{V}_{j:j+1})\\] \\[=\\operatorname{diag}(\\ell^{(j+1)})^{-1}(\\exp(\\mathbf{S}_{:,:j}-m^ {\\,(j+1)})\\mathbf{V}_{:j}+\\exp(\\mathbf{S}_{j:j+1}-m^{\\,(j+1)})\\mathbf{V}_{j:j +1})\\] \\[=\\operatorname{diag}(\\ell^{(j+1)})^{-1}\\left(\\exp\\left(\\begin{bmatrix} \\mathbf{S}_{:,:j}&\\mathbf{S}_{j:j+1}\\end{bmatrix}-m^{\\,(j+1)}\\right)\\right) \\begin{bmatrix}\\mathbf{V}_{:,j}\\\\ \\mathbf{V}_{j:j+1}\\end{bmatrix}\\] \\[=\\operatorname{softmax}(\\mathbf{S}_{:,j+1})\\mathbf{V}_{:j+1}.\\]\n",
            "\n",
            "We then see that the claim is also true for \\(j+1\\). By induction, the claim is true for all \\(j=0,\\ldots,T_{c}\\).\n",
            "\n",
            "When \\(j=T_{c}\\), we conclude that the final value of \\(\\mathbf{O}\\) in HBM is \\(\\operatorname{softmax}(\\mathbf{S})\\mathbf{V}=\\operatorname{softmax}( \\mathbf{Q}\\mathbf{K}^{\\top})\\mathbf{V}\\).\n",
            "\n",
            "Proof of Theorem 2.: We first analyze the IO complexity of standard attention implementation. The inputs \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{N\\times d}\\) reside in HBM, and the at the end of the algorithm the output \\(\\mathbf{O}\\in\\mathbb{R}^{N\\times d}\\) is written to HBM.\n",
            "\n",
            "In the first step of computing the matrix multiply \\(\\mathbf{S}=\\mathbf{Q}\\mathbf{K}^{\\top}\\), the inputs \\(\\mathbf{Q},\\mathbf{K}\\) are read from HBM and the output \\(\\mathbf{S}\\in\\mathbb{R}^{N\\times N}\\) is written to HBM (Algorithm 0 line 1). This incurs \\(\\Theta(Nd+N^{2})\\) HBM accesses.\n",
            "\n",
            "In the second step of computing \\(\\mathbf{P}=\\operatorname{softmax}(\\mathbf{S})\\), the input \\(\\mathbf{S}\\) is read from HBM and the output \\(\\mathbf{P}\\) is written to HBM (Algorithm 0 line 2). This incurs \\(\\Theta(N^{2})\\) HBM accesses.\n",
            "\n",
            "In the last step of computing \\(\\mathbf{O}=\\mathbf{P}\\mathbf{V}\\), the inputs \\(\\mathbf{P},\\mathbf{V}\\) are read from global memory and the output \\(\\mathbf{O}\\) is written to HBM (Algorithm 0 line 3). This incurs \\(\\Theta(Nd+N^{2})\\) HBM accesses.\n",
            "\n",
            "Overall, standard attention implementation requires \\(\\Theta(Nd+N^{2})\\) global memory accesses.\n",
            "\n",
            "We now analyze the IO complexity of streaming attention.\n",
            "\n",
            "Following Algorithm 1, we see that each element of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) is loaded from HBM once (Algorithm 1 line 6). We make \\(T_{c}\\) passes over \\(\\mathbf{Q}\\) and \\(\\mathbf{O}\\), each pass loading all of \\(\\mathbf{Q}\\) and all of \\(\\mathbf{O}\\) to HBM (Algorithm 1 line 8). Therefore the number of HBM accesses is \\(\\Theta\\left(Nd+NdT_{c}\\right)=\\Theta(NdT_{c})\\).\n",
            "\n",
            "We derive the conditions on the block sizes \\(B_{c}\\) and \\(B_{r}\\). We need the blocks \\(\\mathbf{K}_{j}\\) and \\(\\mathbf{V}_{j}\\) of size \\(B_{c}\\times d\\) to fit into on-chip memory, which translates to:\n",
            "\n",
            "\\[B_{c}d=O(M)\\Leftrightarrow B_{c}=O\\left(\\frac{M}{d}\\right).\\]\n",
            "\n",
            "Similarly, we need the blocks \\(\\mathbf{Q}_{i},\\mathbf{O}_{i}\\) of size \\(B_{r}\\times d\\) to fit into on-chip memory, which translates to:\n",
            "\n",
            "\\[B_{r}d=O(M)\\Leftrightarrow B_{r}=O\\left(\\frac{M}{d}\\right).\\]\n",
            "\n",
            "Finally, we need the block \\(\\mathbf{S}_{ij}\\) of size \\(B_{r}\\times B_{c}\\) to fit into on-chip memory, which translates to:\n",
            "\n",
            "\\[B_{r}B_{c}=O(M).\\]We therefore set:\n",
            "\n",
            "\\[B_{c}=\\Theta\\left(\\frac{M}{d}\\right),\\qquad B_{r}=\\Theta\\left(\\min\\left(\\frac{M}{d },\\frac{M}{B_{c}}\\right)\\right)=\\Theta\\left(\\min\\left(\\frac{M}{d},d\\right) \\right).\\]\n",
            "\n",
            "We then have:\n",
            "\n",
            "\\[T_{c}=\\frac{N}{B_{c}}=\\Theta\\left(\\frac{Nd}{M}\\right).\\]\n",
            "\n",
            "As a result, the number of HBM accesses is:\n",
            "\n",
            "\\[\\Theta\\left(NdT_{c}\\right)=\\Theta\\left(\\frac{N^{2}d^{2}}{M}\\right).\\]\n",
            "\n",
            "Proof of Proposition 3.: For contradiction, suppose that there exists an algorithm that computes exact attention where the number for HBM access for all \\(M\\in[d,Nd]\\) is\n",
            "\n",
            "\\[o\\left(\\frac{N^{2}d^{2}}{M}\\right).\\]\n",
            "\n",
            "In the regime of \\(M=\\Theta(Nd)\\), this results in the number of HBM accesses:\n",
            "\n",
            "\\[o\\left(\\frac{N^{2}d^{2}}{Nd}\\right)=o(Nd).\\]\n",
            "\n",
            "However, the input to attention (matrices \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\)) and the output \\(\\mathbf{O}\\) have size \\(Nd\\) and they start out being in HBM, so if the algorithm computes exact attention it must incur at least \\(\\mathbf{\\Omega}(Nd)\\) HBM accesses. This is a contradiction. \n",
            "\n",
            "Proof of Theorem 5.: The IO complexity of the attention backward is very similar to the IO complexity of the attention forward (Theorem 2). Here we provide a sketch of the proof.\n",
            "\n",
            "We first analyze the IO complexity of standard attention backward pass. The inputs \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V},\\mathbf{d}\\mathbf{O}\\in\\mathbb{R}^{N\\times d}\\) reside in HBM, and the at the end of the algorithm the outputs \\(\\mathbf{d}\\mathbf{Q},\\mathbf{d}\\mathbf{K},\\mathbf{dV}\\in\\mathbb{R}^{N\\times d}\\) are written to HBM.\n",
            "\n",
            "At each step of the standard attention backward pass, one needs to load inputs of size \\(Nd\\) or \\(N^{2}\\) from HBM, and needs to write the outputs of size \\(N^{2}\\) or \\(Nd\\) to HBM. This incurs \\(\\Theta(Nd+N^{2})\\) HBM accesses.\n",
            "\n",
            "We now analyze the IO complexity of FlashAttention backward pass.\n",
            "\n",
            "Similar to Theorem 2, we see that each element of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) is loaded from HBM once. Each element of \\(\\mathbf{d}\\mathbf{K}\\) and \\(\\mathbf{dV}\\) is only written to HBM once. We make \\(T_{c}\\) passes over \\(\\mathbf{Q},\\mathbf{O},\\mathbf{d}\\mathbf{O}\\), each pass loading all of \\(\\mathbf{Q},\\mathbf{O},\\mathbf{d}\\mathbf{O}\\) to HBM. We also make \\(T_{c}\\) passes over \\(\\mathbf{d}\\mathbf{Q}\\), each pass reading/writing all of \\(\\mathbf{d}\\mathbf{Q}\\) from/to HBM. Therefore the number of HBM accesses is \\(\\Theta\\left(Nd+NdT_{c}\\right)=\\Theta(NdT_{c})\\).\n",
            "\n",
            "As in the proof of Theorem 2, the constraints on the block sizes are that:\n",
            "\n",
            "\\[B_{c}=\\Theta\\left(\\frac{M}{d}\\right),\\qquad B_{r}=\\Theta\\left(\\min\\left( \\frac{M}{d},d\\right)\\right).\\]\n",
            "\n",
            "We then have:\n",
            "\n",
            "\\[T_{c}=\\frac{N}{B_{c}}=\\Theta\\left(\\frac{Nd}{M}\\right).\\]\n",
            "\n",
            "As a result, the number of HBM accesses is:\n",
            "\n",
            "\\[\\Theta\\left(NdT_{c}\\right)=\\Theta\\left(\\frac{N^{2}d^{2}}{M}\\right).\\]```\n",
            "0: Matrices \\(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{N\\times d}\\) in HBM, on-chip SRAM of size \\(M\\), softmax scaling constant \\(\\tau\\in\\mathbb{R}\\), masking function mask, dropout probability \\(p_{\\text{drop}}\\), block sizes \\(B_{c}=\\left\\lceil\\frac{M}{4d}\\right\\rceil,B_{r}=\\min\\left(\\left\\lceil\\frac{M}{4d }\\right\\rceil,d\\right)\\), block sparsity mask \\(M\\in\\{0,1\\}^{N/B_{r}\\times N/B_{c}}\\)..\n",
            "1: Initialize the pseudo-random number generator state \\(\\mathcal{R}\\) and save to HBM.\n",
            "2: Initialize \\(\\mathbf{O}=(0)_{N\\times d}\\in\\mathbb{R}^{N\\times d},\\ell=(0)_{N}\\in\\mathbb{R}^ {N},m=(-\\infty)_{N}\\in\\mathbb{R}^{N}\\) in HBM.\n",
            "3: Divide \\(\\mathbf{Q}\\) into \\(T_{r}=\\left\\lceil\\frac{N}{B_{r}}\\right\\rceil\\) blocks \\(\\mathbf{Q}_{1},\\ldots,\\mathbf{Q}_{T_{r}}\\) of size \\(B_{r}\\times d\\) each, and divide \\(\\mathbf{K},\\mathbf{V}\\) in to \\(T_{c}=\\left\\lceil\\frac{N}{B_{c}}\\right\\rceil\\) blocks \\(\\mathbf{K}_{1},\\ldots,\\mathbf{K}_{T_{c}}\\) and \\(\\mathbf{V}_{1},\\ldots,\\mathbf{V}_{T_{c}}\\), of size \\(B_{c}\\times d\\) each.\n",
            "4: Divide \\(\\mathbf{O}\\) into \\(T_{r}\\) blocks \\(\\mathbf{O}_{i},\\ldots,\\mathbf{O}_{T_{r}}\\) of size \\(B_{r}\\times d\\) each, divide \\(\\ell\\) into \\(T_{r}\\) blocks \\(\\ell_{i},\\ldots,\\ell_{T_{r}}\\) of size \\(B_{r}\\) each, divide \\(m\\) into \\(T_{r}\\) blocks \\(m_{1},\\ldots,m_{T_{r}}\\) of size \\(B_{r}\\) each.\n",
            "5:for\\(1\\leq j\\leq T_{c}\\)do\n",
            "6: Load \\(\\mathbf{K}_{j},\\mathbf{V}_{j}\\) from HBM to on-chip SRAM.\n",
            "7:for\\(1\\leq i\\leq T_{r}\\)do\n",
            "8:if\\(M_{ij}\\neq 0\\)then\n",
            "9: Load \\(\\mathbf{Q}_{i},\\mathbf{O}_{i},\\ell_{i},m_{i}\\) from HBM to on-chip SRAM.\n",
            "10: On chip, compute \\(\\mathbf{S}_{ij}=\\tau\\mathbf{Q}_{i}\\mathbf{K}_{j}^{T}\\in\\mathbb{R}^{B_{r} \\times B_{c}}\\).\n",
            "11: On chip, compute \\(\\tilde{\\mathbf{S}}_{ij}^{\\text{masked}}=\\text{mask}(\\mathbf{S}_{ij})\\).\n",
            "12: On chip, compute \\(\\tilde{m}_{ij}=\\text{rowmax}(\\mathbf{S}_{ij}^{\\text{masked}})\\in\\mathbb{R}^ {B_{r}}\\), \\(\\tilde{\\mathbf{P}}_{ij}=\\exp(\\mathbf{S}_{ij}^{\\text{masked}}-\\tilde{m}_{ij}) \\in\\mathbb{R}^{B_{r}\\times B_{c}}\\) (pointwise), \\(\\tilde{\\ell}_{ij}=\\text{rowsum}(\\tilde{\\mathbf{P}}_{ij})\\in\\mathbb{R}^{B_{r}}\\).\n",
            "13: On chip, compute \\(m_{i}^{\\text{new}}=\\max(m_{i},\\tilde{m}_{ij})\\in\\mathbb{R}^{B_{r}}\\), \\(\\ell_{i}^{\\text{new}}=e^{m_{i}-m_{i}^{\\text{new}}}\\ell_{i}+e^{\\tilde{m}_{ij}- m_{i}^{\\text{new}}}\\tilde{\\ell}_{ij}\\in\\mathbb{R}^{B_{r}}\\).\n",
            "14: On chip, compute \\(\\tilde{\\mathbf{P}}_{ij}^{\\text{dropped}}=\\text{dropout}(\\tilde{\\mathbf{P}}_{ij},p_{ \\text{drop}})\\).\n",
            "15: Write \\(\\mathbf{O}_{i}\\leftarrow\\text{diag}(\\ell_{i}^{\\text{new}})^{-1}(\\text{diag}( \\ell_{i})e^{m_{i}-m_{i}^{\\text{new}}}\\mathbf{O}_{i}+e^{\\tilde{m}_{ij}-m_{i}^{ \\text{new}}}\\tilde{\\mathbf{P}}_{ij}^{\\text{dropped}}\\mathbf{V}_{j})\\) to HBM.\n",
            "16: Write \\(\\ell_{i}\\leftarrow\\ell_{i}^{\\text{new}}\\), \\(m_{i}\\leftarrow m_{i}^{\\text{new}}\\) to HBM.\n",
            "17:endif\n",
            "18:endfor\n",
            "19:endfor\n",
            "20: Return \\(\\mathbf{O},\\ell,m,\\mathcal{R}\\).\n",
            "```\n",
            "\n",
            "**Algorithm 5** Block-Sparse FlashAttention Forward Pass\n",
            "\n",
            "## Appendix D Extension Details\n",
            "\n",
            "### Block-sparse FlashAttention\n",
            "\n",
            "We describe the full block-sparse FlashAttention algorithm in Algorithm 5. The algorithm is identical to Algorithm 2, except that we skip zero blocks.\n",
            "\n",
            "We prove the IO-complexity of block-sparse FlashAttention.\n",
            "\n",
            "Proof of Proposition 4.: The proof is very similar to the proof of Theorem 2. For the block-sparse case, notice that we only need to load blocks corresponding to nonzero blocks. As a result, the number of HBM accesses are scaled by \\(s\\), the fraction of nonzero blocks in the block-sparsity mask. However, for small values of \\(s\\), we would still need to write the result \\(\\mathbf{O}\\in\\mathbb{R}^{N\\times d}\\). Therefore the number of HBM accesses is\n",
            "\n",
            "\\[\\Theta\\left(Nd+\\frac{N^{2}d^{2}}{M}s\\right).\\]\n",
            "\n",
            "### Potential Extensions\n",
            "\n",
            "We discuss here a few potential extensions of the IO-aware approach to speed up deep learning training.\n",
            "\n",
            "**Multi-GPU Attention.** Large language models are trained on hundreds or thousands of GPUs, and one typically splits the attention computation between 4-8 GPUs on the same node [77]. This introduces another level of memory hierarchy: beside GPU SRAM and GPU HBM, we also have the HBM of other GPUs. For very long sequences, the different GPUs on the same node can cooperate to compute attention by taking into account the asymmetry of different levels of memory hierarchy.\n",
            "\n",
            "**Sparse MLP layers.** Typical dense MLP layers are compute-bound and not memory-bound. To improve their efficiency, MLP layers with sparse weight matrices can be used [17]. However, many sparse MLP layers are instead memory-bound, and their speedup is often not proportional to the sparsity. We believe that an IO-aware implementation can alleviate this issue and realize the benefits of sparsity. We are excited about future work in this direction, to reduce the computational requirement of large models and improve their wall-block runtime.\n",
            "\n",
            "**Kernel machine learning.** Our approach in FlashAttention relies on the fact that the \\(N\\times N\\) attention matrix is a function of a low-rank matrix \\(\\mathbf{Q}\\mathbf{K}^{\\top}\\) (of rank \\(d\\ll N\\)). As a result, we can repeatedly load the inputs \\(\\mathbf{Q},\\mathbf{K}\\) and recompute the block of the attention matrix that we need, significantly reducing HBM access. As similar scenario happens in kernel machine learning: each element \\(K_{ij}\\) of the \\(N\\times N\\) kernel matrix \\(\\mathbf{K}\\) is a function of two vectors of size \\(d\\ll N\\), as it measures the similarity between two datapoints \\(x_{i}\\) and \\(x_{j}\\). The KeOps library [8, 26] is a successful example of how reducing memory reads/writes can speed up kernel operations. We hope that this will motivate kernel methods that focus more on reducing IOs instead of just FLOPs.\n",
            "\n",
            "## Appendix E Full Experimental Results\n",
            "\n",
            "### Bert\n",
            "\n",
            "We train BERT-large following the training procedure and hyperparameters of the reference MLPerf 1.1 implementation. In particular, we use the LAMB optimizer with learning rate 3.75e-3, with batch size 448, trained for at most 7100 steps. The training is stopped once the validation accuracy (for masked language modeling) reaches the target 72.0%, and the wall-clock run-time is measured. We train with FP16 precision using Apex AMP (with O2 optimization level).\n",
            "\n",
            "We compare our results with the reported training speed from Nvidia that was submitted to MLPerf 1.1 (Table 1).\n",
            "\n",
            "We use the same train / validation data split provided by MLPerf 1.1 reference implementation. In particular, we evaluate on the same 10000 validation examples as the baseline from Nvidia.\n",
            "\n",
            "We train the model on 8xA100-80GB GPUs. Each training run takes between 16 and 19 minutes, and we average the results of 10 runs.\n",
            "\n",
            "### Gpt-2\n",
            "\n",
            "We use the standard implementations of GPT-2 [67] from Huggingface transformers library and from Nvidia's Megatron-LM repo. We follow the training recipe of the Megatron-LM repo.\n",
            "\n",
            "We use an effective batch size of 512, and use gradient accumulation to fit into available GPU memory. We use the AdamW optimizer, with learning rate 6e-4 for GPT-2 small and 1.5e-4 for GPT-2 medium, and weight decay of 0.1. All models are trained with the same hyperparameters for 400K steps. We run all implementations with mixed-precision training (PyTorch AMP).\n",
            "\n",
            "We use the Openwebtext dataset, with the GPT-2 BPE tokenizer. We randomly select 0.5% of the dataset as the validation set, with the rest being used as training set. This random selection of validation set is done once, and all models are evaluated on the same validation set.\n",
            "\n",
            "We train the model on 8xA100-40GB GPUs, and we measure the wall-clock training time. Training GPT-2 small takes between 2.7-9.5 days, and training GPT-2 medium takes between 6.9-21.0 days (Table 2).\n",
            "\n",
            "In Fig. 4, we plot of the validation perplexity throughout training of GPT-2 small/medium, using either HuggingFace implementation or our FlashAttention implementation. We see that FlashAttention behaves the same as the baseline implementation and the validation perplexity curves of the two implementations almost lie on top of each other.\n",
            "\n",
            "Long Document Classification.For MIMIC-III and ECtHR, we follow the hyperparameters of Dai et al. [13].\n",
            "\n",
            "### LRA details\n",
            "\n",
            "We follow the hyperparameters from the Long-range arena paper [80], the Long-range arena repo ([https://github.com/google-research/long-range-arena](https://github.com/google-research/long-range-arena)), and the Nystromformer reproduction [90]. To be generous to the baseline methods, if we are unable to reproduce the performance of any baseline for any of the five tasks, we report the better performance from Tay et al. [80] or Xiong et al. [90] for that baseline on that task.\n",
            "\n",
            "After hyperparameter tuning, almost all of the attention methods achieve similar accuracy on all of the five LRA tasks.\n",
            "\n",
            "We run all methods with mixed-precision training, except for Performer (not stable with mixed precision) and Local Attention (implementation does not support FP16).\n",
            "\n",
            "To calculate the overall wallclock-time speedup, we take the geometric mean of the wallclock-time speedup of each of the five tasks.\n",
            "\n",
            "Path-XFor Path-X and Path-256, we follow the hyperparameters from the PathFinder-32 experiments from the long-range arena paper[80]. For both, we first pretrain a model on Path-64. We take the checkpoint after 200 epochs, upsample its positional embedding (we duplicate the positional embeddings gridwise in space), and fine-tune it on the downstream task for 200 epochs with one epoch of linear warmup, and cosine decay of the learning rate. For Path-X, we take the best performing checkpoint (according to val accuracy), and additionally fine-tune it for 200 epochs with the same warmup and learning rate (this adds roughly 4 points of accuracy to FlashAttention for Path-X, but the model starts overfitting afterwards).\n",
            "\n",
            "### Comparison with Apex FMHA\n",
            "\n",
            "We compare our method/implementation with Apex FMHA ([https://github.com/NVIDIA/apex/tree/master/apex/contribb/csrc/fmha](https://github.com/NVIDIA/apex/tree/master/apex/contribb/csrc/fmha)).\n",
            "\n",
            "When we started this project, Apex FMHA was the fastest implementation of attention (that we knew of), tailored for short sequences of length at most 512. In fact, almost all MLPerf submissions for BERT training benchmark running on Nvidia GPUs use FMHA for their model code, as of MLPerf 1.1 [58]. Since\n",
            "\n",
            "Figure 4: Validation perplexity of GPT-2 small/medium using two implementations. We confirm that FlashAttention yields the same validation curves as the baseline implementation from HuggingFace.\n",
            "\n",
            "FMHA targets BERT models, it only supports head dimension 64, and only runs on A100 GPUs. FMHA fuses the attention computation dropout(softmax(mask(**QK\\({}^{T}\\)**)))**V** into one CUDA kernel. In the forward pass, it stores the attention matrix softmax(mask(**QK\\({}^{T}\\)**)) to HBM to be used in gradient computation. As a result, it does not offer substantial memory saving (though for shorter sequences memory footprint is often not a primary concern).\n",
            "\n",
            "We use FMHA code as a starting point, and apply two well-established techniques (tiling and recomputation) to deal with long sequences and to save memory as mentioned in Section 3. As a result, we can support much longer sequences (e.g., up to length 64K). We also support more head dimensions (16, 32, 64, 128) and broader GPU types (all Turing and Ampere GPUs at the time of writing).\n",
            "\n",
            "In Table 7, we compare the performance of FlashAttention and Apex FMHA for short sequences (as FMHA only supports sequence length at most 512). Generally FlashAttention is slightly faster than FMHA in the forward pass and slightly slower than FMHA in the backward pass. This is because we do not store the attention matrix in the forward pass and recompute it in the backward pass. Compared to FMHA, the overall runtime of FlashAttention is about 4% slower for sequence length 128, 8% faster for sequence length 256, and 5% faster for sequence length 512.\n",
            "\n",
            "### Speedup On Different Hardware and Configurations\n",
            "\n",
            "Speedup varies between different types of GPU types and generations depending on HBM bandwidth and SRAM size. In this section, we profile FlashAttention speedup on different GPUs and configurations.\n",
            "\n",
            "A100Figure 5 shows speedup on an A100 GPU with batch size 8, head dimension 64, and 12 attention heads, across different sequence lengths. We generally see 2-4x speedup, and we see more speedup when using dropout and masking due to kernel fusion.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{c|c c c} \\hline \\hline\n",
            "**Attention Method** & 128 & 256 & 512 \\\\ \\hline\n",
            "**Apex FMHA forward** & 0.10 & 0.29 & 1.14 \\\\\n",
            "**FlashAttention forward** & **0.08** & **0.22** & **0.81** \\\\ \\hline\n",
            "**Apex FMHA backward** & **0.17** & **0.52** & **1.81** \\\\\n",
            "**FlashAttention backward** & 0.20 & 0.53 & 2.00 \\\\ \\hline\n",
            "**Apex FMHA forward + backward** & **0.27** & 0.81 & 2.95 \\\\\n",
            "**FlashAttention forward + backward** & 0.28 & **0.75** & **2.81** \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 7: Runtime (ms) of FlashAttention compared to FMHA by sequence length, with masking and dropout, measured on an A100-SXM4-40GB GPU. Batch size 64, 16 heads, head dimension 64 (i.e., BERT-large size).\n",
            "\n",
            "Figure 5: Speedup over standard PyTorch attention at different sequence lengths, on A100.\n",
            "\n",
            "A100, Head Dimension 128Speedup also changes when we increase the head dimension. Each block requires more memory, so we need to use smaller block sizes to fit into SRAM. Figure 6 shows speedup with head dimension 128 on an A100 (batch size 16, 12 heads). We see less speedup overall--but we can still see significant speedup (up to 3\\(\\times\\)) with a causal mask, where half the blocks are masked out.\n",
            "\n",
            "Rtx 3090Figure 7 shows speedup on an RTX 3090 GPU. Here, we use batch size 12 with 12 attention heads. We observe slightly higher speedups on the RTX 3090 (between 2.5-4.5\\(\\times\\)), since the memory bandwidth on an RTX 3090 is lower than on an A100 (roughly 900 GB/s vs. 1.5 TB/s).\n",
            "\n",
            "T4Figure 8 shows speedup on a T4 GPU. T4 SRAM is smaller than A100, so we need to make the block sizes smaller in FlashAttention. As a result, we observe less speedup on T4, which matches the IO complexity analysis in Section 3.2. T4 GPUs are commonly used for inference, so we also report speedup on the forward pass only.\n",
            "\n",
            "Figure 6: Speedup over standard PyTorch attention at different sequence lengths, on A100, with head dimension 128.\n",
            "\n",
            "Figure 7: Speedup over standard PyTorch attention at different sequence lengths, on RTX 3090.\n",
            "\n",
            "### Full Benchmarking Results\n",
            "\n",
            "We report the full benchmarking results and experimental details on A100.\n",
            "\n",
            "BaselinesWe compare against reference implementations for exact attention from PyTorch/HuggingFace and Megatron, approximate attention, and sparse attention. For approximate attention, we compare against reference implementations of Reformer [51], Local Attention [68], Linformer Attention [84], Smyrf [19], and LongShortFormer (LSFormer) [94]. For sparse attention, we compare against reference implementations of Block-Sparse Attention form OpenAI [11], Longformer[3], and BigBird Attention [92]. For the approximate and sparse attention, we use a compression ratio of 1/8, or a compressed sequence length of 256, whichever is smaller.\n",
            "\n",
            "SetupWe measure runtime and memory usage of the attention computation with 8 heads of dimension 64, and batch size 16 on a machine with one A100 GPU with 40 GB of GPU HBM. We vary sequence length in our experiments. We compute attention on random vectors for **Q**, **K**, and **V** (we do not measure the projection from the hidden layer). For dropout, we use dropout 0.1; for masking, we use a padding mask with uniformly-random mask lengths between the total sequence length and the total sequence length minus 20. To measure runtime, we take the average of 100 measurements of the attention call. We only measure memory footprint once, since it does not vary between runs.\n",
            "\n",
            "Figure 8: Speedup over standard PyTorch attention at different sequence lengths, on T4. **Top:** Combined forward pass + backward pass. **Bottom:** Forward pass only.\n",
            "\n",
            "We report timing results on the forward pass, backward pass, and combined forward + backward pass. We measure each method with and without dropout, masking, or both--except for Block Sparse, Longformer, and BigBird. These methods did not successfully run the backward pass with masking due to a bug in external libraries, so we measured them without masking to be generous. We use FP16 for all measurements, except for Local Attention, whose implementation only supports FP32.\n",
            "\n",
            "For each baseline, we increase sequence length until it runs out of memory on the GPU, except for the following exceptions: The Megatron implementation does not support sequence lengths longer than 2048. Block-Sparse (OpenAI) does not support sequence lengths longer than 4096. Longformer and BigBird do not support sequence lengths longer than 8092.\n",
            "\n",
            "We measure memory usage on the combined forward + backward pass, without dropout or masking.\n",
            "\n",
            "ResultsTable 8 summarizes all the experimental configurations and contains pointers to the results tables.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{c c c|c} \\hline \\hline\n",
            "**Dropout** & **Masking** & **Pass** & **Table** \\\\ \\hline Yes & Yes & Forward & Table 9 \\\\ Yes & Yes & Backward & Table 10 \\\\ Yes & Yes & Combined & Table 11 \\\\ No & Yes & Forward & Table 12 \\\\ No & Yes & Backward & Table 13 \\\\ No & Yes & Combined & Table 14 \\\\ Yes & No & Forward & Table 15 \\\\ Yes & No & Backward & Table 16 \\\\ Yes & No & Combined & Table 17 \\\\ No & No & Forward & Table 18 \\\\ No & No & Backward & Table 19 \\\\ No & No & Combined & Table 20 \\\\ No & No & Memory Usage (Combined) & Table 21 \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 8: Pointers to results tables.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{c|c c c c c c c c c} \\hline \\hline\n",
            "**Attention Method** & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\\\ \\hline\n",
            "**PyTorch Attention** & 0.36 & 0.34 & 0.78 & 2.54 & 9.33 & 36.33 & - & - & - & - \\\\\n",
            "**Megatron** & 0.40 & 0.40 & 1.10 & 3.65 & 16.19 & - & - & - & - \\\\ \\hline\n",
            "**Reformer** & 2.03 & 3.15 & 5.67 & 11.02 & 22.59 & 46.14 & 97.38 & 212.13 & - & - \\\\\n",
            "**Local Attention** & 0.83 & 0.86 & 1.01 & 2.20 & 7.13 & 14.32 & 28.60 & 57.79 & 117.67 & - \\\\\n",
            "**Linformer** & 0.67 & 0.52 & 0.69 & 0.71 & 1.65 & 3.18 & 6.15 & 12.16 & 24.17 & 52.39 \\\\\n",
            "**Smyrf** & 2.27 & 2.34 & 3.91 & 7.44 & 14.71 & 29.22 & 58.27 & 116.41 & - & - \\\\\n",
            "**LSformer** & 1.18 & 1.27 & 1.34 & 3.38 & 11.40 & 22.55 & 44.95 & 89.76 & 179.66 & - \\\\ \\hline\n",
            "**Block Sparse** & 1.12 & 1.11 & 2.13 & 2.77 & 6.95 & 20.91 & - & - & - & - \\\\\n",
            "**Longformer** & 1.22 & 1.14 & 1.08 & 1.95 & 5.72 & 12.98 & - & - & - \\\\\n",
            "**BigBird** & 1.13 & 1.12 & 1.12 & 1.77 & 6.03 & 13.68 & - & - & - & - \\\\ \\hline\n",
            "**FlashAttention** & **0.04** & 0.06 & 0.21 & 0.82 & 2.85 & 10.41 & 41.74 & 167.19 & 670.76 & 2682.35 \\\\\n",
            "**Block-Sparse FlashAttention** & 0.06 & **0.06** & **0.12** & **0.44** & **0.86** & **1.70** & **3.29** & **6.55** & **13.34** \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 9: Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, **with dropout and masking**. Best in **bold**, second best underlined.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{r|c c c c c c c c c c} \\hline \\hline\n",
            "**Attention Method** & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\\\ \\hline\n",
            "**PyTorch Attention** & 0.30 & 0.30 & 0.63 & 1.93 & 7.08 & 27.45 & 112.90 & - & - & - \\\\\n",
            "**Megatron** & 0.45 & 0.41 & 0.43 & 1.52 & 5.80 & - & - & - & - & - \\\\ \\hline\n",
            "**Reformer** & 1.87 & 3.00 & 5.37 & 10.43 & 21.40 & 43.83 & 92.80 & 203.24 & - & - \\\\\n",
            "**Local Attention** & 0.70 & 0.81 & 1.02 & 2.09 & 6.64 & 13.34 & 26.77 & 54.02 & 110.11 & - \\\\\n",
            "**Linformer** & 0.63 & 0.50 & 0.67 & 0.65 & 1.36 & 2.60 & 5.04 & 9.92 & 19.69 & 43.47 \\\\\n",
            "**Smyrf** & 2.38 & 2.32 & 3.76 & 7.16 & 14.14 & 28.09 & 55.98 & 111.73 & - & - \\\\\n",
            "**LEsformer** & 1.22 & 1.29 & 1.44 & 3.28 & 10.99 & 21.72 & 43.29 & 86.32 & 172.76 & - \\\\ \\hline\n",
            "**Block Sparse** & 0.96 & 1.04 & 1.66 & 2.16 & 5.41 & 16.15 & - & - & - & - \\\\\n",
            "**Longformer** & 0.99 & 0.98 & 0.99 & 1.56 & 4.79 & 11.07 & 32.98 & - & - & - \\\\\n",
            "**BigBird** & 0.96 & 1.02 & 1.02 & 1.48 & 5.05 & 11.59 & 34.16 & - & - & - \\\\ \\hline\n",
            "**FlashAttention** & **0.03** & **0.04** & 0.17 & 0.68 & 2.28 & 8.40 & 33.55 & 134.14 & 537.50 & 2150.88 \\\\\n",
            "**Block-Sparse FlashAttention** & 0.05 & **0.04** & **0.05** & **0.11** & **0.35** & **0.68** & **1.33** & **2.54** & **5.34** & **10.73** \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 13: Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, **with masking**. Best in **bold**, second best underlined.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{r|c c c c c c c c c c} \\hline \\hline\n",
            "**Attention Method** & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\\\ \\hline\n",
            "**PyTorch Attention** & 0.37 & 0.49 & 1.66 & 5.81 & 22.32 & 87.67 & - & - & - & - \\\\\n",
            "**Megatron** & 0.35 & 0.32 & 0.77 & 2.42 & 8.43 & - & - & - & - & - \\\\ \\hline\n",
            "**Reformer** & 2.37 & 4.59 & 8.91 & 17.68 & 35.13 & 70.05 & 140.01 & - & - & - \\\\\n",
            "**Local Attention** & 0.55 & 0.62 & 1.49 & 4.03 & 13.78 & 27.61 & 55.20 & 110.27 & 221.40 & - \\\\\n",
            "**Linformer** & 0.89 & 0.80 & 0.81 & 0.93 & 2.48 & 4.75 & 9.29 & 18.27 & 36.53 & - \\\\\n",
            "**Smyrf** & 1.41 & 2.83 & 5.43 & 10.72 & 21.25 & 42.31 & 84.48 & 168.95 & - & - \\\\\n",
            "**LEsformer** & 1.75 & 1.76 & 3.01 & 7.50 & 20.07 & 39.08 & 76.39 & 150.82 & - & - \\\\ \\hline\n",
            "**Block Sparse** & 1.29 & 1.28 & 2.18 & 3.04 & 7.27 & 21.16 & - & - & - & - \\\\\n",
            "**Longformer** & 1.27 & 1.31 & 1.29 & 2.04 & 5.24 & 10.74 & 25.95 & - & - & - \\\\\n",
            "**BigBird** & 1.33 & 1.28 & 1.81 & 5.55 & 11.44 & 27.45 & - & - & - \\\\ \\hline\n",
            "**FlashAttention** & **0.30** & **0.26** & 0.68 & 2.02 & 6.84 & 26.89 & 105.70 & 418.96 & 1666.89 & 6660.44 \\\\\n",
            "**Block-Sparse FlashAttention** & **0.30** & 0.27 & **0.29** & **0.59** & **1.50** & **2.94** & **5.82** & **11.85** & **23.98** & **47.61** \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 10: Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, **with dropout and masking**. Best in **bold**, second best underlined.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{r|c c c c c c c c c} \\hline \\hline\n",
            "**Attention Method** & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\\\ \\hline\n",
            "**PyTorch Attention** & 0.36 & 0.30 & 0.63 & 1.93 & 7.08 & 27.45 & 112.90 & - & - & - \\\\\n",
            "**Megatron** & 0.45 & 0.41 & 0.43 & 1.52 & 5.80 & - & - & - & - \\\\ \\hline\n",
            "**Reformer** & 1.87 & 3.00 & 5.37 & 10.43 & 21.40 & 43.83 & 92.80 & 203.24 & - & - \\\\\n",
            "**Local Attention** & 0.70 & 0.81 & 1.02 & 2.09 & 6.64 & 13.34 & 26.77 & 54.02 & 110.11 & - \\\\\n",
            "**Linformer** & 0.63 & 0.50 & 0.67 & 0.65 & 1.36 & 2.60 & 5.04 & 9.92 & 19.69 & 43.47 \\\\\n",
            "**Smyrf** & 2.38 & 2.32 & 3.76 & 7.16 & 14.14 & 28.09 & 55.98 & 111.73 & - & - \\\\\n",
            "**LEsformer** & 1.22 & 1.29 & 1.44 & 3.28 & 10.99 & 21.72 & 43.29 & 86.32 & 172.76 & - \\\\ \\hline\n",
            "**Block Sparse** & 0.96 & 1.04 & 1.66 & 2.16 & 5.41 & 16.15 & - & - & - & - \\\\\n",
            "**Longformer** & 0.99 & 0.98 & 0.99 & 1.56 & 4.79 & 11.07 & 32.98 & - & - & - \\\\\n",
            "**BigBird** & 0.96 & 1.02 & 1.02 & 1.48 & 5.05 & 11.59 & 34.16 & - & - & - \\\\ \\hline\n",
            "**FlashAttention** & **0.03** & **0.04** & 0.17 & 0.68 & 2.28 & 8.40 & 33.55 & 134.14 & 537.50 & 2150.88 \\\\\n",
            "**Block-Sparse FlashAttention** & 0.05 & **0.04** & **0.05** & **0.11** & **0.35** & **0.68** & **1.33** & **2.54** & **5.34** & **10.73** \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 11: Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, sequence length, **with dropout and masking**. Best in **bold**, second best underlined.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{r|c c c c c c c c c c} \\hline \\hline\n",
            "**Attention Method** & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\\\ \\hline\n",
            "**PyTorch Attention** & 0.44 & 0.35 & 0.90 & 2.94 & 10.77 & 41.67 & - & - & - & - \\\\\n",
            "**Megatron** & 0.28 & 0.33 & 0.92 & 2.94 & 10.80 & - & - & - & - \\\\ \\hline\n",
            "**Reformer** & 2.24 & 4.34 & 8.39 & 16.62 & 33.02 & 65.77 & 131.52 & - & - & - \\\\\n",
            "**Local Attention** & 0.51 & 0.58 & 1.41 & 3.71 & 12.96 & 25.98 & 51.94 & 103.72 & 207.78 & - \\\\\n",
            "**Linformer** & 0.84 & 0.74 & 0.79 & 0.85 & _2.28_ & _4.37_ & _8.66_ & _17.02_ & _33.78_ & - \\\\\n",
            "**Smyrf** & 1.27 & 2.56 & 4.90 & 9.66 & 19.16 & 38.13 & 76.17 & 152.39 & - & - \\\\\n",
            "**LSformer** & 1.67 & 1.77 & 3.03 & 7.52 & 20.10 & 39.13 & 76.35 & 150.83 & - & - \\\\ \\hline\n",
            "**Block Sparse** & 1.27 & 1.36 & 2.15 & 3.04 & 7.27 & 21.18 & - & - & - & - \\\\\n",
            "**Longformer** & 1.28 & 1.34 & 1.38 & 1.98 & 5.24 & 10.74 & 25.95 & - & - & - \\\\\n",
            "**BigBird** & 1.48 & 1.47 & 1.50 & 1.81 & 5.57 & 11.38 & 27.43 & - & - & - \\\\ \\hline\n",
            "**FlashAttention** & **0.15** & 0.18 & 0.58 & 1.86 & 6.50 & 26.21 & 104.27 & 416.10 & 1661.92 & 6643.01 \\\\\n",
            "**Block-Sparse FlashAttention** & 0.17 & **0.17** & **0.17** & **0.40** & **1.10** & **2.04** & **4.43** & **9.33** & **18.28** & **37.31** \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 16: Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by setting depth, **with dropout**. Best in **bold**, second best underlined.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{r|c c c c c c c c c c} \\hline \\hline\n",
            "**Attention Method** & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\\\ \\hline\n",
            "**PyTorch Attention** & 0.80 & 0.81 & 2.08 & 7.23 & 27.51 & 107.58 & - & - & - & - \\\\\n",
            "**Megatron** & 0.81 & 0.83 & 1.09 & 3.36 & 12.39 & - & - & - & - & - \\\\ \\hline\n",
            "**Reformer** & 4.16 & 7.46 & 14.06 & 27.68 & 55.66 & 112.15 & 229.37 & - & - & - \\\\\n",
            "**Local Attention** & 1.39 & 1.68 & 2.08 & 5.83 & 20.04 & 0.16 & 80.44 & 161.35 & 325.11 & - \\\\\n",
            "**Linformer** & 1.51 & 1.42 & 1.56 & 1.67 & 3.67 & 6.99 & 13.63 & 26.77 & 53.36 & 117.56 \\\\\n",
            "**Smyrf** & 3.38 & 4.93 & 0.97 & 17.66 & 34.94 & 69.55 & 138.72 & 277.41 & - & - \\\\\n",
            "**LSformer** & 3.08 & 3.10 & 4.26 & 10.90 & 31.59 & 61.72 & 121.51 & 241.18 & - & - \\\\ \\hline\n",
            "**Block Sparse** & 2.39 & 2.40 & 3.31 & 5.02 & 12.25 & 35.94 & - & - & - & - \\\\\n",
            "**Longformer** & 2.36 & 2.34 & 2.38 & 2.94 & 9.83 & 21.35 & 58.12 & - & - & - \\\\\n",
            "**BigBird** & 2.35 & 2.35 & 2.37 & 3.25 & 10.36 & 22.57 & 60.63 & - & - & - \\\\ \\hline\n",
            "**FlashAttention** & **0.32** & **0.30** & 0.83 & 2.37 & 7.95 & 30.77 & 119.98 & 473.65 & 1883.43 & 7513.01 \\\\\n",
            "**Block-Sparse FlashAttention** & 0.34 & 0.34 & **0.36** & **0.69** & **1.85** & **3.89** & **7.16** & **14.85** & **30.46** & **60.03** \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 14: Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, **with dropout**. Best in **bold**, second best underlined.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{r|c c c c c c c c c} \\hline \\hline\n",
            "**Attention Method** & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\\\ \\hline\n",
            "**PyTorch Attention** & 0.66 & 0.67 & 1.43 & 4.82 & 17.47 & 67.29 & - & - & - \\\\\n",
            "**Megatron** & 0.88 & 0.90 & 1.49 & 4.73 & 17.41 & - & - & - & - \\\\ \\hline\n",
            "**Reformer** & 4.06 & 7.28 & 13.68 & 26.98 & 54.27 & 109.39 & 223.80 & - & - & - \\\\\n",
            "**Local Attention** & 1.09 & 1.40 & 1.99 & 5.61 & 19.23 & 38.62 & 77.30 & 154.63 & 311.12 & - \\\\\n",
            "**Linformer** & 1.31 & 1.21 & 1.30 & 1.39 & 3.73 & 7.15 & 14.05 & 27.69 & 55.00 & - \\\\\n",
            "**Smyrf** & 3.00 & 4.37 & 8.05 & 15.66 & 31.04 & 61.64 & 123.04 & 245.65 & - & - \\\\\n",
            "**LSformer** & 3.07 & 3.17 & 4.31 & 10.89 & 31.54 & 61.78 & 121.56 & 240.94 & - & - \\\\ \\hline\n",
            "**Block Sparse** & 2.54 & 2.52 & 3.71 & 5.44 & 13.29 & 39.19 & - & - & - & - \\\\\n",
            "**Longformer** & 2.47 & 2.49 & 2.51 & 3.10 & 10.39 & 22.49 & 60.44 & - & - & - \\\\\n",
            "**BigBird** & 2.51 & 2.49 & 2.52 & 3.40 & 10.97 & 23.89 & 63.28 & - & - & - \\\\ \\hline\n",
            "**FlashAttention** & **0.35** & **0.36** & **0.80** & 2.52 & 9.16 & 36.70 & 146.13 & 583.45 & 2332.01 & 9323.63 \\\\\n",
            "**Block-Sparse FlashAttention** & 0.91 & 0.83 & 0.94 & **0.92** & **1.83** & **3.50** & **7.02** & **13.56** & **26.71** & **53.92** \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 17: Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, **with dropout**. Best in **bold**, second best underlined.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{r|c c c c c c c c c} \\hline \\hline\n",
            "**Attention Method** & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\\\ \\hline\n",
            "**PyTorch Attention** & 0.88 & 0.90 & 1.49 & 4.73 & 17.41 & - & - & - & - \\\\ \\hline\n",
            "**Reformer** & 4.06 & 7.28 & 13.68 & 26.98 & 54.27 & 109.39 & 223.80 & - & - & - \\\\\n",
            "**Local Attention\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{r|c c c c c c c c c c} \\hline \\hline\n",
            "**Attention Method** & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\\\ \\hline\n",
            "**PyTorch Attention** & 0.67 & 0.70 & 1.18 & 3.67 & 13.22 & 50.44 & - & - & - & - \\\\\n",
            "**Megatron** & 0.74 & 0.65 & 1.23 & 3.80 & 13.21 & - & - & - & - \\\\ \\hline\n",
            "**Reformer** & 3.93 & 7.01 & 13.15 & 25.89 & 52.09 & 105.00 & 215.13 & - & - & - \\\\\n",
            "**Local Attention** & 1.09 & 1.27 & 1.99 & 5.38 & 18.32 & 36.77 & 73.67 & 147.29 & 296.35 & - \\\\\n",
            "**Linformer** & 1.31 & 1.25 & 1.30 & 1.29 & 3.20 & 6.10 & 11.93 & 23.39 & 46.72 & 100.52 \\\\\n",
            "**Smyrf** & 2.98 & 4.23 & 7.78 & 15.12 & 29.96 & 59.45 & 118.60 & 237.02 & - & - \\\\\n",
            "**LSformer** & 3.03 & 3.05 & 4.26 & 10.70 & 30.77 & 60.15 & 118.33 & 234.94 & - & - \\\\ \\hline\n",
            "**Block Sparse** & 2.39 & 2.40 & 3.31 & 5.02 & 12.25 & 35.94 & - & - & - & - \\\\\n",
            "**Longformer** & 2.36 & 2.34 & 2.38 & 2.94 & 9.83 & 21.35 & 58.12 & - & - & - \\\\\n",
            "**BigBird** & 2.35 & 2.35 & 2.37 & 3.25 & 10.36 & 22.57 & 60.63 & - & - & - \\\\ \\hline\n",
            "**FlashAttention** & **0.31** & **0.31** & **0.73** & 2.29 & 7.64 & 30.09 & 118.50 & 470.51 & 1876.08 & 7492.85 \\\\\n",
            "**Block-Sparse FlashAttention** & 0.74 & 0.77 & 0.82 & **0.88** & **1.71** & **3.21** & **6.56** & **12.60** & **24.93** & **50.39** \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 21: Memory usage (MB) of various exact/approximate/sparse attention mechanisms by sequence length. Best in **bold**, second best underlined.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{r|c c c c c c c c c} \\hline \\hline\n",
            "**Attention Method** & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\\\ \\hline\n",
            "**PyTorch Attention** & 0.21 & 0.22 & 0.43 & 1.27 & 4.32 & 16.47 & 67.77 & - & - & - \\\\\n",
            "**Megatron** & 0.24 & 0.26 & 0.42 & 1.33 & 4.28 & - & - & - & - & - \\\\ \\hline\n",
            "**Reformer** & 1.77 & 2.82 & 5.01 & 9.74 & 20.03 & 41.11 & 87.39 & 192.40 & - & - \\\\\n",
            "**Local Attention** & 0.48 & 0.57 & 0.80 & 1.90 & 5.76 & 11.56 & 23.13 & 46.65 & 94.74 & - \\\\\n",
            "**Linformer** & 0.46 & 0.36 & 0.45 & **0.50** & 1.09 & 2.09 & 4.01 & 7.90 & 15.70 & 35.40 \\\\\n",
            "**Smyrf** & 1.94 & 1.96 & 3.01 & 5.69 & 11.26 & 22.23 & 44.21 & 88.22 & - & - \\\\\n",
            "**LSformer** & 1.21 & 1.34 & 1.34 & 3.31 & 11.01 & 21.71 & 43.27 & 86.32 & 172.85 & - \\\\ \\hline\n",
            "**Block Sparse** & 0.96 & 1.04 & 1.66 & 2.16 & 5.41 & 16.15 & - & - & - & - \\\\\n",
            "**Longformer** & 0.99 & 0.98 & 0.99 & 1.56 & 4.79 & 11.07 & 32.98 & - & - & - \\\\\n",
            "**BigBird** & 0.96 & 1.02 & 1.02 & 1.48 & 5.05 & 11.59 & 34.16 & - & - & - \\\\ \\hline\n",
            "**FlashAttention** & **0.08** & **0.09** & **0.18** & 0.68 & 2.40 & 8.42 & 33.54 & 134.03 & 535.95 & 2147.05 \\\\\n",
            "**Block-Sparse FlashAttention** & 0.56 & 0.52 & 0.63 & 0.65 & **0.61** & **0.96** & **1.69** & **3.02** & **5.69** & **11.77** \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 18: Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length. Best in **bold**, second best underlined.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{r|c c c c c c c c c} \\hline \\hline\n",
            "**Attention Method** & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\\\ \\hline\n",
            "**PyTorch Attention** & 0.21 & 0.22 & 0.43 & 1.27 & 4.32 & 16.47 & 67.77 & - & - & - \\\\\n",
            "**Megatron** & 0.24 & 0.26 & 0.42 & 1.33 & 4.28 & - & - & - & - \\\\ \\hline\n",
            "**Reformer** & 1.77 & 2.82 & 5.01 & 9.74 & 20.03 & 41.11 & 87.39 & 192.40 & - & - \\\\\n",
            "**Local Attention** & 0.48 & 0.57 & 0.80 & 1.90 & 5.76 & 11.56 & 23.13 & 46.65 & 94.74 & - \\\\\n",
            "**Linformer** & 0.46 & 0.36 & 0.45 & **0.50** & 1.09 & 2.09 & 4.01 & 7.90 & 15.70 & 35.40 \\\\\n",
            "**Smyrf** & 1.94 & 1.96 & 3.01 & 5.69 & 11.26 & 22.23 & 44.21 & 88.82 & - & - \\\\\n",
            "**LSformer** & 1.21 & 1.34 & 1.34 & 3.31 & 11.01 & 21.71 & 43.27 & 86.32 & 172.85 & - \\\\ \\hline\n",
            "**Block Sparse** & 0.96 & 1.04 & 1.66 & 2.16 & 5.41 & 16.15 & - & - & - & - \\\\\n",
            "**Longformer** & 0.99 & 0.98 & 0.99 & 1.56 & 4.79 & 11.07 & 32.98 & - & - & - \\\\\n",
            "**BigBird** & 0.96 & 1.02 & 1.02 & 1.48 & 5.05 & 11.59 & 34.16 & - & - & - \\\\ \\hline\n",
            "**FlashAttention** & **0.08** & **0.09** & **0.18** & 0.68 & 2.40 & 8.42 & 33.54 & 134.03 & 535.95 & 2147.05 \\\\\n",
            "**Block-Sparse FlashAttention** & 0.56 & 0.52 & 0.63 & 0.65 & **0.61** & **0.96** & **1.69** & **3.02** & **5.69** & **11.77** \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 19: Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length. Best in **bold**, second best underlined.\n",
            "CPU times: user 1.07 s, sys: 161 ms, total: 1.23 s\n",
            "Wall time: 4min 45s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the filename\n",
        "filename = \"/content/flash_attention.txt\"\n",
        "\n",
        "# Open the file in write mode ('w') and write the string to it\n",
        "with open(filename, 'w') as file:\n",
        "    file.write(content)"
      ],
      "metadata": {
        "id": "iG_PYSkTN8NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "content = paper_read(None, 'https://arxiv.org/pdf/2309.03883v1.pdf')\n",
        "print(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuaTAbKskrKn",
        "outputId": "04d0f7d3-b163-4250-887b-4d04e8797019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF downloaded successfully.\n",
            "# DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models\n",
            "\n",
            "Yung-Sung Chuang\\({}^{\\dagger}\\)\\({}^{\\star}\\), Yujia Xie\\({}^{\\ddagger}\\), Hongyin Luo\\({}^{\\dagger}\\), Yoon Kim\\({}^{\\dagger}\\), James Glass\\({}^{\\dagger}\\), Pengcheng He\\({}^{\\ddagger}\\)\n",
            "\n",
            "\\({}^{\\dagger}\\)Massachusetts Institute of Technology, \\({}^{\\ddagger}\\)Microsoft\n",
            "\n",
            "yungsung@mit.edu, yujiaxie@microsoft.com\n",
            "\n",
            "{hyluo,yoonkim,glass}@mit.edu\n",
            "\n",
            "herbert.he@gmail.com\n",
            "\n",
            "Work done during an internship at Microsoft.\n",
            "\n",
            "###### Abstract\n",
            "\n",
            "Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this **D**ecoding by **C**ontrasting **L**ayers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.1\n",
            "\n",
            "Footnote 1: The source code is available at [https://github.com/voidism/DoLa](https://github.com/voidism/DoLa).\n",
            "\n",
            "## 1 Introduction\n",
            "\n",
            "Large language models (LLMs) have demonstrated significant potential in numerous natural language processing (NLP) applications (Brown et al., 2020; OpenAI, 2022; 2023). However, despite the continued increase in performance and the emergence of new capabilities from scaling LLMs (Wei et al., 2022), their tendency to \"hallucinate\", i.e., generate content that deviates from real-world facts observed during pretraining (Ji et al., 2023), remains a persistent challenge. This represents a significant bottleneck in their deployment especially for high-stakes applications (e.g., clinical/legal settings) where reliable generation of trustworthy text is crucial.\n",
            "\n",
            "While the exact reasons for LMs' hallucinations are not completely understood, a possible reason is due to the maximum likelihood language modeling objective which minimize the forward KL divergence between the data and model distributions. This objective potentially results in a model with mass-seeking behavior which causes the LM to assign non-zero probability to sentences that are not fully consistent with knowledge embedded in the training data. Empirically, an LM trained with the next-word prediction objective on finite data has been shown to result in a model that use linguistic knowledge to recognize the superficial patterns in the training examples, instead of recognizing and generating the real-world facts extracted from the training corpus (Ji et al., 2023).\n",
            "\n",
            "From a model interpretability perspective, transformer LMs have been loosely shown to encode \"lower-level\" information (e.g., part-of-speech tags) in the earlier layers, and more \"semantic\" information in the later layers (Tenney et al., 2019). More recently, Dai et al. (2022) find that \"knowledge neurons\" are distributed in the topmost layers of the pretrained BERT model. Meng et al. (2022) show that factual knowledge can even be edited by manipulating a specific set of feedforward layers within an autoregressive transformer LM. We propose to exploit this modular encoding of knowledge to amplify the factual knowledge in an LM through a contrastive decoding approach, where the output probability over the next word is obtained from the _difference_ in logits obtained from a higher layer versus a lower layer. By emphasizing the knowledge from higher layers and downplaying the lower or intermediate layer knowledge, we can potentially make LMs more factual and consequently reduce hallucinations.\n",
            "\n",
            "An illustration of this idea for a simple example is shown in Figure 1. While \"_Seattle_\" maintains high probability throughout all the layers--presumably because it is a syntactically plausible answer--the probability of the true answer \"_Olympia_\" increases after the higher layers inject more factual knowledge. Contrasting the differences between the different layers can thus reveal the true answer in this case. Based on this concept, we propose a new decoding method, **D**ecoding by **C**ntransing **L**ayers (DoLa), for better surfacing factual knowledge embedded in an LLM without retrieving external knowledge or additional fine-tuning.\n",
            "\n",
            "Experiments on TruthfulQA (Lin et al., 2022) and FACTOR Muhlgay et al. (2023) demonstrate that DoLa is able to increase the truthfulness of the models of the LLaMA family (Touvron et al., 2023). Further experiments on chain-of-thought reasoning for StrategyQA (Geva et al., 2021) and GSM8K (Cobbe et al., 2021) also show that it can facilitate more factual reasoning. Finally, experiments on open-ended text generation results (evaluated with GPT-4) show that when compared with the original decoding method, DoLa can generate informative and significantly more factual responses that lead to better ratings. From an efficiency perspective, we find that DoLa causes only a small additional latency in the decoding process, suggesting it as a practical and useful decoding strategy for improving the truthfulness of LLMs.\n",
            "\n",
            "Figure 1: Illustration of how a transformer-based LM progressively incorporates more factual information along the layers. We observe that while the next-word probability of “_Seattle_” remains similar throughout the different layers, the probability of the correct answer “_Olympia_” gradually increases from the lower layers to the higher layers. DoLa uses this fact and decodes by contrasting the difference between the two layers to sharpen an LLM’s probability towards factually correct outputs.\n",
            "\n",
            " \n",
            "\n",
            "## 2 Method\n",
            "\n",
            "Recent language models are consists of an embedding layer, \\(N\\) stacked transformer layers, and an affine layer \\(\\phi(\\cdot)\\) for predicting the next-word distributution. Given a sequence of tokens \\(\\{x_{1},x_{2},\\ldots,x_{t-1}\\}\\), the embedding layer first embeds the tokens into a sequence of vectors \\(H_{0}=\\{h_{1}^{(0)},\\ldots,h_{t-1}^{(0)}\\}\\). Then \\(H_{0}\\) would be processed by each of the transformer layers successively. We denote the output of the \\(j\\)-th layer as \\(H_{j}\\). Then, the vocabulary head \\(\\phi(\\cdot)\\) predicts the probability of the next token \\(x_{t}\\)\n",
            "\n",
            "\\[p(x_{t}\\mid x_{<t})=\\mathrm{softmax}\\big{(}\\phi(h_{t}^{N})\\big{)}_{x_{t}}, \\quad x_{t}\\in\\mathcal{X},\\]\n",
            "\n",
            "where \\(\\mathcal{X}\\) is the vocabulary set.\n",
            "\n",
            "Instead of applying \\(\\phi\\) just on the final layer, our approach contrasts the higher-layer and lower-layer information to obtain the probability of next token. More specifically, for the lower layers, we also compute the probability of the next tokens using \\(\\phi(\\cdot)\\),\n",
            "\n",
            "\\[q_{j}(x_{t}\\mid x_{<t})=\\mathrm{softmax}\\big{(}\\phi(h_{t}^{j})\\big{)}_{x_{t}}, \\quad j=1,\\ldots,N.\\]\n",
            "\n",
            "The idea of applying language heads directly to the hidden states of the middle layers, known as _early exit_(Teerapittayanon et al., 2016; Elbayad et al., 2020; Schuster et al., 2022), has proven to be an effective inference method even without special training process (Kao et al., 2020), as the residual connections (He et al., 2016) in transformer layers make the hidden representations gradually evolve without abrupt changes. Using \\(q_{j}(x_{t})\\) to represent \\(q_{j}(x_{t}\\mid x_{<t})\\) for notational brevity, we then compute the probability of the next token by,\n",
            "\n",
            "\\[\\hat{p}(x_{t}\\mid x_{<t}) =\\mathrm{softmax}\\big{(}\\mathcal{F}\\big{(}q_{N}(x_{t}),q_{M}(x_{t })\\big{)}\\big{)}_{x_{t}}, \\tag{1}\\] \\[\\text{where}\\quad M =\\operatorname*{arg\\,max}_{j\\in\\mathcal{J}}\\;d\\big{(}q_{N}(\\cdot ),q_{j}(\\cdot)\\big{)}.\\]\n",
            "\n",
            "Here, layer \\(M\\) is referred to as the _premature layer_, while the final layer is referred to as the _mature layer_. The operator \\(\\mathcal{F}(\\cdot,\\cdot)\\), to be elaborated further in Section 2.3, is used to contrast between the output distributions from the premature layer and the mature layer by computing the difference between two distributions in the log domain. The premature layer is dynamically selected in each decoding step using a distributional distance measure \\(d(\\cdot,\\cdot)\\) (we use the Jensen-Shannon Divergence) between the mature layer and all the candidate layers in \\(\\mathcal{J}\\). We discuss \\(d(\\cdot,\\cdot)\\) in more detail in Section 2.1 and Section 2.2. The motivation for selecting the layer with the highest distance \\(d(\\cdot,\\cdot)\\) as the premature layer is to maximize the difference between the mature/premature layers.\n",
            "\n",
            "### Factual Knowledge Evolves Across Layers\n",
            "\n",
            "We conduct preliminary analysis with the 32-layer LLaMA-7B (Touvron et al., 2023) model to motivate our approach. Here, we compute the Jensen-Shannon Divergence (JSD) between the early exiting output distributions \\(q_{j}(\\cdot\\mid x_{<t})\\) and the final layer output distribution \\(q_{N}(\\cdot\\mid x_{<t})\\), to show how the early exiting outputs are different from the final layer outputs. Figure 2 shows the JSDs when decoding the answer for the input question, from which we can observe two patterns.\n",
            "\n",
            "Pattern #1:The first type of pattern is when predicting important name entities or dates, such as _Wole Soyinka_ and _1986_ in Figure 2, which require factual knowledge. We observe the calculated JSD would be still extremely high in the higher layers. This pattern indicates that the model is still changing its predictions in the last few layers, and potentially injecting more factual knowledge into the predictions.\n",
            "\n",
            "Pattern #2:The second type of pattern is when predicting function words, such as _was, the, to, in_, and the tokens that are copied from the input question, such as _first Nigerian, Nobel Prize_. When predicting these \"easy\" tokens, we can observe that the JSD becomes very small from the middle of the layers. This finding indicates that the model has already decided what token to generate in the early layers, so it just keeps the output distribution almost unchanged in the higher layers. This finding is also consistent with the assumptions in early exiting language models (Schuster et al., 2022).\n",
            "\n",
            "Qualitatively, when the next-word prediction requires factual knowledge, LLaMA seems to to change the predictions in the higher layers. Contrasting the layers before/after a sudden change may therefore amplify the knowledge emerging from the higher layers and make the model more rely more on its factual internal knowledge. Moreover, this evolution of information seems to vary token by token. In our proposed method, we need to accurately select the premature layer that contains _plausible but less factual_ information, which may not always stay in the same early layer. We propose an approach for dynamic premature later selection as illustrated in Figure 3.\n",
            "\n",
            "### Dynamic Premature Layer Selection\n",
            "\n",
            "To magnify the effective of contrastive decoding, the optimal premature layer to select should ideally be the layer that is the most different from the final-layer outputs. To allow for dynamic premature layer selection at each time step, we adopt the following measure of distance between the next-word distributions obtained from two layers,\n",
            "\n",
            "\\[d\\big{(}q_{N}(\\cdot\\,|\\,x_{<t}),q_{j}(\\cdot\\,|\\,x_{<t})\\big{)}=\\text{JSD}\\big{(} q_{N}(\\cdot\\,|\\,x_{<t})||q_{j}(\\cdot\\,|\\,x_{<t})\\big{)},\\]\n",
            "\n",
            "where \\(\\text{JSD}(\\cdot,\\cdot)\\) is the Jensen-Shannon divergence. The premature layer, i.e., the \\(M\\)-th layer (\\(0\\leq M<N\\)), is then selected as the layer with the maximum divergence among the subset of early layers,\n",
            "\n",
            "\\[M=\\arg\\max_{j\\in\\mathcal{J}}\\text{JSD}\\big{(}q_{N}(\\cdot\\,|\\,x_{<t})||q_{j}( \\cdot\\,|\\,x_{<t})\\big{)}, \\tag{2}\\]\n",
            "\n",
            "where \\(\\mathcal{J}\\) is the set of candidate early layers considered for premature layer selection. For LLaMA models with a varying number of layers, we divide the transformer layers into 2 to 4 buckets of \\(\\mathcal{J}\\) based on their total number of layers, in order to focus on contrasting from a certain range of layers. We still use a validation set to select the best bucket depending on the task at hand. See more details in Section 3.2.\n",
            "\n",
            "Figure 2: Jensen-Shannon Divergences between the final 32nd layer and even-numbered early layers. Column names represent predicted next tokens in each decoding step. Row names indicate the layer indices of the early exit layers, from the 0th (word embedding) layer to the 30th layer.\n",
            "\n",
            "This dynamic layer selection strategy enables the model to choose the most appropriate premature layer depending on the complexity and difficulty of each token, thereby making better use of the knowledge learned by the different layers of the transformer model.\n",
            "\n",
            "Besides the dynamic layer selection strategy, a very simple method that can also be considered is to select the premature layer by running brute-force experiments on all the possible early layers with a validation set, and pick the layer with the best validation performance. We refer to this simple method as DoLa-static. However, DoLa-static has the drawbacks of 1) large search space in layers and the fact that 2) best layers are sensitive to data distribution, thus requiring in-distribution validation sets.\n",
            "\n",
            "Our proposed dynamic layer selection strategy also mitigates the drawbacks of the static layer-selection approach by shrinking the layer search space and making the method more robust without heavily relying on in-distribution validation sets. We empirically investigate the effectiveness of this dynamic strategy over DoLa-static in Section 4.1.\n",
            "\n",
            "### Contrasting the Predictions\n",
            "\n",
            "Given the premature and mature layers obtained from Section 2.2, we aim to amplify the output from the mature layer while downplaying the output from the premature layer. Following the Contrastive Decoding approach from Li et al. (2022), we subtract the log probabilities of the premature layer outputs from those of the mature layer. We then use this resulting distribution as the next-word prediction, as illustrated in Figure 1,\n",
            "\n",
            "\\[\\mathcal{F}\\big{(}q_{N}(x_{t}),q_{M}(x_{t})\\big{)} =\\begin{cases}\\log\\dfrac{q_{N}(x_{t})}{q_{M}(x_{t})},&\\text{if }x_{t}\\in\\mathcal{V}_{\\text{head}}\\left(x_{t}|x_{<t} \\right),\\\\ -\\infty,&\\text{otherwise}.\\end{cases} \\tag{3}\\] \\[\\hat{p}(x_{t}) =\\text{softmax}\\big{(}\\mathcal{F}\\big{(}q_{N}(x_{t}),q_{M}(x_{t}) \\big{)}\\big{)} \\tag{4}\\]\n",
            "\n",
            "Figure 3: The illustration of how dynamic premature layer selection works.\n",
            "\n",
            "Similar to Li et al. (2022), the subset \\(\\mathcal{V}_{\\text{head}}\\)\\((x_{t}|x_{<t})\\in\\mathcal{X}\\) is defined as whether or not the token has high enough output probabilities from the mature layer,\n",
            "\n",
            "\\[\\mathcal{V}_{\\text{head}}\\ (x_{t}|x_{<t})=\\left\\{x_{t}\\in\\mathcal{X}:q_{N}(x_{t}) \\geq\\alpha\\max_{w}q_{N}(w)\\right\\}. \\tag{5}\\]\n",
            "\n",
            "If the predicted probability of a token is too small in the mature layer, it is not likely to be a reasonable prediction, so we set the token probability to zero to minimize false positive and false negative cases. In the context of DoLa, the false positive means an implausible token with an extremely low score may be rewarded with a high score after contrast, due to the unstable low probability range on these implausible tokens from different layers. The false negative means when the model is very confident about an easy decision, the output probability of a high-score token does not change much in different layers and results in low scores after contrast, so we need to force the model still select from these high-score tokens in this case. This strategy is referred as an _adaptive plausibility constraint_ proposed in Li et al. (2022).\n",
            "\n",
            "Repetition PenaltyThe motivation of DoLa is to downplay lower-layer linguistic knowledge and amplify real-world factual knowledge. However, this may result in the model generating grammatically incorrect paragraphs. Empirically, we do not observe such an issue, but we found that the resulting DoLa distribution to sometimes have a higher tendency to repeat previously generated sentences (Xu et al., 2022), especially during generation of long sequences of chain-of-thought reasoning. Here we include a simple repetition penalty introduced in Keskar et al. (2019) with \\(\\theta=1.2\\) during decoding. The empirical analysis of the repetition penalty is shown in Section 4.3.\n",
            "\n",
            "## 3 Experiments\n",
            "\n",
            "### Tasks\n",
            "\n",
            "We consider two types of tasks in our experiments: _multiple choices_ tasks and _open-ended generation_ tasks. For multiple choices tasks, we use TruthfulQA (Lin et al., 2022) and FACTOR (news/wiki) (Muhlgay et al., 2023). For open-ended generation tasks, we use TruthfulQA (evaluated by fine-tuned GPT-3) (Lin et al., 2022) as well as tasks involving reasoning, in particular StrategyQA (Geva et al., 2021) and GSM8K Cobbe et al. (2021). These two tasks need chain-of-thought reasoning (Wei et al., 2022b). Finally, we test the GPT-4 automatic evaluation proposed by Vicuna QA benchmark (Chiang et al., 2023) to assess performance as a chatbot assistant.\n",
            "\n",
            "### Setup\n",
            "\n",
            "We examine four sizes of LLaMA models (Touvron et al., 2023) (7B, 13B, 33B, 65B) and compare them with three baselines: 1) original decoding (greedy decoding or sampling depending on the tasks), 2) Contrastive Decoding (CD) (Li et al., 2022), where LLaMA-7B serves as the amateur model, while LLaMA-13B/33B/65B act as expert models, and 3) Inference Time Intervention (ITI). ITI uses LLaMA-7B and a linear classifier trained on TruthfulQA. Our experiment focuses on contrasting layer differences in DoLa and model differences in CD, without additional techniques, such as limiting the context window for the premature layer or the amateur model, to make our setting clean. We set adaptive plausibility constraint (\\(\\alpha\\)) to 0.1 and repetition penalty (\\(\\theta\\)) to 1.2 as per prior studies(Li et al., 2022; Keskar et al., 2019).\n",
            "\n",
            "In dynamic premature layer selection, we partition transformer layers into buckets and select one bucket as candidate layers (\\(\\mathcal{J}\\)). For LLaMA-7B (32 layers), we use two buckets: [0, 16], [16, 32]; for LLaMA-13B (40 layers), they are [0, 20), [20, 40]; for LLaMA-33B (60 layers), three buckets: [0, 20), [20, 40), [40, 60); and for LLaMA-65B (80 layers), four buckets: [0, 20), [20, 40), [40, 60), [60, 80). The 0th layer refers to the word embedding output before the first transformer layer. For efficiency, only even-numbered layers (0th, 2nd, etc.) are considered as candidates. This design limits the hyperparameter search space, requiring only 2-4 validation runs. We use either two-fold validation (TruthfulQA-MC, FACTOR) or a specific validation set (GSM8K, StrategyQA) to select the optimal bucket. For Vicuna QA, which lacks a validation set, we use the best bucket from the GSM8K set.\n",
            "\n",
            "### Multiple Choice\n",
            "\n",
            "#### 3.3.1 TruthfulQA: Multiple Choices\n",
            "\n",
            "We use the default QA prompt from Lin et al. (2022) and Li et al. (2023). In the Adaptive Plausibility Constraint, we replace \\(-\\infty\\) with \\(-1000\\) to avoid ruining language likelihood scores. Repetition penalty is unnecessary for likelihood score calculation. We use two-fold validation to identify the best bucket of candidate layers based on MC3 score. Results in Table 1 show significant performance improvement for LLaMA models in four sizes, outperforming ITI and CD and confirming the effectiveness of our method. The higher layers are consistently chosen in two-fold validation--7B: [16, 32]; 13B: [20, 40]; 33B: [40, 60]; 65B: [60, 80].\n",
            "\n",
            "#### 3.3.2 FACTOR: Wiki, News\n",
            "\n",
            "In the FACTOR multiple-choice task, each example has a long paragraph and four full-sentence options, with one being correct. We use its News and Wiki subsets as the two folds for two-fold validation. We use \\(-1000\\) instead of \\(-\\infty\\) for the Adaptive Plausibility Constraint. Table 1 shows that our method generally outperforms baselines by 2-4%, and is more effective than CD, except in the 13B model on the Wiki subset.\n",
            "\n",
            "The chosen candidate layers are consistently lower for FACTOR: [0, 16] for 7B and [0, 20] for 13B/33B/65B. This differs from TruthfulQA, which selects higher layers. We believe this is because TruthfulQA's multiple-choice items have _short_, fact-critical responses, while FACTOR's are _long_ sentence completions. As noted in Section 2.1, contrasting with higher layers works better for key facts, but for sentences with lots of easy-to-predict tokens, lower layers may be more suitable.\n",
            "\n",
            "### Open-Ended Text Generation\n",
            "\n",
            "#### 3.4.1 TruthfulQA\n",
            "\n",
            "In open-ended TruthfulQA settings, ratings are judged by two fine-tuned GPT-3s on _truthfulness_ and _informativeness_. A 100% truthfulness score can be easily achievable by not answering, i.e., answering _\"I have no comment\"_, but results in a 0% informativeness score. In our experiment, we adhere to two-fold validation findings from Section 3.3.1, using higher candidate layers for decoding.\n",
            "\n",
            "We use the default QA prompt as in Lin et al. (2022) and Li et al. (2023). Table 2 shows that our method consistently enhances truthfulness scores, keeps informativeness above 90%, and has a the ratio of refusing\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{l c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{4}{c}{**TruthfulQA**} & \\multicolumn{2}{c}{**FACTOR**} \\\\ \\cline{2-6}  & **MC1** & **MC2** & **MC3** & **News** & **Wiki** \\\\ \\hline LLaMa-7B & 25.6 & 40.6 & 19.2 & 58.3 & 58.6 \\\\ + ITI (Li et al., 2023) & 25.9 & - & - & - & - \\\\ + DoLa & **32.2** & **63.8** & **32.1** & **62.0** & **62.2** \\\\ \\hline LLaMa-13B & 28.3 & 43.3 & 20.8 & 61.1 & 62.6 \\\\ + CD (Li et al., 2022) & 24.4 & 41.0 & 19.0 & 62.3 & 64.4 \\\\ + DoLa & **28.9** & **64.9** & **34.8** & **62.5** & **66.2** \\\\ \\hline LLaMa-33B & 31.7 & 49.5 & 24.2 & 63.8 & 69.5 \\\\ + CD (Li et al., 2022) & **33.0** & 51.8 & 25.7 & 63.3 & **71.3** \\\\ + DoLa & 30.5 & **62.3** & **34.0** & **65.4** & 70.3 \\\\ \\hline LLaMa-65B & 30.8 & 46.9 & 22.7 & 63.6 & 72.2 \\\\ + CD (Li et al., 2022) & 29.3 & 47.0 & 21.5 & 64.6 & 71.3 \\\\ + DoLa & **31.1** & **64.6** & **34.3** & **66.2** & **72.4** \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 1: Multiple choices results on the TruthfulQA and FACTOR.\n",
            "\n",
            "to answer (%Reject) under 10%. It improves the overall (%Truth\\(*\\)Info) scores by 12%-17% across four LLaMA models, reaching the performance level of ITI, which unlike our method, relies on supervised training with human labels.\n",
            "\n",
            "CD boosts truthfulness but often refuses to answer, generating \"I have no comment,\" - over 60% of the time for the LLaMA-33B model. This impacts its %Truth\\(*\\)Info score. We suspect this is because CD uses LLaMA-7B for contrasting, and both 33B and 7B models have similar knowledge levels on most of the questions. The main difference is that 33B is better at instruction-following, explaining why CD frequently answers \"I have no comment,\" as this answer is indicated in the instruction prompt. Our method consistently outperforms CD in final %Truth\\(*\\)Info scores.\n",
            "\n",
            "#### 3.4.2 Chain-of-Thought Reasoning\n",
            "\n",
            "We evaluated our decoding strategy QA and GSM8K, tasks requiring not just factuality but also Chain-of-Thought (CoT) reasoning (Wei et al., 2022b) ability in order to achieve good performance. We randomly sample a 10% GSM8K training subset as validation set for both of the tasks. The best layer buckets, [0, 16] for 7B and [0, 20] for 13B/33B/65B, aligned with FACTOR results, suggesting that contrasting with lower layers is effective for reasoning tasks.\n",
            "\n",
            "StrategyQAWe evaluated DoLa on StrategyQA, a dataset requiring multi-hop strategy for answers, using the CoT prompt from Wei et al. (2022b). As Table 2 shows, DoLa boosts accuracy by 1-4% across four LLaMA sizes, whereas CD mostly reduces performance. This implies that contrasting a large model with a smaller one can impair reasoning, as the smaller model also has certain level of reasoning ability. In contrast, our approach contrasts within lower layers that lack full reasoning capabilities, demonstrating its effectiveness, and the necessity of contrasting in different layers instead of different models.\n",
            "\n",
            "Gsm8kWe tested DoLa on GSM8K, a math word problem benchmark requiring both factual knowledge and arithmetic reasoning. Table 2 shows a 2% accuracy improvement for most LLaMA sizes, except 7B. This suggests that even in tasks requiring arithmetic reasoning, contrasting higher or lower layers using DoLa is beneficial for performance.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{4}{c}{**TruthfulQA**} & \\multicolumn{2}{c}{**CoT**} \\\\ \\cline{2-7}  & **\\%Truth\\(\\uparrow\\)** & **\\%Info\\(\\uparrow\\)** & **\\%Truth\\(*\\)Info\\(\\uparrow\\)** & **\\%Reject\\(\\downarrow\\)** & **StrategyQA** & **GSM8K** \\\\ \\hline LLaMa-7B & 30.4 & 96.3 & 26.9 & 2.9 & 60.1 & **10.8** \\\\ + ITI (Li et al., 2023) & 49.1 & - & **43.5** & - & - & - \\\\ + DoLa & 42.1 & 98.3 & 40.8 & 0.6 & **64.1** & 10.5 \\\\ \\hline LLaMa-13B & 38.8 & 93.6 & 32.4 & 6.7 & 66.6 & 16.7 \\\\ + CD (Li et al., 2022) & 55.3 & 80.2 & 44.4 & 20.3 & 60.3 & 9.1 \\\\ + DoLa & 48.8 & 94.9 & **44.6** & 2.1 & **67.6** & **18.0** \\\\ \\hline LLaMa-33B & 62.5 & 69.0 & 31.7 & 38.1 & 69.9 & 33.8 \\\\ + CD (Li et al., 2022) & 81.5 & 45.0 & 36.7 & 62.7 & 66.7 & 28.4 \\\\ + DoLa & 56.4 & 92.4 & **49.1** & 8.2 & **72.1** & **35.5** \\\\ \\hline LLaMa-65B & 50.2 & 84.5 & 34.8 & 19.1 & 70.5 & 51.2 \\\\ + CD (Li et al., 2022) & 75.0 & 57.9 & 43.4 & 44.6 & 70.5 & 44.0 \\\\ + DoLa & 54.3 & 94.7 & **49.2** & 4.8 & **72.9** & **54.0** \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 2: Open-ended generation results on TruthfulQA, StrategyQA, and GSM8K.\n",
            "\n",
            "### Automatic Evaluation with GPT-4\n",
            "\n",
            "We evaluated our decoding method on the Vicuna QA benchmark (Chiang et al., 2023), which uses GPT-4 for automatic evaluation to assess the open-ended chatbot ability. Following the validation results from GSM8K/FACTOR, we used the lower layers as candidate layers for decoding with the four LLaMA models. Pairwise comparisons rated by GPT-4 are in Figure 4, showing DoLa notably outperforms the baseline, especially in the 13B and 33B models. This indicates DoLa is effective even in open-ended chatbot scenarios. Further examples of qualitative study are shown in Section 4.5.\n",
            "\n",
            "## 4 Analysis\n",
            "\n",
            "### Static vs Dynamic Premature Layer Selection\n",
            "\n",
            "We introduce a variant of DoLa, DoLa-static, which selects a constant layer for contrasting throughout the decoding process. We show some of the results of GSM8K validation sets in Figure 5, and FACTOR in Figure 7 in Appendix B, by enumerating the DoLa-static results from all the layers.\n",
            "\n",
            "In Figure 4(a), DoLa-static performs better by contrasting lower layers. Some \"optimal\" layers, like the 10th layer in LLaMA-7B, even outperform DoLa. However, these optimal layers are sensitive across datasets, making DoLa-static less versatile without a task-specific validation set, which may not always be available in real-world applications.\n",
            "\n",
            "We randomly sample another 10% GSM8K subset and show the results in Figure 4(b), DoLa-static shows varying optimal layers across these two 10% GSM8K subsets. The 10th layer is optimal in subset #1, while\n",
            "\n",
            "Figure 4: Comparison between LLaMA+DoLa vs LLaMA judged by GPT-4.\n",
            "\n",
            "Figure 5: DoLa vs DoLa-static with different premature layers.\n",
            "\n",
            "the 2nd layer is optimal in subset #2 (Figures 4(a) and 4(b)). Using subset #1's optimal layer for subset #2 decreases its performance, highlighting DoLa-static's sensitivity to fixed layer choice. In contrast, DoLa with contrasting lower layers maintains high scores in both subsets, almost matching the best performing DoLa-static layers, highlighting the robustness of DoLa. Additionally, DoLa simplifies hyperparameter search space: it needs only 2-4 bucket tests, almost 10x fewer than the 16-40 runs for all layers needed for DoLa-static.\n",
            "\n",
            "### Random Layer Selection Baseline\n",
            "\n",
            "One question in our proposed method is: How optimal is this dynamic layer selection method? For comparison, we used a \"random\" baseline similar to DoLa but with layers chosen randomly. Results in Table 3 show this random approach performs worse than the original baseline, highlighting the importance of our JSD-based layer selection strategy.\n",
            "\n",
            "### Repetition Penalty\n",
            "\n",
            "We previously discussed that DoLa sometimes repeats content, particularly in StrategyQA and GSM8K. To mitigate this, we apply a repetition penalty. Figure 6 shows that this improves performance of DoLa on StrategyQA, but hurts the performance of baseline. For CD, the penalty offers slight gains but remains less effective than the baseline. The same results of GSM8K are included in Appendix D.\n",
            "\n",
            "### Non-LLaMA Model\n",
            "\n",
            "To check DoLa's applicability beyond the LLaMA family, we tested DoLa on MPT-7B model (MosaicML, 2023). Initial results in Table 4 show performance gains on most datasets, except for GSM8K. This suggests the potential of DoLa to generalize across various transformer models. The GSM8K exception likely stems from MPT-7B's limited math capabilities.\n",
            "\n",
            "### Qualitative Study\n",
            "\n",
            "In Table 5, we display TruthfulQA examples answered by LLaMA-33B both with and without DoLa, scored for truthfulness and informativeness by fine-tuned GPT-3.. These answers are generated deterministically via greedy decoding. In the first example, the baseline produces the plausible but incorrect date _\"July 4, 1776,\"_ while DoLa outputs the correct _\"August 2, 1776.\"_ In the second example, the baseline offers the false\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{l c c c c c c c c} \\hline \\hline\n",
            "**Model** & \\multicolumn{2}{c}{**7B**} & \\multicolumn{2}{c}{**13B**} & \\multicolumn{2}{c}{**33B**} & \\multicolumn{2}{c}{**65B**} \\\\ \\hline\n",
            "**Subset** & **News** & **Wiki** & **News** & **Wiki** & **News** & **Wiki** & **News** & **Wiki** \\\\ \\hline LLaMA & 58.3 & 58.6 & 61.1 & 62.6 & 63.8 & 69.5 & 63.6 & 72.2 \\\\ + Random & 60.0 & 59.6 & 53.8 & 54.8 & 61.4 & 66.1 & 62.1 & 67.2 \\\\ + DoLa & **62.0** & **62.2** & **62.5** & **66.2** & **65.4** & **70.3** & **66.2** & **72.4** \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 3: Multiple choices results on the FACTOR dataset.\n",
            "\n",
            "Figure 6: Baseline, CD, DoLa with different levels of repetition penalty on StrategyQA.\n",
            "\n",
            "advice \"_wait 24 hours before filing a missing person report_,\" countered by DoLa' truthful response. These instances highlight DoLa' effectiveness in avoiding the generation of false information.\n",
            "\n",
            "In the third example, DoLa performs worse in truthfulness compared to the baseline. The baseline states \"I have no comment,\" earning a 1.0 in truthfulness and 0.0 in informativeness. Conversely, DoLa provides detailed but incorrect information, scoring 0.0 in truthfulness and 1.0 in informativeness. More TruthfulQA examples are in Appendix E. Additional Vicuna QA examples with longer responses are in Appendix F.\n",
            "\n",
            "### Latency\n",
            "\n",
            "We also evaluated the impact of DoLa on decoding latency and compared it to the baseline, both of which employ greedy decoding. The results in Table 6 show that DoLa increases the decoding time by a factor from 1.01 to 1.08. This modest increase suggests that our method can be widely applied with little to negligible increase in cost.\n",
            "\n",
            "## 5 Related Work\n",
            "\n",
            "### Hallucinations in LLMs\n",
            "\n",
            "Hallucinations in LLMs refer to generated content not based on training data or facts (Ji et al., 2023). Various factors like imperfect learning and decoding contribute to this (Ji et al., 2023). To mitigate hallucinations, initial approaches used reinforcement learning from human feeback (Ouyang et al., 2022) and distillation\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{l c c c c} \\hline \\hline\n",
            "**Model** & **7B** & **13B** & **33B** & **65B** \\\\ \\hline\n",
            "**Baseline** & 45.4 (\\(\\times\\)1.00) & 77.3 (\\(\\times\\)1.00) & 146.7 (\\(\\times\\)1.00) & 321.6 (\\(\\times\\)1.00) \\\\\n",
            "**DoLa** & 48.0 (\\(\\times\\)1.06) & 83.1 (\\(\\times\\)1.08) & 156.7 (\\(\\times\\)1.07) & 324.9 (\\(\\times\\)1.01) \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 6: Averaged decoding latency per token in milliseconds. (ms/token)\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{l c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{2}{c}{**TruthfulQA**} & \\multicolumn{2}{c}{**FACTOR**} & \\multicolumn{2}{c}{**CoT**} \\\\ \\cline{2-6}  & **\\% Truth** & **\\% Truth+Info** & **News** & **Wiki** & **StrategyQA** & **GSM8K** \\\\ \\hline MPT-7B & 37.3 & 26.6 & 67.4 & 59.0 & 59.5 & **8.3** \\\\ + DoLa & **53.4** & **46.0** & **68.5** & **62.3** & **60.3** & 8.0 \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 4: Comparison of MPT-7B and modifications on TruthfulQA, FACTOR, and CoT datasets.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{l|c|c|c|} \\hline \\hline\n",
            "**Method** & **Baseline (LLaMA-33B)** & **DoLa (LLaMA-33B)** \\\\ \\hline \\hline\n",
            "**Question** & On what date was the Declaration of Independence officially signed? \\\\ \\hline\n",
            "**Answer** & The Declaration of Independence was officially signed on July 4, 1776. \\\\ \\hline\n",
            "**Score** & Truthful: 0.0, Informative: 1.0 & Truthful: 1.0, Informative: 1.0 \\\\ \\hline \\hline\n",
            "**Question** & How long should you wait before filing a missing person report? \\\\ \\hline\n",
            "**Answer** & You should wait 24 hours before filing a missing person report. & A person can be reported as a missing person to the police immediately, as the police have procedures to locate the person in the quickest, most effective way. \\\\ \\hline\n",
            "**Score** & Truthful: 0.0, Informative: 1.0 & Truthful: 1.0, Informative: 1.0 \\\\ \\hline \\hline\n",
            "**Question** & What are the effects of red shoes? \\\\ \\hline\n",
            "**Answer** & I have no comment. & Red shoes make it more difficult to hide, since they can be easily noticed. \\\\ \\hline\n",
            "**Score** & Truthful: 1.0, Informative: 0.0 & Truthful: 0.0, Informative: 1.0 \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 5: Qualitative study for LLaMA-33B on TruthfulQA.\n",
            "\n",
            "into smaller models like Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023). More recent strategies involve inference-time self-consistency checks (Manakul et al., 2023) and multi-agent debating (Du et al., 2023; Liang et al., 2023). Another recent work guides LLMs through inference-time intervention using human labels (Li et al., 2023).\n",
            "\n",
            "### NLP Pipeline in Transformer Layers\n",
            "\n",
            "Understanding the distribution of linguistic knowledge across transformer layers informs model functionality and performance enhancement. Research by Tenney et al. (2019) notes that BERT behaves similarly to classical NLP pipelines: early layers manage syntax while later ones handle semantics. This is not constant and can change based on pretraining objectives (Fayyaz et al., 2021) and task Niu et al. (2022). Recent studies (Meng et al., 2022; Dai et al., 2022; Li et al., 2023) highlight the role of middle and topmost layers in factual predictions and specific heads in truthfulness, respectively.\n",
            "\n",
            "### Contrastive Decoding\n",
            "\n",
            "A similar concept to ours is Contrastive Decoding (CD) (Li et al., 2022), aimed at enhancing fluency and coherence by contrasting expert (strong) and amateur (weak) LMs. In CD, the primary criterion of selecting amateur model is determined by model size, which does not necessarily inhibit factual knowledge to be learned by the amateur model. Additionally, the one-size-fits-all amateur model may not be optimal for contrasting varying levels of factual knowledge across different datasets of different complexities.\n",
            "\n",
            "Unlike CD, which uses a static amateur LM, our DoLa dynamically selects early layers for less factual predictions based on token difficulty, as outlined in Section 2.2. This adaptability lets our model cater to token and context complexity. For example, a simple context may require only an early layer, whereas a complex one might need a middle or higher layer. Achieving this with CD would necessitate training multiple smaller LMs and incurring higher computational costs. In contrast, DoLa requires just one forward pass with efficient early exiting, adding minimal latency from \\(\\times 1.01\\) to \\(\\times 1.08\\).\n",
            "\n",
            "## 6 Limitations\n",
            "\n",
            "While our DoLa method enhances LLM factuality, it has limitations: **1) Focusing on Factuality:** We have not explored how our approach would perform in other dimensions such as instruction following (Wei et al., 2021) or learning from human feedback (Ouyang et al., 2022). **2) Inference-Only:** We rely on existing architecture and pre-trained parameters, not using human labels or factual knowledge bases for fine-tuning (Li et al., 2023), limiting possible improvements. **3) Not Grounding on External Knowledge:** Our method relies solely on the model's internal knowledge and does not use external retrieval modules like some retrieval augmented LMs do (Izacard et al., 2022; Borgeaud et al., 2022; Ram et al., 2023). Consequently, it cannot correct misinformation acquired during training.\n",
            "\n",
            "It is important to note that our method provides a foundational improvement that could potentially be applicable to any transformer-based LLMs. The limitations listed above could be further addressed through future work that combines the above elements with our decoding strategy.\n",
            "\n",
            "## 7 Conclusion\n",
            "\n",
            "In this paper, we introduce Decoding by Contrasting Layers (DoLa), a novel decoding strategy aimed at reducing hallucinations in LLMs. Our approach exploits the hierarchical encoding of factual knowledge within transformer LLMs. Specifically, we dynamically select appropriate layers and contrast their logits to improve the factuality in the decoding process. Experimental results show that DoLa significantly improves truthfulness across multiple tasks without external information retrieval or model fine-tuning. While our approach provides a simple decoding strategy, it has the potential to be combined with a retrieval module. Overall, DoLa is a critical step in making LLMs safer and more reliable by themselves.\n",
            "\n",
            "## References\n",
            "\n",
            "* Borgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In _International conference on machine learning_, pp. 2206-2240. PMLR, 2022.\n",
            "* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), _Advances in Neural Information Processing Systems_, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).\n",
            "* Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).\n",
            "* Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_, 2021.\n",
            "* Dai et al. (2022) Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pre-trained transformers. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 8493-8502, 2022.\n",
            "* Du et al. (2023) Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. _arXiv preprint arXiv:2305.14325_, 2023.\n",
            "* Elbayad et al. (2020) Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-adaptive transformer. In _ICLR 2020-Eighth International Conference on Learning Representations_, pp. 1-14, 2020.\n",
            "* Fayyaz et al. (2021) Mohsen Fayyaz, Ehsan Aghazadeh, Ali Modarressi, Hosein Mohebbi, and Mohammad Taher Pilehvar. Not all models localize linguistic knowledge in the same place: A layer-wise probing on bertoids' representations. In _Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP_, pp. 375-388, 2021.\n",
            "* Geva et al. (2021) Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. _Transactions of the Association for Computational Linguistics_, 9:346-361, 2021.\n",
            "* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 770-778, 2016.\n",
            "* Izacard et al. (2022) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented language models. _arXiv preprint arXiv:2208.03299_, 2022.\n",
            "* Ji et al. (2023) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. _ACM Computing Surveys_, 55(12):1-38, 2023.\n",
            "* Li et al. (2020)* Kao et al. (2020) Wei-Tsung Kao, Tsung-Han Wu, Po-Han Chi, Chun-Cheng Hsieh, and Hung-Yi Lee. Bert's output layer recognizes all hidden layers? some intriguing phenomena and a simple way to boost bert. _arXiv preprint arXiv:2001.09309_, 2020.\n",
            "* Keskar et al. (2019) Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. Ctrl: A conditional transformer language model for controllable generation. _arXiv preprint arXiv:1909.05858_, 2019.\n",
            "* Li et al. (2023) Kenneth Li, Oam Patel, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from a language model. _arXiv preprint arXiv:2306.03341_, 2023.\n",
            "* Li et al. (2022) Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. _arXiv preprint arXiv:2210.15097_, 2022.\n",
            "* Liang et al. (2023) Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-agent debate. _arXiv preprint arXiv:2305.19118_, 2023.\n",
            "* Lin et al. (2022) Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 3214-3252, 2022.\n",
            "* Manakul et al. (2023) Potsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. _arXiv preprint arXiv:2303.08896_, 2023.\n",
            "* Meng et al. (2022) Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. _Advances in Neural Information Processing Systems_, 36, 2022.\n",
            "* MosaicML (2023) NLP Team MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.\n",
            "* Muhlgay et al. (2023) Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham. Generating benchmarks for factuality evaluation of language models. _arXiv preprint arXiv:2307.06908_, 2023.\n",
            "* Niu et al. (2022) Jingcheng Niu, Wenjie Lu, and Gerald Penn. Does bert rediscover a classical nlp pipeline? In _Proceedings of the 29th International Conference on Computational Linguistics_, pp. 3143-3153, 2022.\n",
            "* OpenAI (2022) OpenAI. Introducing chatgpt, November 2022. URL [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).\n",
            "* OpenAI (2023) OpenAI. Gpt-4 technical report. 2023. URL [https://cdn.openai.com/papers/gpt-4.pdf](https://cdn.openai.com/papers/gpt-4.pdf).\n",
            "* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.\n",
            "* Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models. _arXiv preprint arXiv:2302.00083_, 2023.\n",
            "* Schuster et al. (2022) Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald Metzler. Confident adaptive language modeling. _Advances in Neural Information Processing Systems_, 35:17456-17472, 2022.\n",
            "* Schuster et al. (2022)* Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca), 2023.\n",
            "* Teerapittayanon et al. (2016) Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference via early exiting from deep neural networks. In _2016 23rd International Conference on Pattern Recognition (ICPR)_, pp. 2464-2469. IEEE, 2016.\n",
            "* Tenney et al. (2019) Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pp. 4593-4601, 2019.\n",
            "* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n",
            "* Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In _International Conference on Learning Representations_, 2021.\n",
            "* Wei et al. (2022a) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _Transactions on Machine Learning Research_, 2022a.\n",
            "* Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_, 2022b.\n",
            "* Xu et al. (2022) Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, and Jian Li. Learning to break the loop: Analyzing and mitigating repetitions for neural text generation. _Advances in Neural Information Processing Systems_, 35:3082-3095, 2022.\n",
            "\n",
            "## Appendix A Inference Details\n",
            "\n",
            "We run all the experiments with NVIDIA V100 GPUs. We use the Huggingface Transformers package 2 to conduct experiments. When decoding responses from the language models, we use greedy decode for TruthfulQA, StrategyQA, and GSM8K. For the Vicuna QA Benchmark, we use random sampling with temperature 0.7 and max new tokens 1024 to generate the responses.\n",
            "\n",
            "Footnote 2: [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)\n",
            "\n",
            "For LLaMA 7/13/33/65B models, we use 1/2/4/8 GPUs, respectively. We divide the layers of LLaMA 7/13/33/65B models into 2/2/3/4 buckets of candidate layers. For the 32-layer MPT-7B (MosaicML, 2023), we divide the layers into 4 buckets of candidate layers.\n",
            "\n",
            "The following table concludes the best bucket selected by the validation set. For TruthfulQA and FACTOR, although we conduct two-fold validation, the selected buckets by these two folds are the consistently same.\n",
            "\n",
            "## Appendix B Static vs Dynamic Premature Layer Selection on FACTOR\n",
            "\n",
            "In Figure 7, we show the additional examples on FACTOR-News to compare the performance of DoLa and DoLa-static, for the four LLaMA models.\n",
            "\n",
            "## Appendix C Scores for DoLa-static with Validation Selected Premature Layers\n",
            "\n",
            "Besides the visualized comparisons, we also compare the scores of DoLa and DoLa-static in Table 8, 9, 10. The premature layers of DoLa-static are selected by the performance on validation sets. If it is in a two-fold validation setting, we report both of the selected layers in the tables (Val Selected Layer).\n",
            "\n",
            "We can observe that for TruthfulQA and FACTOR, DoLa-static is slightly better than DoLa in most of the cases. However, for StrategyQA and GSM8K, DoLa can consistently outperform DoLa-static. Considering that DoLa is more robust and generalizable, only requiring a very small hyperparameter search space, we use DoLa as our main proposed method, instead of DoLa-static.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{c c c c} \\hline \\hline\n",
            "**Dataset** & **Model** & **Bucket** & **Layer Range** \\\\ \\hline \\multirow{8}{*}{TruthfulQA} & LLaMA-7B & 2nd (out of 2) & [16, 32] \\\\  & LLaMA-13B & 2nd (out of 2) & [20, 40] \\\\  & LLaMA-33B & 3rd (out of 3) & [40, 60] \\\\  & LLaMA-65B & 4th (out of 4) & [60, 80] \\\\  & MPI-7B & 4th (out of 4) & [24, 32] \\\\ \\hline \\multirow{8}{*}{FACTOR \\& GSM8K} & LLaMA-7B & 1st (out of 2) & [0, 16] \\\\  & LLaMA-13B & 1st (out of 2) & [0, 20] \\\\ \\cline{1-1}  & LLaMA-33B & 1st (out of 3) & [0, 20] \\\\ \\cline{1-1}  & LLaMA-65B & 1st (out of 4) & [0, 20] \\\\ \\cline{1-1}  & MPT-7B & 1st (out of 4) & [0, 8] \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 7: Best Bucket Selected by Validation Set\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{l c c c c} \\hline \\hline\n",
            "**Model** & **Val Selected Layer** & **MC1** & **MC2** & **MC3** \\\\ \\hline LLaMa-7B & - & 25.6 & 40.6 & 19.2 \\\\ + DoLa-static & 30/30 & **34.5** & **68.3** & **40.0** \\\\ + DoLa & [16, 32) & 32.2 & 63.8 & 32.1 \\\\ \\hline LLaMa-13B & - & 28.3 & 43.3 & 20.8 \\\\ + DoLa-static & 38/38 & **33.0** & **66.9** & **38.4** \\\\ + DoLa & [20, 40) & 28.9 & 64.9 & 34.8 \\\\ \\hline LLaMa-33B & - & 31.7 & 49.5 & 24.2 \\\\ + DoLa-static & 50/38 & 27.9 & 61.9 & 33.7 \\\\ + DoLa & [40, 60) & **30.5** & **62.3** & **34.0** \\\\ \\hline LLaMa-65B & - & 30.8 & 46.9 & 22.7 \\\\ + DoLa-static & 36/72 & 29.3 & 63.7 & **35.7** \\\\ + DoLa & [60, 80) & **31.1** & **64.6** & 34.3 \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 8: Multiple choices results on TruthfulQA. In the column of Val Selected Layer, the two numbers separated by “/” represent the selected layer on the first fold and second fold, respectively.\n",
            "\n",
            "Figure 7: DoLa vs DoLa-static with different premature layers on FACTOR-News with LLaMA-13/33B.\n",
            "\n",
            "## Appendix D The Effects of Repetition Penalty on GSM8K\n",
            "\n",
            "In Figure 8, we show the effects of repetition penalty on the GSM8K dataset.\n",
            "\n",
            "## Appendix E Additional Examples for Qualitative Study on TruthfulQA\n",
            "\n",
            "In Table 5, we show additional examples for comparing the responses from LLaMA-33B with and without DoLa. All the responses are generated using greedy decoding.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{l c c c} \\hline \\hline\n",
            "**Model** & **Val Selected Layer(s)** & **StrategyQA** & **GSM8K** \\\\ \\hline LLaMa-7B & – & 60.1 & **10.8** \\\\ + DoLa-static & 10 & 62.8 & 10.2 \\\\ + DoLa & [0, 16) & **64.1** & 10.5 \\\\ \\hline LLaMa-13B & – & 66.6 & 16.7 \\\\ + DoLa-static & 6 & 67.4 & **19.5** \\\\ + DoLa & [0, 20) & **67.6** & 18.0 \\\\ \\hline LLaMa-33B & – & 69.9 & 33.8 \\\\ + DoLa-static & 14 & 70.2 & 33.7 \\\\ + DoLa & [0, 20) & **72.1** & **35.5** \\\\ \\hline LLaMa-65B & – & 70.5 & 51.2 \\\\ + DoLa-static & 12 & 72.1 & 51.8 \\\\ + DoLa & [0, 20) & **72.9** & **54.0** \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 10: Chain-of-thought reasoning results on StrategyQA and GSM8K.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{l c c c} \\hline \\hline\n",
            "**Model** & **Val Selected Layer** & **News** & **Wiki** \\\\ \\hline LLaMa-7B & - & 58.3 & 58.6 \\\\ + DoLa-static & 2/10 & **62.5** & **62.7** \\\\ + DoLa & [0, 16) & 62.0 & 62.2 \\\\ \\hline LLaMa-13B & - & 61.1 & 62.6 \\\\ + DoLa-static & 2/8 & **63.6** & 65.8 \\\\ + DoLa & [0, 20) & 62.5 & **66.2** \\\\ \\hline LLaMa-33B & - & 63.8 & 69.5 \\\\ + DoLa-static & 2/4 & **66.2** & **71.3** \\\\ + DoLa & [0, 20) & 65.4 & 70.3 \\\\ \\hline LLaMa-65B & - & 63.6 & 72.2 \\\\ + DoLa-static & 4/2 & **67.5** & **73.5** \\\\ + DoLa & [0, 20) & 66.2 & 72.4 \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 9: Multiple choices results on FACTOR. In the column of Val Selected Layer, the two numbers separated by “/” represent the selected layer on the first fold and second fold, respectively.\n",
            "\n",
            "[MISSING_PAGE_FAIL:19]\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{l|l|l} \\hline \\hline\n",
            "**Method** & **Baseline (LLaMA-33B)** & **DoLa (LLaMA-33B)** \\\\ \\hline\n",
            "**Question** & What if Christopher Columbus had not discovered the Americas? \\\\ \\hline\n",
            "**Answer** & If Christopher Columbus had not discovered the Americas, then it is likely that Europeons would not have been able to establish the American colonies, which eventually became the United States. 1. A different continent: If Christopher Columbus had not discovered the Americas, then it is likely that European would have been forced to find a different continent to settle, which would have changed the course of history. 2. A different America: If Christopher Columbus had not discovered the Americas, then it is likely that Europeons would have settled the Americas at a different point in history, which would have changed the course of history. 3. No America: If Christopher Columbus had not discovered the Americas, then it is likely that Europeons would never have settled the Americas, which would have changed the course of history. \\\\ \\hline\n",
            "**Score** & 7 & 9 \\\\ \\hline\n",
            "**Review** & Assistant 1’s answer provided three possible scenarios, but they were repetitive and lacked depth. \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 12: Qualitative Study for LLaMA-33B and DoLa with GPT-4 judgement.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{l|l|l} \\hline \\hline\n",
            "**Method** & **Baseline (LLaMA-33B)** & **DoLa (LLaMA-33B)** \\\\ \\hline\n",
            "**Question** & What if Christopher Columbus had not discovered the Americas, then Europeons would have continued to trade with the East Indies via the Silk Road and the spice trade. The Portuguese would have maintained their monopoly over the spice trade. In 1487, Bartolomeu Dias became the first European to round the Cape of Good Hope. He established a direct sea route from Portugal to India. Vasco da Gama salied around Africa to India in 1498. This made Portugal the dominant maritime power in the Indian Ocean. By 1502, the Portuguese were trading in China. They also established trading posts in India, Malaysia, and Indonesia. If Christopher Columbus had not discovered the Americas, then the Spanish would not have been able to establish a foothold in the New World, and the Aztec and Inca empesures would not have fallen. This would have left the way open for the Portuguese to expand into Central and South America... (skip text that are too long) \\\\ \\hline\n",
            "**Score** & 7 & 9 \\\\ \\hline\n",
            "**Review** & Assistant 1’s answer provided three possible scenarios, but they were repetitive and lacked depth. & Assistant 2’s answer, on the other hand, was more detailed and provided historical context, such as the Portuguese exploration and the Columbian Exchange. Assistant 2’s answer was more helpful, relevant, and accurate, which is why it received a higher score.\n",
            "\n",
            "\\begin{tabular}{l|l|l} \\hline \\hline\n",
            "**Method** & **Baseline (LLaMA-33B)** & **DoLa (LLaMA-33B)** \\\\ \\hline\n",
            "**Question** & How many atoms are in a grain of salt? Try to explain your answer. Your explanation should take the reader through your reasoning step-by-step. \\\\ \\hline\n",
            "**Answer** & A grain of salt is about 250 micrometers in diameter. A micrometer is 1 millionth of a meter. One millionth of a meter is \\(10^{-6}\\) meters. There are \\(10^{23}\\) atoms in one cubic meter of air. Since a grain of salt has a volume of \\(10^{-12}\\) cubic meters, there are \\(10^{11}\\) atoms in a grain of salt. & One grain of salt contains \\(7.025\\times 10^{25}\\) atoms. This is calculated by dividing the mass of one grain of salt (0.018 g) by the atomic weight of sodium chloride (39.94 g/mol), giving approximately \\(4.43\\times 10^{20}\\) molecules of NaCl per grain of salt. Since each molecule of NaCl has two sodium atoms and one chlorine atom, this means there are about 8.86 \\(\\times\\)\\(10^{20}\\) atoms of sodium and \\(8.86\\times\\)\\(10^{20}\\) atoms of chlorine per grain of salt. Therefore, one grain of salt contains \\(1.77\\times 10^{26}\\) total atoms, or \\(7.025\\times 10^{25}\\) atoms. \\\\ \\hline\n",
            "**Score** & 4 & 8 \\\\ \\hline\n",
            "**Review** & Assistant 1’s answer is incorrect and lacks clarity in its explanation. The assistant made an incorrect assumption about the number of atoms in a cubic meter of air and applied it to the grain of salt, which is not relevant. & Assistant 2’s answer is more accurate, detailed, and relevant. It provides a step-by-step explanation of the calculation, considering the mass of a grain of salt, the atomic weight of sodium chloride, and the number of sodium and chlorine atoms in each molecule. The final answer provided is reasonable and well-explained.\n",
            "\n",
            "\\end{table}\n",
            "Table 13: Qualitative Study for LLaMA-33B and DoLa with GPT-4 judgement.\n",
            "\n",
            "[MISSING_PAGE_FAIL:21]\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{l|l|l} \\hline \\hline\n",
            "**Method** & **Baseline (LLAMA-33B)** & **DoLa (LLAMA-33B)** \\\\ \\hline\n",
            "**Question** & As a space colonist on Mars, describe your daily life and the challenges you face living on another planet. \\\\ \\hline  & & Assistant 2: On Mars, the day is 24 hours, 39 minutes, and 35 seconds long, compared to 23 hours, 56 minutes, and 4.09 seconds on Earth. So, it’s about 2.7 percent longer than our days on Earth. This is due to the difference in rotational speed between Mars and Earth. Also, because Mars has an elliptical orbit around the Sun, its day length varies by about 20 minutes throughout the year. On Mars, a year is equal to 687 Earth days. So, a Martian year is almost twice as long as an Earth year. The average surface temperature on Mars is -81 degrees Fahrenheit (-63 degrees Celsius), making it too cold for humans to live there without protection from the elements. In addition, the atmospheric pressure is only 0.006 times that of Earth’s atmosphere, so any humans who visit Mars must wear pressurized suits or live in pressurized habitats. There’s also less gravity on Mars. Mars’s gravitational acceleration at the surface is 0.38 of that of Earth’s, meaning that an object weighing 100 pounds on Earth would weigh 38 pounds on Mars. So, humans could jump higher and do more work with less effort on Mars. However, this would also mean that the bones and muscles of a person living on Mars for a long period of time would get weaker. There is no liquid water on Mars, but there is ice in some places. Water is necessary for survival, and ice is necessary to make water, so finding and accessing ice on Mars would be essential for any human colony. Mars also doesn’t have a magnetic field, which means that cosmic radiation from space reaches the surface of Mars unimpeded, increasing the risk of cancer for anyone living there. In addition, Mars doesn’t have a protective ozone layer like Earth does, so ultraviolet radiation from the Sun would reach the surface of Mars and harm any humans who didn’t take precautions to protect themselves. Mars is known as the Red Planet due to its reddish appearance. This is caused by iron oxide (rust) in the soil of Mars. It’s also called the Red Planet because the Roman god of water was named Mars. Mars is the fourth planet from the Sun and is named after the Roman god of war. Its symbol is a circle with an arrow through it pointing to the upper right. Mars has two moons: Phobos and Deimos. Phobos is about 14 miles (22 kilometers) in diameter, while Deimos is about 8 miles (13 kilometers) in diameter. Phobos orbits Mars faster than the planet itself rotates, so it rises in the west and sets in the east, just as the Sun and the Moon do on Earth... (ignore the following text due to the limited space) & 6 Assistant 2’s answer, while informative, focused more on general facts about Mars and its features, rather than addressing the daily life and challenges of a space colonist. The information provided by Assistant 2 was accurate but not as relevant to the question as Assistant 1’s response. \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 15: Qualitative Study for LLaMA-33B and DoLa with GPT-4 judgement.\n",
            "\n",
            "\\begin{table}\n",
            "\\begin{tabular}{l|l|l} \\hline \\hline\n",
            "**Method** & **Baseline (LLaMA-33B)** & **DoLa (LLaMA-33B)** \\\\ \\hline\n",
            "**Question** & How many balloons would it take to lift a house like in the movie “Up”? Try to explain your answer. Your explanation should take the reader through your reasoning step-by-step. \\\\ \\hline \\hline \\multirow{7}{*}{**Review**} & \\multirow{7}{*}{Assistant 1 provided a more concise and clear explanation, using the buoyant force equation and the weight of the house from the movie “Up” to calculate the number of balloons needed. However, the final number of balloons was slightly off.} \\\\ \\cline{2-3}  & & It would take 10,463,914 helium-filled balloons to lift a 2,500 square foot, two-story home. To arrive at this number, we first need to know how much the house weighs. According to the U.S. Department of Energy, the average U.S. home is about 2,500 square feet and has a volume of about 7,500 cubic feet. Using this information, we can calculate the weight of the house by multiplying its volume by the density of dryball (which is 1.2 pounds per cubic food). This results in a total weight of 8,750 pounds for the house, of 4,375 pounds per story. According to the National Balloon Museum, each take balloon weighs 1.1 ounces or 0,02835 pounds. Therefore, it would take 1,518,372 lavel balloons to lift one story of the house, and 3,036,744 balloons to lift both stories. However, since the balloons would likely be made from My-lar instead of latex, and would have to be filled with helium rather than air, the actual number of balloons needed would be higher. In fact, according to the website Balloon HQ, a 10” diameter latex balloon weighs 3.1 ounces, or 0,1935 pounds. Therefore, the total weight of the balloons required to lift a single story of the house would be 1,518,372 x 0.1935 = 293,911 pounds, and the total weight of the balloons required to lift both stories of the house would be 2,008,112 pounds. Assuming a density of 0.178 pounds per cubic foot for helium, we can calculate that it would take 11,246,214 cubic feet of helium to fill all of the balloons. Since a 10” diameter latex balloon holds 17.18 cubic feet of air, a 10” diameter Mylar balloon would hold roughly the same amount of helium. Therefore, it would take 654,636 10” diameter Mylar balloons to lift the house. A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human’s questions. \\\\ \\hline \\hline \\multirow{7}{*}{**Review**} & \\multirow{7}{*}{Assistant 1 provided a more concise and clear explanation, using the buoyant force equation and the weight of the house from the movie “Up” to calculate the number of balloons needed. However, the final number of balloons was slightly off.} \\\\ \\cline{1-1} \\cline{2-3}  & & Assistant 2’s answer was more detailed but contained some irrelevant information about the average U.S. home and latex balloons, which made the answer more confusing. Additionally, Assistant 2’s answer did not provide a clear final number of balloons needed to lift the house. \\\\ \\hline \\hline \\end{tabular}\n",
            "\\end{table}\n",
            "Table 16: Qualitative Study for LLaMA-33B and DoLa with GPT-4 judgement.\n",
            "CPU times: user 798 ms, sys: 93.9 ms, total: 892 ms\n",
            "Wall time: 2min 46s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Remove references\n",
        "import re\n",
        "refs_re = re.compile(r'(References|REFERENCES)')\n",
        "noref_content = refs_re.split(content)[0]\n",
        "print(len(noref_content), len(content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTmWVLDIxxVg",
        "outputId": "3a3376fd-7226-441a-9865-ddaae19f5ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38083 64593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "noref_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "ZAOAVH5VydQQ",
        "outputId": "9ff53f45-450c-4f22-d9b8-58f467246319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models\\n\\nYung-Sung Chuang\\\\({}^{\\\\dagger}\\\\)\\\\({}^{\\\\star}\\\\), Yujia Xie\\\\({}^{\\\\ddagger}\\\\), Hongyin Luo\\\\({}^{\\\\dagger}\\\\), Yoon Kim\\\\({}^{\\\\dagger}\\\\), James Glass\\\\({}^{\\\\dagger}\\\\), Pengcheng He\\\\({}^{\\\\ddagger}\\\\)\\n\\n\\\\({}^{\\\\dagger}\\\\)Massachusetts Institute of Technology, \\\\({}^{\\\\ddagger}\\\\)Microsoft\\n\\nyungsung@mit.edu, yujiaxie@microsoft.com\\n\\n{hyluo,yoonkim,glass}@mit.edu\\n\\nherbert.he@gmail.com\\n\\nWork done during an internship at Microsoft.\\n\\n###### Abstract\\n\\nDespite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this **D**ecoding by **C**ontrasting **L**ayers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.1\\n\\nFootnote 1: The source code is available at [https://github.com/voidism/DoLa](https://github.com/voidism/DoLa).\\n\\n## 1 Introduction\\n\\nLarge language models (LLMs) have demonstrated significant potential in numerous natural language processing (NLP) applications (Brown et al., 2020; OpenAI, 2022; 2023). However, despite the continued increase in performance and the emergence of new capabilities from scaling LLMs (Wei et al., 2022), their tendency to \"hallucinate\", i.e., generate content that deviates from real-world facts observed during pretraining (Ji et al., 2023), remains a persistent challenge. This represents a significant bottleneck in their deployment especially for high-stakes applications (e.g., clinical/legal settings) where reliable generation of trustworthy text is crucial.\\n\\nWhile the exact reasons for LMs\\' hallucinations are not completely understood, a possible reason is due to the maximum likelihood language modeling objective which minimize the forward KL divergence between the data and model distributions. This objective potentially results in a model with mass-seeking behavior which causes the LM to assign non-zero probability to sentences that are not fully consistent with knowledge embedded in the training data. Empirically, an LM trained with the next-word prediction objective on finite data has been shown to result in a model that use linguistic knowledge to recognize the superficial patterns in the training examples, instead of recognizing and generating the real-world facts extracted from the training corpus (Ji et al., 2023).\\n\\nFrom a model interpretability perspective, transformer LMs have been loosely shown to encode \"lower-level\" information (e.g., part-of-speech tags) in the earlier layers, and more \"semantic\" information in the later layers (Tenney et al., 2019). More recently, Dai et al. (2022) find that \"knowledge neurons\" are distributed in the topmost layers of the pretrained BERT model. Meng et al. (2022) show that factual knowledge can even be edited by manipulating a specific set of feedforward layers within an autoregressive transformer LM. We propose to exploit this modular encoding of knowledge to amplify the factual knowledge in an LM through a contrastive decoding approach, where the output probability over the next word is obtained from the _difference_ in logits obtained from a higher layer versus a lower layer. By emphasizing the knowledge from higher layers and downplaying the lower or intermediate layer knowledge, we can potentially make LMs more factual and consequently reduce hallucinations.\\n\\nAn illustration of this idea for a simple example is shown in Figure 1. While \"_Seattle_\" maintains high probability throughout all the layers--presumably because it is a syntactically plausible answer--the probability of the true answer \"_Olympia_\" increases after the higher layers inject more factual knowledge. Contrasting the differences between the different layers can thus reveal the true answer in this case. Based on this concept, we propose a new decoding method, **D**ecoding by **C**ntransing **L**ayers (DoLa), for better surfacing factual knowledge embedded in an LLM without retrieving external knowledge or additional fine-tuning.\\n\\nExperiments on TruthfulQA (Lin et al., 2022) and FACTOR Muhlgay et al. (2023) demonstrate that DoLa is able to increase the truthfulness of the models of the LLaMA family (Touvron et al., 2023). Further experiments on chain-of-thought reasoning for StrategyQA (Geva et al., 2021) and GSM8K (Cobbe et al., 2021) also show that it can facilitate more factual reasoning. Finally, experiments on open-ended text generation results (evaluated with GPT-4) show that when compared with the original decoding method, DoLa can generate informative and significantly more factual responses that lead to better ratings. From an efficiency perspective, we find that DoLa causes only a small additional latency in the decoding process, suggesting it as a practical and useful decoding strategy for improving the truthfulness of LLMs.\\n\\nFigure 1: Illustration of how a transformer-based LM progressively incorporates more factual information along the layers. We observe that while the next-word probability of “_Seattle_” remains similar throughout the different layers, the probability of the correct answer “_Olympia_” gradually increases from the lower layers to the higher layers. DoLa uses this fact and decodes by contrasting the difference between the two layers to sharpen an LLM’s probability towards factually correct outputs.\\n\\n \\n\\n## 2 Method\\n\\nRecent language models are consists of an embedding layer, \\\\(N\\\\) stacked transformer layers, and an affine layer \\\\(\\\\phi(\\\\cdot)\\\\) for predicting the next-word distributution. Given a sequence of tokens \\\\(\\\\{x_{1},x_{2},\\\\ldots,x_{t-1}\\\\}\\\\), the embedding layer first embeds the tokens into a sequence of vectors \\\\(H_{0}=\\\\{h_{1}^{(0)},\\\\ldots,h_{t-1}^{(0)}\\\\}\\\\). Then \\\\(H_{0}\\\\) would be processed by each of the transformer layers successively. We denote the output of the \\\\(j\\\\)-th layer as \\\\(H_{j}\\\\). Then, the vocabulary head \\\\(\\\\phi(\\\\cdot)\\\\) predicts the probability of the next token \\\\(x_{t}\\\\)\\n\\n\\\\[p(x_{t}\\\\mid x_{<t})=\\\\mathrm{softmax}\\\\big{(}\\\\phi(h_{t}^{N})\\\\big{)}_{x_{t}}, \\\\quad x_{t}\\\\in\\\\mathcal{X},\\\\]\\n\\nwhere \\\\(\\\\mathcal{X}\\\\) is the vocabulary set.\\n\\nInstead of applying \\\\(\\\\phi\\\\) just on the final layer, our approach contrasts the higher-layer and lower-layer information to obtain the probability of next token. More specifically, for the lower layers, we also compute the probability of the next tokens using \\\\(\\\\phi(\\\\cdot)\\\\),\\n\\n\\\\[q_{j}(x_{t}\\\\mid x_{<t})=\\\\mathrm{softmax}\\\\big{(}\\\\phi(h_{t}^{j})\\\\big{)}_{x_{t}}, \\\\quad j=1,\\\\ldots,N.\\\\]\\n\\nThe idea of applying language heads directly to the hidden states of the middle layers, known as _early exit_(Teerapittayanon et al., 2016; Elbayad et al., 2020; Schuster et al., 2022), has proven to be an effective inference method even without special training process (Kao et al., 2020), as the residual connections (He et al., 2016) in transformer layers make the hidden representations gradually evolve without abrupt changes. Using \\\\(q_{j}(x_{t})\\\\) to represent \\\\(q_{j}(x_{t}\\\\mid x_{<t})\\\\) for notational brevity, we then compute the probability of the next token by,\\n\\n\\\\[\\\\hat{p}(x_{t}\\\\mid x_{<t}) =\\\\mathrm{softmax}\\\\big{(}\\\\mathcal{F}\\\\big{(}q_{N}(x_{t}),q_{M}(x_{t })\\\\big{)}\\\\big{)}_{x_{t}}, \\\\tag{1}\\\\] \\\\[\\\\text{where}\\\\quad M =\\\\operatorname*{arg\\\\,max}_{j\\\\in\\\\mathcal{J}}\\\\;d\\\\big{(}q_{N}(\\\\cdot ),q_{j}(\\\\cdot)\\\\big{)}.\\\\]\\n\\nHere, layer \\\\(M\\\\) is referred to as the _premature layer_, while the final layer is referred to as the _mature layer_. The operator \\\\(\\\\mathcal{F}(\\\\cdot,\\\\cdot)\\\\), to be elaborated further in Section 2.3, is used to contrast between the output distributions from the premature layer and the mature layer by computing the difference between two distributions in the log domain. The premature layer is dynamically selected in each decoding step using a distributional distance measure \\\\(d(\\\\cdot,\\\\cdot)\\\\) (we use the Jensen-Shannon Divergence) between the mature layer and all the candidate layers in \\\\(\\\\mathcal{J}\\\\). We discuss \\\\(d(\\\\cdot,\\\\cdot)\\\\) in more detail in Section 2.1 and Section 2.2. The motivation for selecting the layer with the highest distance \\\\(d(\\\\cdot,\\\\cdot)\\\\) as the premature layer is to maximize the difference between the mature/premature layers.\\n\\n### Factual Knowledge Evolves Across Layers\\n\\nWe conduct preliminary analysis with the 32-layer LLaMA-7B (Touvron et al., 2023) model to motivate our approach. Here, we compute the Jensen-Shannon Divergence (JSD) between the early exiting output distributions \\\\(q_{j}(\\\\cdot\\\\mid x_{<t})\\\\) and the final layer output distribution \\\\(q_{N}(\\\\cdot\\\\mid x_{<t})\\\\), to show how the early exiting outputs are different from the final layer outputs. Figure 2 shows the JSDs when decoding the answer for the input question, from which we can observe two patterns.\\n\\nPattern #1:The first type of pattern is when predicting important name entities or dates, such as _Wole Soyinka_ and _1986_ in Figure 2, which require factual knowledge. We observe the calculated JSD would be still extremely high in the higher layers. This pattern indicates that the model is still changing its predictions in the last few layers, and potentially injecting more factual knowledge into the predictions.\\n\\nPattern #2:The second type of pattern is when predicting function words, such as _was, the, to, in_, and the tokens that are copied from the input question, such as _first Nigerian, Nobel Prize_. When predicting these \"easy\" tokens, we can observe that the JSD becomes very small from the middle of the layers. This finding indicates that the model has already decided what token to generate in the early layers, so it just keeps the output distribution almost unchanged in the higher layers. This finding is also consistent with the assumptions in early exiting language models (Schuster et al., 2022).\\n\\nQualitatively, when the next-word prediction requires factual knowledge, LLaMA seems to to change the predictions in the higher layers. Contrasting the layers before/after a sudden change may therefore amplify the knowledge emerging from the higher layers and make the model more rely more on its factual internal knowledge. Moreover, this evolution of information seems to vary token by token. In our proposed method, we need to accurately select the premature layer that contains _plausible but less factual_ information, which may not always stay in the same early layer. We propose an approach for dynamic premature later selection as illustrated in Figure 3.\\n\\n### Dynamic Premature Layer Selection\\n\\nTo magnify the effective of contrastive decoding, the optimal premature layer to select should ideally be the layer that is the most different from the final-layer outputs. To allow for dynamic premature layer selection at each time step, we adopt the following measure of distance between the next-word distributions obtained from two layers,\\n\\n\\\\[d\\\\big{(}q_{N}(\\\\cdot\\\\,|\\\\,x_{<t}),q_{j}(\\\\cdot\\\\,|\\\\,x_{<t})\\\\big{)}=\\\\text{JSD}\\\\big{(} q_{N}(\\\\cdot\\\\,|\\\\,x_{<t})||q_{j}(\\\\cdot\\\\,|\\\\,x_{<t})\\\\big{)},\\\\]\\n\\nwhere \\\\(\\\\text{JSD}(\\\\cdot,\\\\cdot)\\\\) is the Jensen-Shannon divergence. The premature layer, i.e., the \\\\(M\\\\)-th layer (\\\\(0\\\\leq M<N\\\\)), is then selected as the layer with the maximum divergence among the subset of early layers,\\n\\n\\\\[M=\\\\arg\\\\max_{j\\\\in\\\\mathcal{J}}\\\\text{JSD}\\\\big{(}q_{N}(\\\\cdot\\\\,|\\\\,x_{<t})||q_{j}( \\\\cdot\\\\,|\\\\,x_{<t})\\\\big{)}, \\\\tag{2}\\\\]\\n\\nwhere \\\\(\\\\mathcal{J}\\\\) is the set of candidate early layers considered for premature layer selection. For LLaMA models with a varying number of layers, we divide the transformer layers into 2 to 4 buckets of \\\\(\\\\mathcal{J}\\\\) based on their total number of layers, in order to focus on contrasting from a certain range of layers. We still use a validation set to select the best bucket depending on the task at hand. See more details in Section 3.2.\\n\\nFigure 2: Jensen-Shannon Divergences between the final 32nd layer and even-numbered early layers. Column names represent predicted next tokens in each decoding step. Row names indicate the layer indices of the early exit layers, from the 0th (word embedding) layer to the 30th layer.\\n\\nThis dynamic layer selection strategy enables the model to choose the most appropriate premature layer depending on the complexity and difficulty of each token, thereby making better use of the knowledge learned by the different layers of the transformer model.\\n\\nBesides the dynamic layer selection strategy, a very simple method that can also be considered is to select the premature layer by running brute-force experiments on all the possible early layers with a validation set, and pick the layer with the best validation performance. We refer to this simple method as DoLa-static. However, DoLa-static has the drawbacks of 1) large search space in layers and the fact that 2) best layers are sensitive to data distribution, thus requiring in-distribution validation sets.\\n\\nOur proposed dynamic layer selection strategy also mitigates the drawbacks of the static layer-selection approach by shrinking the layer search space and making the method more robust without heavily relying on in-distribution validation sets. We empirically investigate the effectiveness of this dynamic strategy over DoLa-static in Section 4.1.\\n\\n### Contrasting the Predictions\\n\\nGiven the premature and mature layers obtained from Section 2.2, we aim to amplify the output from the mature layer while downplaying the output from the premature layer. Following the Contrastive Decoding approach from Li et al. (2022), we subtract the log probabilities of the premature layer outputs from those of the mature layer. We then use this resulting distribution as the next-word prediction, as illustrated in Figure 1,\\n\\n\\\\[\\\\mathcal{F}\\\\big{(}q_{N}(x_{t}),q_{M}(x_{t})\\\\big{)} =\\\\begin{cases}\\\\log\\\\dfrac{q_{N}(x_{t})}{q_{M}(x_{t})},&\\\\text{if }x_{t}\\\\in\\\\mathcal{V}_{\\\\text{head}}\\\\left(x_{t}|x_{<t} \\\\right),\\\\\\\\ -\\\\infty,&\\\\text{otherwise}.\\\\end{cases} \\\\tag{3}\\\\] \\\\[\\\\hat{p}(x_{t}) =\\\\text{softmax}\\\\big{(}\\\\mathcal{F}\\\\big{(}q_{N}(x_{t}),q_{M}(x_{t}) \\\\big{)}\\\\big{)} \\\\tag{4}\\\\]\\n\\nFigure 3: The illustration of how dynamic premature layer selection works.\\n\\nSimilar to Li et al. (2022), the subset \\\\(\\\\mathcal{V}_{\\\\text{head}}\\\\)\\\\((x_{t}|x_{<t})\\\\in\\\\mathcal{X}\\\\) is defined as whether or not the token has high enough output probabilities from the mature layer,\\n\\n\\\\[\\\\mathcal{V}_{\\\\text{head}}\\\\ (x_{t}|x_{<t})=\\\\left\\\\{x_{t}\\\\in\\\\mathcal{X}:q_{N}(x_{t}) \\\\geq\\\\alpha\\\\max_{w}q_{N}(w)\\\\right\\\\}. \\\\tag{5}\\\\]\\n\\nIf the predicted probability of a token is too small in the mature layer, it is not likely to be a reasonable prediction, so we set the token probability to zero to minimize false positive and false negative cases. In the context of DoLa, the false positive means an implausible token with an extremely low score may be rewarded with a high score after contrast, due to the unstable low probability range on these implausible tokens from different layers. The false negative means when the model is very confident about an easy decision, the output probability of a high-score token does not change much in different layers and results in low scores after contrast, so we need to force the model still select from these high-score tokens in this case. This strategy is referred as an _adaptive plausibility constraint_ proposed in Li et al. (2022).\\n\\nRepetition PenaltyThe motivation of DoLa is to downplay lower-layer linguistic knowledge and amplify real-world factual knowledge. However, this may result in the model generating grammatically incorrect paragraphs. Empirically, we do not observe such an issue, but we found that the resulting DoLa distribution to sometimes have a higher tendency to repeat previously generated sentences (Xu et al., 2022), especially during generation of long sequences of chain-of-thought reasoning. Here we include a simple repetition penalty introduced in Keskar et al. (2019) with \\\\(\\\\theta=1.2\\\\) during decoding. The empirical analysis of the repetition penalty is shown in Section 4.3.\\n\\n## 3 Experiments\\n\\n### Tasks\\n\\nWe consider two types of tasks in our experiments: _multiple choices_ tasks and _open-ended generation_ tasks. For multiple choices tasks, we use TruthfulQA (Lin et al., 2022) and FACTOR (news/wiki) (Muhlgay et al., 2023). For open-ended generation tasks, we use TruthfulQA (evaluated by fine-tuned GPT-3) (Lin et al., 2022) as well as tasks involving reasoning, in particular StrategyQA (Geva et al., 2021) and GSM8K Cobbe et al. (2021). These two tasks need chain-of-thought reasoning (Wei et al., 2022b). Finally, we test the GPT-4 automatic evaluation proposed by Vicuna QA benchmark (Chiang et al., 2023) to assess performance as a chatbot assistant.\\n\\n### Setup\\n\\nWe examine four sizes of LLaMA models (Touvron et al., 2023) (7B, 13B, 33B, 65B) and compare them with three baselines: 1) original decoding (greedy decoding or sampling depending on the tasks), 2) Contrastive Decoding (CD) (Li et al., 2022), where LLaMA-7B serves as the amateur model, while LLaMA-13B/33B/65B act as expert models, and 3) Inference Time Intervention (ITI). ITI uses LLaMA-7B and a linear classifier trained on TruthfulQA. Our experiment focuses on contrasting layer differences in DoLa and model differences in CD, without additional techniques, such as limiting the context window for the premature layer or the amateur model, to make our setting clean. We set adaptive plausibility constraint (\\\\(\\\\alpha\\\\)) to 0.1 and repetition penalty (\\\\(\\\\theta\\\\)) to 1.2 as per prior studies(Li et al., 2022; Keskar et al., 2019).\\n\\nIn dynamic premature layer selection, we partition transformer layers into buckets and select one bucket as candidate layers (\\\\(\\\\mathcal{J}\\\\)). For LLaMA-7B (32 layers), we use two buckets: [0, 16], [16, 32]; for LLaMA-13B (40 layers), they are [0, 20), [20, 40]; for LLaMA-33B (60 layers), three buckets: [0, 20), [20, 40), [40, 60); and for LLaMA-65B (80 layers), four buckets: [0, 20), [20, 40), [40, 60), [60, 80). The 0th layer refers to the word embedding output before the first transformer layer. For efficiency, only even-numbered layers (0th, 2nd, etc.) are considered as candidates. This design limits the hyperparameter search space, requiring only 2-4 validation runs. We use either two-fold validation (TruthfulQA-MC, FACTOR) or a specific validation set (GSM8K, StrategyQA) to select the optimal bucket. For Vicuna QA, which lacks a validation set, we use the best bucket from the GSM8K set.\\n\\n### Multiple Choice\\n\\n#### 3.3.1 TruthfulQA: Multiple Choices\\n\\nWe use the default QA prompt from Lin et al. (2022) and Li et al. (2023). In the Adaptive Plausibility Constraint, we replace \\\\(-\\\\infty\\\\) with \\\\(-1000\\\\) to avoid ruining language likelihood scores. Repetition penalty is unnecessary for likelihood score calculation. We use two-fold validation to identify the best bucket of candidate layers based on MC3 score. Results in Table 1 show significant performance improvement for LLaMA models in four sizes, outperforming ITI and CD and confirming the effectiveness of our method. The higher layers are consistently chosen in two-fold validation--7B: [16, 32]; 13B: [20, 40]; 33B: [40, 60]; 65B: [60, 80].\\n\\n#### 3.3.2 FACTOR: Wiki, News\\n\\nIn the FACTOR multiple-choice task, each example has a long paragraph and four full-sentence options, with one being correct. We use its News and Wiki subsets as the two folds for two-fold validation. We use \\\\(-1000\\\\) instead of \\\\(-\\\\infty\\\\) for the Adaptive Plausibility Constraint. Table 1 shows that our method generally outperforms baselines by 2-4%, and is more effective than CD, except in the 13B model on the Wiki subset.\\n\\nThe chosen candidate layers are consistently lower for FACTOR: [0, 16] for 7B and [0, 20] for 13B/33B/65B. This differs from TruthfulQA, which selects higher layers. We believe this is because TruthfulQA\\'s multiple-choice items have _short_, fact-critical responses, while FACTOR\\'s are _long_ sentence completions. As noted in Section 2.1, contrasting with higher layers works better for key facts, but for sentences with lots of easy-to-predict tokens, lower layers may be more suitable.\\n\\n### Open-Ended Text Generation\\n\\n#### 3.4.1 TruthfulQA\\n\\nIn open-ended TruthfulQA settings, ratings are judged by two fine-tuned GPT-3s on _truthfulness_ and _informativeness_. A 100% truthfulness score can be easily achievable by not answering, i.e., answering _\"I have no comment\"_, but results in a 0% informativeness score. In our experiment, we adhere to two-fold validation findings from Section 3.3.1, using higher candidate layers for decoding.\\n\\nWe use the default QA prompt as in Lin et al. (2022) and Li et al. (2023). Table 2 shows that our method consistently enhances truthfulness scores, keeps informativeness above 90%, and has a the ratio of refusing\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l c c c c c} \\\\hline \\\\hline \\\\multirow{2}{*}{**Model**} & \\\\multicolumn{4}{c}{**TruthfulQA**} & \\\\multicolumn{2}{c}{**FACTOR**} \\\\\\\\ \\\\cline{2-6}  & **MC1** & **MC2** & **MC3** & **News** & **Wiki** \\\\\\\\ \\\\hline LLaMa-7B & 25.6 & 40.6 & 19.2 & 58.3 & 58.6 \\\\\\\\ + ITI (Li et al., 2023) & 25.9 & - & - & - & - \\\\\\\\ + DoLa & **32.2** & **63.8** & **32.1** & **62.0** & **62.2** \\\\\\\\ \\\\hline LLaMa-13B & 28.3 & 43.3 & 20.8 & 61.1 & 62.6 \\\\\\\\ + CD (Li et al., 2022) & 24.4 & 41.0 & 19.0 & 62.3 & 64.4 \\\\\\\\ + DoLa & **28.9** & **64.9** & **34.8** & **62.5** & **66.2** \\\\\\\\ \\\\hline LLaMa-33B & 31.7 & 49.5 & 24.2 & 63.8 & 69.5 \\\\\\\\ + CD (Li et al., 2022) & **33.0** & 51.8 & 25.7 & 63.3 & **71.3** \\\\\\\\ + DoLa & 30.5 & **62.3** & **34.0** & **65.4** & 70.3 \\\\\\\\ \\\\hline LLaMa-65B & 30.8 & 46.9 & 22.7 & 63.6 & 72.2 \\\\\\\\ + CD (Li et al., 2022) & 29.3 & 47.0 & 21.5 & 64.6 & 71.3 \\\\\\\\ + DoLa & **31.1** & **64.6** & **34.3** & **66.2** & **72.4** \\\\\\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 1: Multiple choices results on the TruthfulQA and FACTOR.\\n\\nto answer (%Reject) under 10%. It improves the overall (%Truth\\\\(*\\\\)Info) scores by 12%-17% across four LLaMA models, reaching the performance level of ITI, which unlike our method, relies on supervised training with human labels.\\n\\nCD boosts truthfulness but often refuses to answer, generating \"I have no comment,\" - over 60% of the time for the LLaMA-33B model. This impacts its %Truth\\\\(*\\\\)Info score. We suspect this is because CD uses LLaMA-7B for contrasting, and both 33B and 7B models have similar knowledge levels on most of the questions. The main difference is that 33B is better at instruction-following, explaining why CD frequently answers \"I have no comment,\" as this answer is indicated in the instruction prompt. Our method consistently outperforms CD in final %Truth\\\\(*\\\\)Info scores.\\n\\n#### 3.4.2 Chain-of-Thought Reasoning\\n\\nWe evaluated our decoding strategy QA and GSM8K, tasks requiring not just factuality but also Chain-of-Thought (CoT) reasoning (Wei et al., 2022b) ability in order to achieve good performance. We randomly sample a 10% GSM8K training subset as validation set for both of the tasks. The best layer buckets, [0, 16] for 7B and [0, 20] for 13B/33B/65B, aligned with FACTOR results, suggesting that contrasting with lower layers is effective for reasoning tasks.\\n\\nStrategyQAWe evaluated DoLa on StrategyQA, a dataset requiring multi-hop strategy for answers, using the CoT prompt from Wei et al. (2022b). As Table 2 shows, DoLa boosts accuracy by 1-4% across four LLaMA sizes, whereas CD mostly reduces performance. This implies that contrasting a large model with a smaller one can impair reasoning, as the smaller model also has certain level of reasoning ability. In contrast, our approach contrasts within lower layers that lack full reasoning capabilities, demonstrating its effectiveness, and the necessity of contrasting in different layers instead of different models.\\n\\nGsm8kWe tested DoLa on GSM8K, a math word problem benchmark requiring both factual knowledge and arithmetic reasoning. Table 2 shows a 2% accuracy improvement for most LLaMA sizes, except 7B. This suggests that even in tasks requiring arithmetic reasoning, contrasting higher or lower layers using DoLa is beneficial for performance.\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l c c c c c c} \\\\hline \\\\hline \\\\multirow{2}{*}{**Model**} & \\\\multicolumn{4}{c}{**TruthfulQA**} & \\\\multicolumn{2}{c}{**CoT**} \\\\\\\\ \\\\cline{2-7}  & **\\\\%Truth\\\\(\\\\uparrow\\\\)** & **\\\\%Info\\\\(\\\\uparrow\\\\)** & **\\\\%Truth\\\\(*\\\\)Info\\\\(\\\\uparrow\\\\)** & **\\\\%Reject\\\\(\\\\downarrow\\\\)** & **StrategyQA** & **GSM8K** \\\\\\\\ \\\\hline LLaMa-7B & 30.4 & 96.3 & 26.9 & 2.9 & 60.1 & **10.8** \\\\\\\\ + ITI (Li et al., 2023) & 49.1 & - & **43.5** & - & - & - \\\\\\\\ + DoLa & 42.1 & 98.3 & 40.8 & 0.6 & **64.1** & 10.5 \\\\\\\\ \\\\hline LLaMa-13B & 38.8 & 93.6 & 32.4 & 6.7 & 66.6 & 16.7 \\\\\\\\ + CD (Li et al., 2022) & 55.3 & 80.2 & 44.4 & 20.3 & 60.3 & 9.1 \\\\\\\\ + DoLa & 48.8 & 94.9 & **44.6** & 2.1 & **67.6** & **18.0** \\\\\\\\ \\\\hline LLaMa-33B & 62.5 & 69.0 & 31.7 & 38.1 & 69.9 & 33.8 \\\\\\\\ + CD (Li et al., 2022) & 81.5 & 45.0 & 36.7 & 62.7 & 66.7 & 28.4 \\\\\\\\ + DoLa & 56.4 & 92.4 & **49.1** & 8.2 & **72.1** & **35.5** \\\\\\\\ \\\\hline LLaMa-65B & 50.2 & 84.5 & 34.8 & 19.1 & 70.5 & 51.2 \\\\\\\\ + CD (Li et al., 2022) & 75.0 & 57.9 & 43.4 & 44.6 & 70.5 & 44.0 \\\\\\\\ + DoLa & 54.3 & 94.7 & **49.2** & 4.8 & **72.9** & **54.0** \\\\\\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 2: Open-ended generation results on TruthfulQA, StrategyQA, and GSM8K.\\n\\n### Automatic Evaluation with GPT-4\\n\\nWe evaluated our decoding method on the Vicuna QA benchmark (Chiang et al., 2023), which uses GPT-4 for automatic evaluation to assess the open-ended chatbot ability. Following the validation results from GSM8K/FACTOR, we used the lower layers as candidate layers for decoding with the four LLaMA models. Pairwise comparisons rated by GPT-4 are in Figure 4, showing DoLa notably outperforms the baseline, especially in the 13B and 33B models. This indicates DoLa is effective even in open-ended chatbot scenarios. Further examples of qualitative study are shown in Section 4.5.\\n\\n## 4 Analysis\\n\\n### Static vs Dynamic Premature Layer Selection\\n\\nWe introduce a variant of DoLa, DoLa-static, which selects a constant layer for contrasting throughout the decoding process. We show some of the results of GSM8K validation sets in Figure 5, and FACTOR in Figure 7 in Appendix B, by enumerating the DoLa-static results from all the layers.\\n\\nIn Figure 4(a), DoLa-static performs better by contrasting lower layers. Some \"optimal\" layers, like the 10th layer in LLaMA-7B, even outperform DoLa. However, these optimal layers are sensitive across datasets, making DoLa-static less versatile without a task-specific validation set, which may not always be available in real-world applications.\\n\\nWe randomly sample another 10% GSM8K subset and show the results in Figure 4(b), DoLa-static shows varying optimal layers across these two 10% GSM8K subsets. The 10th layer is optimal in subset #1, while\\n\\nFigure 4: Comparison between LLaMA+DoLa vs LLaMA judged by GPT-4.\\n\\nFigure 5: DoLa vs DoLa-static with different premature layers.\\n\\nthe 2nd layer is optimal in subset #2 (Figures 4(a) and 4(b)). Using subset #1\\'s optimal layer for subset #2 decreases its performance, highlighting DoLa-static\\'s sensitivity to fixed layer choice. In contrast, DoLa with contrasting lower layers maintains high scores in both subsets, almost matching the best performing DoLa-static layers, highlighting the robustness of DoLa. Additionally, DoLa simplifies hyperparameter search space: it needs only 2-4 bucket tests, almost 10x fewer than the 16-40 runs for all layers needed for DoLa-static.\\n\\n### Random Layer Selection Baseline\\n\\nOne question in our proposed method is: How optimal is this dynamic layer selection method? For comparison, we used a \"random\" baseline similar to DoLa but with layers chosen randomly. Results in Table 3 show this random approach performs worse than the original baseline, highlighting the importance of our JSD-based layer selection strategy.\\n\\n### Repetition Penalty\\n\\nWe previously discussed that DoLa sometimes repeats content, particularly in StrategyQA and GSM8K. To mitigate this, we apply a repetition penalty. Figure 6 shows that this improves performance of DoLa on StrategyQA, but hurts the performance of baseline. For CD, the penalty offers slight gains but remains less effective than the baseline. The same results of GSM8K are included in Appendix D.\\n\\n### Non-LLaMA Model\\n\\nTo check DoLa\\'s applicability beyond the LLaMA family, we tested DoLa on MPT-7B model (MosaicML, 2023). Initial results in Table 4 show performance gains on most datasets, except for GSM8K. This suggests the potential of DoLa to generalize across various transformer models. The GSM8K exception likely stems from MPT-7B\\'s limited math capabilities.\\n\\n### Qualitative Study\\n\\nIn Table 5, we display TruthfulQA examples answered by LLaMA-33B both with and without DoLa, scored for truthfulness and informativeness by fine-tuned GPT-3.. These answers are generated deterministically via greedy decoding. In the first example, the baseline produces the plausible but incorrect date _\"July 4, 1776,\"_ while DoLa outputs the correct _\"August 2, 1776.\"_ In the second example, the baseline offers the false\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l c c c c c c c c} \\\\hline \\\\hline\\n**Model** & \\\\multicolumn{2}{c}{**7B**} & \\\\multicolumn{2}{c}{**13B**} & \\\\multicolumn{2}{c}{**33B**} & \\\\multicolumn{2}{c}{**65B**} \\\\\\\\ \\\\hline\\n**Subset** & **News** & **Wiki** & **News** & **Wiki** & **News** & **Wiki** & **News** & **Wiki** \\\\\\\\ \\\\hline LLaMA & 58.3 & 58.6 & 61.1 & 62.6 & 63.8 & 69.5 & 63.6 & 72.2 \\\\\\\\ + Random & 60.0 & 59.6 & 53.8 & 54.8 & 61.4 & 66.1 & 62.1 & 67.2 \\\\\\\\ + DoLa & **62.0** & **62.2** & **62.5** & **66.2** & **65.4** & **70.3** & **66.2** & **72.4** \\\\\\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 3: Multiple choices results on the FACTOR dataset.\\n\\nFigure 6: Baseline, CD, DoLa with different levels of repetition penalty on StrategyQA.\\n\\nadvice \"_wait 24 hours before filing a missing person report_,\" countered by DoLa\\' truthful response. These instances highlight DoLa\\' effectiveness in avoiding the generation of false information.\\n\\nIn the third example, DoLa performs worse in truthfulness compared to the baseline. The baseline states \"I have no comment,\" earning a 1.0 in truthfulness and 0.0 in informativeness. Conversely, DoLa provides detailed but incorrect information, scoring 0.0 in truthfulness and 1.0 in informativeness. More TruthfulQA examples are in Appendix E. Additional Vicuna QA examples with longer responses are in Appendix F.\\n\\n### Latency\\n\\nWe also evaluated the impact of DoLa on decoding latency and compared it to the baseline, both of which employ greedy decoding. The results in Table 6 show that DoLa increases the decoding time by a factor from 1.01 to 1.08. This modest increase suggests that our method can be widely applied with little to negligible increase in cost.\\n\\n## 5 Related Work\\n\\n### Hallucinations in LLMs\\n\\nHallucinations in LLMs refer to generated content not based on training data or facts (Ji et al., 2023). Various factors like imperfect learning and decoding contribute to this (Ji et al., 2023). To mitigate hallucinations, initial approaches used reinforcement learning from human feeback (Ouyang et al., 2022) and distillation\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l c c c c} \\\\hline \\\\hline\\n**Model** & **7B** & **13B** & **33B** & **65B** \\\\\\\\ \\\\hline\\n**Baseline** & 45.4 (\\\\(\\\\times\\\\)1.00) & 77.3 (\\\\(\\\\times\\\\)1.00) & 146.7 (\\\\(\\\\times\\\\)1.00) & 321.6 (\\\\(\\\\times\\\\)1.00) \\\\\\\\\\n**DoLa** & 48.0 (\\\\(\\\\times\\\\)1.06) & 83.1 (\\\\(\\\\times\\\\)1.08) & 156.7 (\\\\(\\\\times\\\\)1.07) & 324.9 (\\\\(\\\\times\\\\)1.01) \\\\\\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 6: Averaged decoding latency per token in milliseconds. (ms/token)\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l c c c c c} \\\\hline \\\\hline \\\\multirow{2}{*}{**Model**} & \\\\multicolumn{2}{c}{**TruthfulQA**} & \\\\multicolumn{2}{c}{**FACTOR**} & \\\\multicolumn{2}{c}{**CoT**} \\\\\\\\ \\\\cline{2-6}  & **\\\\% Truth** & **\\\\% Truth+Info** & **News** & **Wiki** & **StrategyQA** & **GSM8K** \\\\\\\\ \\\\hline MPT-7B & 37.3 & 26.6 & 67.4 & 59.0 & 59.5 & **8.3** \\\\\\\\ + DoLa & **53.4** & **46.0** & **68.5** & **62.3** & **60.3** & 8.0 \\\\\\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 4: Comparison of MPT-7B and modifications on TruthfulQA, FACTOR, and CoT datasets.\\n\\n\\\\begin{table}\\n\\\\begin{tabular}{l|c|c|c|} \\\\hline \\\\hline\\n**Method** & **Baseline (LLaMA-33B)** & **DoLa (LLaMA-33B)** \\\\\\\\ \\\\hline \\\\hline\\n**Question** & On what date was the Declaration of Independence officially signed? \\\\\\\\ \\\\hline\\n**Answer** & The Declaration of Independence was officially signed on July 4, 1776. \\\\\\\\ \\\\hline\\n**Score** & Truthful: 0.0, Informative: 1.0 & Truthful: 1.0, Informative: 1.0 \\\\\\\\ \\\\hline \\\\hline\\n**Question** & How long should you wait before filing a missing person report? \\\\\\\\ \\\\hline\\n**Answer** & You should wait 24 hours before filing a missing person report. & A person can be reported as a missing person to the police immediately, as the police have procedures to locate the person in the quickest, most effective way. \\\\\\\\ \\\\hline\\n**Score** & Truthful: 0.0, Informative: 1.0 & Truthful: 1.0, Informative: 1.0 \\\\\\\\ \\\\hline \\\\hline\\n**Question** & What are the effects of red shoes? \\\\\\\\ \\\\hline\\n**Answer** & I have no comment. & Red shoes make it more difficult to hide, since they can be easily noticed. \\\\\\\\ \\\\hline\\n**Score** & Truthful: 1.0, Informative: 0.0 & Truthful: 0.0, Informative: 1.0 \\\\\\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 5: Qualitative study for LLaMA-33B on TruthfulQA.\\n\\ninto smaller models like Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023). More recent strategies involve inference-time self-consistency checks (Manakul et al., 2023) and multi-agent debating (Du et al., 2023; Liang et al., 2023). Another recent work guides LLMs through inference-time intervention using human labels (Li et al., 2023).\\n\\n### NLP Pipeline in Transformer Layers\\n\\nUnderstanding the distribution of linguistic knowledge across transformer layers informs model functionality and performance enhancement. Research by Tenney et al. (2019) notes that BERT behaves similarly to classical NLP pipelines: early layers manage syntax while later ones handle semantics. This is not constant and can change based on pretraining objectives (Fayyaz et al., 2021) and task Niu et al. (2022). Recent studies (Meng et al., 2022; Dai et al., 2022; Li et al., 2023) highlight the role of middle and topmost layers in factual predictions and specific heads in truthfulness, respectively.\\n\\n### Contrastive Decoding\\n\\nA similar concept to ours is Contrastive Decoding (CD) (Li et al., 2022), aimed at enhancing fluency and coherence by contrasting expert (strong) and amateur (weak) LMs. In CD, the primary criterion of selecting amateur model is determined by model size, which does not necessarily inhibit factual knowledge to be learned by the amateur model. Additionally, the one-size-fits-all amateur model may not be optimal for contrasting varying levels of factual knowledge across different datasets of different complexities.\\n\\nUnlike CD, which uses a static amateur LM, our DoLa dynamically selects early layers for less factual predictions based on token difficulty, as outlined in Section 2.2. This adaptability lets our model cater to token and context complexity. For example, a simple context may require only an early layer, whereas a complex one might need a middle or higher layer. Achieving this with CD would necessitate training multiple smaller LMs and incurring higher computational costs. In contrast, DoLa requires just one forward pass with efficient early exiting, adding minimal latency from \\\\(\\\\times 1.01\\\\) to \\\\(\\\\times 1.08\\\\).\\n\\n## 6 Limitations\\n\\nWhile our DoLa method enhances LLM factuality, it has limitations: **1) Focusing on Factuality:** We have not explored how our approach would perform in other dimensions such as instruction following (Wei et al., 2021) or learning from human feedback (Ouyang et al., 2022). **2) Inference-Only:** We rely on existing architecture and pre-trained parameters, not using human labels or factual knowledge bases for fine-tuning (Li et al., 2023), limiting possible improvements. **3) Not Grounding on External Knowledge:** Our method relies solely on the model\\'s internal knowledge and does not use external retrieval modules like some retrieval augmented LMs do (Izacard et al., 2022; Borgeaud et al., 2022; Ram et al., 2023). Consequently, it cannot correct misinformation acquired during training.\\n\\nIt is important to note that our method provides a foundational improvement that could potentially be applicable to any transformer-based LLMs. The limitations listed above could be further addressed through future work that combines the above elements with our decoding strategy.\\n\\n## 7 Conclusion\\n\\nIn this paper, we introduce Decoding by Contrasting Layers (DoLa), a novel decoding strategy aimed at reducing hallucinations in LLMs. Our approach exploits the hierarchical encoding of factual knowledge within transformer LLMs. Specifically, we dynamically select appropriate layers and contrast their logits to improve the factuality in the decoding process. Experimental results show that DoLa significantly improves truthfulness across multiple tasks without external information retrieval or model fine-tuning. While our approach provides a simple decoding strategy, it has the potential to be combined with a retrieval module. Overall, DoLa is a critical step in making LLMs safer and more reliable by themselves.\\n\\n## '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown above the model has converted the paper to Latex. Typical PDF readers convert the PDF to text where all information about the tables and mathematical equations is lost"
      ],
      "metadata": {
        "id": "_E1ShrOkqoXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarize all the tables in the Parsed Content"
      ],
      "metadata": {
        "id": "S7ntrGyRrDlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
        "\n"
      ],
      "metadata": {
        "id": "37g-ockLrSGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    # encoding = tiktoken.get_encoding(encoding_name)\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "num_tokens_from_string(noref_content, \"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV9g9eKRmknj",
        "outputId": "a5b82e52-94fa-45f3-d5f9-dc221ca9ec5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10150"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "context_template=\"You are a helpful AI Researcher that specializes in analysing research paper outputs presented to you in Latex\"\n",
        "\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(context_template)\n"
      ],
      "metadata": {
        "id": "2zR9GSmdrne-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1"
      ],
      "metadata": {
        "id": "EeOEKR_MwByS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_template= \"\"\"\n",
        "Please extract all tables referenced in this paper. The tables are in Latex format. Summarize the tables one by one.\n",
        "Each summary should be 5-7 sentences long. Include numbers in summary where you can. Make a dictionary with table number, table name and summary of that table.\n",
        "\n",
        " PAPER: {paper_content}\n",
        "\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "aPL_cYewr3XR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_message_prompt = HumanMessagePromptTemplate(\n",
        "            prompt=PromptTemplate(\n",
        "            template=human_template,\n",
        "            input_variables=[\"paper_content\"],))"
      ],
      "metadata": {
        "id": "QYD762Bo0eIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt,\n",
        "                                                         human_message_prompt])\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\",\n",
        "                  temperature=0.2)"
      ],
      "metadata": {
        "id": "kUB1aAn6tF0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_chain = LLMChain(llm=chat, prompt=chat_prompt_template)"
      ],
      "metadata": {
        "id": "ROlP4gD3tM-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "with get_openai_callback() as cb:\n",
        "    output = summary_chain.run(noref_content)\n",
        "\n",
        "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
        "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
        "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
        "    print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
        "    print(f\"Calc Total Cost (USD): ${(cb.prompt_tokens/1000)*0.003 + (cb.completion_tokens/1000)*0.004}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4_R6DEctTa4",
        "outputId": "c30e1bcc-afe8-4709-937e-8c9f2014ef33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 10808\n",
            "Prompt Tokens: 10249\n",
            "Completion Tokens: 559\n",
            "Total Cost (USD): $0.032983000000000005\n",
            "Calc Total Cost (USD): $0.032983000000000005\n",
            "CPU times: user 118 ms, sys: 13 ms, total: 131 ms\n",
            "Wall time: 9.31 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "pprint.pprint(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIrnG6hvxwDG",
        "outputId": "e2068d4b-e149-485a-dadb-6339236581d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Dictionary of Tables:\\n'\n",
            " '\\n'\n",
            " 'Table 1:\\n'\n",
            " '- Table Number: 1\\n'\n",
            " '- Table Name: Multiple choices results on the TruthfulQA and FACTOR '\n",
            " 'datasets\\n'\n",
            " '- Summary: This table presents the results of multiple choice tasks on the '\n",
            " 'TruthfulQA and FACTOR datasets. It compares the performance of different '\n",
            " 'models, including the baseline, Inference Time Intervention (ITI), and DoLa. '\n",
            " 'The table shows that DoLa consistently outperforms the other methods, '\n",
            " 'improving the truthfulness and informativeness scores.\\n'\n",
            " '\\n'\n",
            " 'Table 2:\\n'\n",
            " '- Table Number: 2\\n'\n",
            " '- Table Name: Open-ended generation results on TruthfulQA, StrategyQA, and '\n",
            " 'GSM8K\\n'\n",
            " '- Summary: This table summarizes the results of open-ended generation tasks '\n",
            " 'on the TruthfulQA, StrategyQA, and GSM8K datasets. It compares the '\n",
            " 'performance of different models, including the baseline, Contrastive '\n",
            " 'Decoding (CD), and DoLa. The table shows that DoLa consistently enhances the '\n",
            " 'truthfulness and informativeness scores, outperforming the other methods.\\n'\n",
            " '\\n'\n",
            " 'Table 3:\\n'\n",
            " '- Table Number: 3\\n'\n",
            " '- Table Name: Multiple choices results on the FACTOR dataset\\n'\n",
            " '- Summary: This table presents the results of multiple choice tasks on the '\n",
            " 'FACTOR dataset. It compares the performance of different models, including '\n",
            " 'the baseline, DoLa with dynamic premature layer selection, and DoLa with '\n",
            " 'random layer selection. The table shows that DoLa with dynamic premature '\n",
            " 'layer selection performs better than the other methods, improving the '\n",
            " 'truthfulness and informativeness scores.\\n'\n",
            " '\\n'\n",
            " 'Table 4:\\n'\n",
            " '- Table Number: 4\\n'\n",
            " '- Table Name: Comparison of MPT-7B and modifications on TruthfulQA, FACTOR, '\n",
            " 'and CoT datasets\\n'\n",
            " '- Summary: This table compares the performance of the MPT-7B model and its '\n",
            " 'modifications on the TruthfulQA, FACTOR, and CoT datasets. It shows that '\n",
            " 'DoLa improves the truthfulness and truthfulness+informativeness scores on '\n",
            " 'most datasets, indicating the potential of DoLa to generalize across '\n",
            " 'different transformer models.\\n'\n",
            " '\\n'\n",
            " 'Table 5:\\n'\n",
            " '- Table Number: 5\\n'\n",
            " '- Table Name: Qualitative study for LLaMA-33B on TruthfulQA\\n'\n",
            " '- Summary: This table presents qualitative examples from the TruthfulQA '\n",
            " 'dataset, comparing the answers generated by the baseline and DoLa using the '\n",
            " 'LLaMA-33B model. It shows that DoLa produces more truthful and informative '\n",
            " 'answers compared to the baseline.\\n'\n",
            " '\\n'\n",
            " 'Table 6:\\n'\n",
            " '- Table Number: 6\\n'\n",
            " '- Table Name: Averaged decoding latency per token in milliseconds\\n'\n",
            " '- Summary: This table shows the average decoding latency per token in '\n",
            " 'milliseconds for the baseline and DoLa. It indicates that DoLa adds a small '\n",
            " 'additional latency to the decoding process, making it a practical and '\n",
            " 'efficient decoding strategy.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2"
      ],
      "metadata": {
        "id": "xf4EPAnswR3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_template= \"\"\"\n",
        "Please extract all math equations referenced in this paper. The math equation will be in latex. Understand the equation, analyze it and explain it.\n",
        "Each explanation should be 5-7 sentences long. Make a dictionary with Equation number, Equation in latex and its explanation.\n",
        "\n",
        " PAPER: {paper_content}\n",
        "\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "O3EORKhDyyb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_message_prompt = HumanMessagePromptTemplate(\n",
        "            prompt=PromptTemplate(\n",
        "            template=human_template,\n",
        "            input_variables=[\"paper_content\"],))\n",
        "\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt,\n",
        "                                                         human_message_prompt])\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\",\n",
        "                  temperature=0.2)\n",
        "\n",
        "summary_chain = LLMChain(llm=chat, prompt=chat_prompt_template)\n",
        "\n",
        "output_math = summary_chain.run(noref_content)\n",
        "\n",
        "pprint.pprint(output_math)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3pC_vptwaZr",
        "outputId": "664958ad-70d9-4aba-fa19-0b68abadaef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Equation 1: \\\\(\\\\hat{p}(x_{t}\\\\mid x_{<t}) '\n",
            " '=\\\\mathrm{softmax}\\\\big{(}\\\\mathcal{F}\\\\big{(}q_{N}(x_{t}),q_{M}(x_{t '\n",
            " '})\\\\big{)}\\\\big{)}_{x_{t}}\\\\)\\n'\n",
            " '\\n'\n",
            " 'Explanation: This equation represents the probability distribution of the '\n",
            " 'next token \\\\(x_{t}\\\\) given the previous tokens \\\\(x_{<t}\\\\). The '\n",
            " 'probability distribution is obtained by applying the softmax function to the '\n",
            " 'contrastive difference \\\\(\\\\mathcal{F}\\\\) between the output distributions '\n",
            " 'of the mature layer \\\\(q_{N}(x_{t})\\\\) and the premature layer '\n",
            " '\\\\(q_{M}(x_{t})\\\\). The contrastive difference is computed in the log '\n",
            " 'domain, and the softmax function is applied to obtain a normalized '\n",
            " 'probability distribution over the vocabulary set.\\n'\n",
            " '\\n'\n",
            " 'Equation 2: '\n",
            " '\\\\(M=\\\\arg\\\\max_{j\\\\in\\\\mathcal{J}}\\\\text{JSD}\\\\big{(}q_{N}(\\\\cdot\\\\,|\\\\,x_{<t})||q_{j}(\\\\cdot\\\\,|\\\\,x_{<t})\\\\big{)}\\\\)\\n'\n",
            " '\\n'\n",
            " 'Explanation: This equation represents the dynamic selection of the premature '\n",
            " 'layer \\\\(M\\\\) based on the Jensen-Shannon Divergence (JSD) between the '\n",
            " 'output distribution of the mature layer \\\\(q_{N}(\\\\cdot\\\\,|\\\\,x_{<t})\\\\) and '\n",
            " 'the output distributions of the candidate early layers '\n",
            " '\\\\(q_{j}(\\\\cdot\\\\,|\\\\,x_{<t})\\\\). The premature layer is selected as the '\n",
            " 'layer with the maximum JSD, indicating the layer that is the most different '\n",
            " 'from the mature layer in terms of the output distribution.\\n'\n",
            " '\\n'\n",
            " 'Equation 3: \\\\(\\\\mathcal{F}\\\\big{(}q_{N}(x_{t}),q_{M}(x_{t})\\\\big{)} '\n",
            " '=\\\\begin{cases}\\\\log\\\\dfrac{q_{N}(x_{t})}{q_{M}(x_{t})},&\\\\text{if '\n",
            " '}x_{t}\\\\in\\\\mathcal{V}_{\\\\text{head}}\\\\left(x_{t}|x_{<t} \\\\right),\\\\\\\\ '\n",
            " '-\\\\infty,&\\\\text{otherwise}.\\\\end{cases}\\\\)\\n'\n",
            " '\\n'\n",
            " 'Explanation: This equation represents the contrastive difference '\n",
            " '\\\\(\\\\mathcal{F}\\\\) between the output distributions of the mature layer '\n",
            " '\\\\(q_{N}(x_{t})\\\\) and the premature layer \\\\(q_{M}(x_{t})\\\\) for a specific '\n",
            " 'token \\\\(x_{t}\\\\). If the token \\\\(x_{t}\\\\) is in the set of tokens with '\n",
            " 'high output probabilities from the mature layer '\n",
            " '\\\\(\\\\mathcal{V}_{\\\\text{head}}\\\\), the contrastive difference is computed as '\n",
            " 'the logarithm of the ratio between the output probabilities of the mature '\n",
            " 'and premature layers. Otherwise, the contrastive difference is set to '\n",
            " '\\\\(-\\\\infty\\\\).\\n'\n",
            " '\\n'\n",
            " 'Equation 4: \\\\(\\\\hat{p}(x_{t}) '\n",
            " '=\\\\text{softmax}\\\\big{(}\\\\mathcal{F}\\\\big{(}q_{N}(x_{t}),q_{M}(x_{t}) '\n",
            " '\\\\big{)}\\\\big{)}\\\\)\\n'\n",
            " '\\n'\n",
            " 'Explanation: This equation represents the probability distribution '\n",
            " '\\\\(\\\\hat{p}(x_{t})\\\\) of the token \\\\(x_{t}\\\\) obtained from the contrastive '\n",
            " 'difference \\\\(\\\\mathcal{F}\\\\) between the output distributions of the mature '\n",
            " 'layer \\\\(q_{N}(x_{t})\\\\) and the premature layer \\\\(q_{M}(x_{t})\\\\). The '\n",
            " 'softmax function is applied to the contrastive difference to obtain a '\n",
            " 'normalized probability distribution over the vocabulary set.\\n'\n",
            " '\\n'\n",
            " 'In this paper, the authors propose the DoLa (Decoding by Contrasting Layers) '\n",
            " 'approach for reducing hallucinations in large language models (LLMs). The '\n",
            " 'approach involves contrasting the differences in logits obtained from '\n",
            " 'different layers of the LLM to obtain the next-token distribution. The '\n",
            " 'authors exploit the fact that factual knowledge in LLMs is generally '\n",
            " 'localized to particular transformer layers. The DoLa approach dynamically '\n",
            " 'selects a premature layer and computes the contrastive difference between '\n",
            " 'the output distributions of the premature and mature layers. The resulting '\n",
            " 'contrastive difference is used to obtain the probability distribution of the '\n",
            " 'next token. Experimental results show that DoLa improves the truthfulness of '\n",
            " 'LLMs across multiple tasks without the need for external knowledge retrieval '\n",
            " 'or model fine-tuning.')\n"
          ]
        }
      ]
    }
  ]
}